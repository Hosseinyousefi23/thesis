

\فصل{مقدمه}

یادگیری ماشین کاربردی از هوش مصنوعی (AI) است که به کامپیوترها، توانایی یادگیری خودکار از طریق تجربه  را می دهد، بدون اینکه مسنقیما برنامه ریزی شود. حوزه یادگیری ماشینی بر توسعه برنامه های رایانه ای متمرکز است که می توانند به داده ها دسترسی پیدا کنند و از آنها برای یادگیری استفاده کنند. فرایند یادگیری با مشاهده داده ها یا تجربه مستقیم  به منظور کشف الگوهای نهفته در داده ها آغاز می شود؛  سپس از الگوهای  استخراج شده برای اتخاذ تصمیمات بهتری در آینده استفاده می شود. . هدف اصلی این است که به کامپیوترها اجازه دهیم بدون دخالت یا کمک انسان و به طور خودکار یاد بگیرند و اقدامات را بر اساس آن تنظیم کنند.

\section{دسته بندی روش های یادگیری ماشین}
الگوریتم های یادگیری ماشین، به طور کلاسیک به دو دسته کلی یادگیری نظارت شده و یادگیری بدون نظارت تقسیم می شوند. بعدها دو دسته جدید یادگیری نیمه نظارت شده و یادگیری تقویتی به این مجموعه اضافه شدند.
\subsection{یادگیری نظارت شده}

الگوریتم های یادگیری ماشین تحت نظارت می توانند  با استفاده از داده های برچسب زده شده، الگوهایی  را یادبگیرند و سپس آنچه را که آموخته اند، برای پیش بینی برچسب داده های جدید اعمال کنند. الگوریتم یادگیری تحت نظارت، با تجزیه و تحلیل مجموعه داده های برچسب دار، یک تابع تقریبی را برای پیش بینی مقادیر خروجی می آموزد. الگوریتم یادگیری همچنین می تواند خروجی خود را با خروجی صحیح، مقایسه کند تا بتواند، بر این اساس مدل را اصلاح کند.
%Supervised machine learning algorithms can apply what has been learned in the past to new data using labeled examples to predict future events. Starting from the analysis of a known training dataset, the learning algorithm produces an inferred function to make predictions about the output values. The system is able to provide targets for any new input after sufficient training. The learning algorithm can also compare its output with the correct, intended output and find errors in order to modify the model accordingly.

\subsection{یادگیری بدون نظارت}

در مقابل  یادگیری نظارت\nf شده، الگوریتم\nf های یادگیری ماشین بدون نظارت قرار دارند. این روش\nf ها هنگامی استفاده می\nf شوند که اطلاعات مورد استفاده برای آموزش نه طبقه بندی شده و نه دارای برچسب باشند. در یادگیری بدون نظارت، سیستم\nf ها می\nf توانند تابعی را برای توصیف ساختار پنهان داده\nf های بدون برچسب بیاموزند. این سیستم\nf ها خروجی صحیح را کشف نمی\nf کنند، اما داده\nf ها را کاوش می\nf کنند و می\nf توانند نتایجی را از مجموعه داده\nf ها استخراج کنند.

%In contrast, unsupervised machine learning algorithms are used when the information used to train is neither classified nor labeled. Unsupervised learning studies how systems can infer a function to describe a hidden structure from unlabeled data. The system doesn’t figure out the right output, but it explores the data and can draw inferences from datasets to describe hidden structures from unlabeled data.

\subsection{یادگیری نیمه نظارت شده}

الگوریتم\nf های یادگیری ماشین نیمه نظارت\nf شده در جایی بین یادگیری نظارت\nf شده و بدون نظارت قرار می\nf گیرند، آن\nf ها از هر دو داده برچسب\nf دار و بدون برچسب برای آموزش استفاده می\nf کنند. داده\nf های مورد استفاده، به طور معمول شامل مقدار کمی از داده\nf های برچسب\nf دار و مقدار زیادی از داده\nf های بدون برچسب است. معمولاً یادگیری نیمه نظارت\nf شده زمانی انتخاب می\nf شود که یادگیری نظارت\nf شده به دلایل مختلف عملی نباشد. از جمله این دلایل، می\nf توان به کوچک یا ناکافی بودن داده های برچسب دار  و یا پیچیدگی تابع مورد نظر باشد. سیستم\nf هایی که از این روش استفاده می\nf کنند ، می\nf توانند به میزان قابل توجهی دقت یادگیری را نسبت به روش یادگیری نظارت\nf شده بهبود بخشند. 
%Semi-supervised machine learning algorithms fall somewhere in between supervised and unsupervised learning, since they use both labeled and unlabeled data for training – typically a small amount of labeled data and a large amount of unlabeled data. The systems that use this method are able to considerably improve learning accuracy. Usually, semi-supervised learning is chosen when the acquired labeled data requires skilled and relevant resources in order to train it / learn from it. Otherwise, acquiring unlabeled data generally doesn’t require additional resources.	

\subsection{یادگیری تقویتی}
یادگیری تقویتی، یک روش یادگیری است که از طریف تعامل با محیط و تجربه می \nf آموزد و خطاها یا پاداش ها را کشف می کند. \مهم{آزمون و خطا} و \مهم{تأخیر در پاداش}، مهمترین ویژگی  های یادگیری تقویتی هستند. این روش به عامل اجازه می دهد تا رفتار ایده آل را به طور خودکار پیدا کند تا مقیاس عملکرد آن به حداکثر برسد. سینگال پاداش،  که توسط محیط تولید می شود، این امکان را برای عامل فراهم می \nf کند که بهترین اقدامات را بباموزد. در ادامه این پایان نامه، این روش را دقیق تر بررسی خواهیم کرد.
%Reinforcement machine learning algorithms is a learning method that interacts with its environment by producing actions and discovers errors or rewards. Trial and error search and delayed reward are the most relevant characteristics of reinforcement learning. This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance. Simple reward feedback is required for the agent to learn which action is best; this is known as the reinforcement signal.


\section{تاریخچه یادگیری تقویتی}
تاریخچه اولیه یادگیری تقویتی، دو مسیر اصلی دارد که هر دو طولانی و بسیار غنی هستند. یکی از این مسیرها مربوط می\nf شود به \مهم{یادگیری توسط
آزمون و خطا} که پیدایش آن به حوزه روانشناسی یادگیری حیوانات برمی\nf گردد. این مسیر در اولین مطالعات حوزه هوش مصنوعی به وجود آمد و در اواخر دهه 1980 نیز منجر احیای مجدد هوش مصنوعی شد.

مسیر دیگر مربوط به مسئله کنترل بهینه و حل آن با استفاده از توابع ارزش و برنامه\nf ریزی پویا است.

 اگر چه
دو رشته تا حد زیادی مستقل بوده ، استثنائات حول یک موضوع سوم و کمتر مشخص یعنی روشهای اختلاف زمانی \LTRfootnote{Temporal Difference} می چرخند. هر سه رشته در اواخر دهه 1980 گرد هم آمدند تا زمینه یادگیری تقویتی مدرن را فراهم کنند.


%The early history of reinforcement learning has two main threads, both long and rich, that were pursued
%independently before intertwining in modern reinforcement learning. One thread concerns learning by
%trial and error that started in the psychology of animal learning. This thread runs through some of
%the earliest work in artificial intelligence and led to the revival of reinforcement learning in the early
%1980s. The other thread concerns the problem of optimal control and its solution using value functions
%and dynamic programming. For the most part, this thread did not involve learning. Although the
%two threads have been largely independent, the exceptions revolve around a third, less distinct thread
%concerning temporal-difference methods such as the one used in the tic-tac-toe example in this chapter.
%All three threads came together in the late 1980s to produce the modern field of reinforcement learning


در دهه اخیر، الگوریتم های مدرن یادگیری تقویتی  ژرف مانند DQN با موفقیت های فراوان، توانسته اند انقلابی در حوزه هوش مصنوعی و یادگیری ماشین به وجود آورند. نمونه های آن می توان به
 \lr{OpenAI Five}و
  \lr{AlphaZero} اشاره کرد.
  
  
\lr{OpenAI Five}
   نام یک پروژه یادگیری ماشین است که به عنوان تیمی از ربات های بازی ویدیویی بازی می کند که در مقابل بازیکنان انسان در بازی ویدئویی رقابتی پنج به پنج Dota 2 بازی می کند. این سیستم توسط OpenAI ، یک هوش مصنوعی آمریکایی (AI) ساخته شده است شرکت تحقیق و توسعه با هدف توسعه هوش مصنوعی ایمن به روشی که به نفع بشریت باشد تاسیس شده است. اولین حضور عمومی OpenAI Five در سال 2017 اتفاق افتاد ، جایی که در یک بازی زنده به تنهایی مقابل یک بازیکن حرفه ای بازی معروف به Dendi ، که به آن باخت ، نشان داده شد. سال بعد ، این سیستم تا جایی که به عنوان یک تیم 5 نفره کامل عمل کند ، پیشرفت کرده بود و شروع به بازی در برابر و نشان دادن توانایی شکست تیم های حرفه ای کرد.
   
   
   
   AlphaZero یک برنامه رایانه ای است که توسط شرکت تحقیقاتی هوش مصنوعی DeepMind برای تسلط بر بازی های شطرنج ، شوگی و رفتن ساخته شده است. این الگوریتم از روشی مشابه AlphaGo Zero استفاده می کند.
   
   در تاریخ 5 دسامبر 2017 ، تیم DeepMind با معرفی AlphaZero نسخه اولیه ای را منتشر کرد که در طی 24 ساعت آموزش با شکست دادن برنامه های قهرمان جهان Stockfish ، elmo و نسخه 3 روزه AlphaGo Zero ، به یک بازی فوق بشری در این سه بازی دست یافت. . در هر حالت از واحدهای پردازش تنسور سفارشی (TPU) استفاده شده است که برنامه های Google برای استفاده بهینه شده اند. [1] AlphaZero فقط از طریق "بازی شخصی" با استفاده از 5000 TPU نسل اول برای تولید بازی ها و 64 TPU نسل دوم برای آموزش شبکه های عصبی آموزش داده شد ، همه به طور موازی ، بدون دسترسی به کتاب های باز یا جدول های بازی آخر. پس از چهار ساعت آموزش ، DeepMind تخمین زده بود که AlphaZero در رتبه بندی Elo بالاتر از Stockfish 8 بازی می کند. پس از 9 ساعت آموزش ، الگوریتم Stockfish 8 را در یک تورنمنت 100 بازی با کنترل زمان شکست داد (28 برد ، 0 باخت و 72 تساوی). [1] [2] [3] الگوریتم آموزش دیده بر روی یک ماشین واحد با چهار TPU بازی می کند.
   
   مقاله DeepMind در مورد AlphaZero در ژورنال Science در 7 دسامبر 2018 منتشر شد. [4] در سال 2019 DeepMind مقاله جدیدی را با جزئیات MuZero منتشر کرد ، الگوریتمی جدید که می تواند کار در AlphaZero را با بازی Atari و بازی های رومیزی بدون اطلاع از قوانین یا نمایش های بازی تعمیم دهد. [5]
%
%OpenAI Five is the name of a machine learning project that performs as a team of video game bots playing against human players in the competitive five-on-five video game Dota 2. The system was developed by OpenAI, an American artificial intelligence (AI) research and development company founded with the mission to develop safe AI in a way that benefits humanity. OpenAI Five's first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against a professional player of the game known as Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.
  
  





به طور معمول، مسئله یادگیری تقویتی را با استفاده از مدل سیستم دینامیکی به ویژه فرآیند تصمیم گیری مارکوف مدل سازی می شود. در فصل دوم با فرآیند تصمیم گیری مارکوف و ویژگی های آن بیشتر آشنا خواهیم شد.

واژه یادگیری تقویتی، همزمان به چند معنی اشاره می کند، اول مسئله یادگیری تقویتی، که در ادامه با آن بیشتر آشنا می شویم،  دوم، مجموعه راه حل ها و روش هایی که برای حل این مسئله ارائه شده و به خوبی کار می کنند و سوم، حوزه یادگیری تقویتی که به مطالعه مسئله یادگیری تقویتی و راه حل های آن می پردازد. برای اینکه این چندمعنایی ابهامی ایجاد نکند، از این به بعد وقتی از واژه یادگیری تقویتی استفاده می کنیم، منظورمان  مورد دوم یا سوم است. برای اشاره به مورد اول از عبارت «مسئله یادگیری تقویتی» استفاده خواهیم کرد. 

یک موضوع مهم در یادگیری تقویتی آینده نگری و برنامه ریزی است. ممکن است انتخاب یک عمل، پاداش آنی کمتری داشته باشد اما در طولانی مدت، موجب کسب پاداش های بهتر شود. 

یک روش یادگیری ماشین تعریف می‌شود که مربوط به نحوه اقدام یک عامل هوشمند \LTRfootnote{Agent} در محیط براساس هدفی مشخص است. یادگیری تقویتی عبارت است از یادگیری اینکه عامل هوشمند چه کاری باید انجام دهد(نحوه انتخاب اقدامات برحسب موقعیت) تا به حداکثر پاداش برسد. این روش تمامی‌ مسائل یک عامل هدفمند را صریحا در تعامل با یک محیط نامشخص بررسی می‌کند. به عامل هوشمند گفته نمی‌شود که چه کارهایی را انجام دهد ، اما درعوض باید کشف کند که کدام اقدامات، بیشترین پاداش را به همراه دارد. در جالب ترین و چالش برانگیزترین موارد ، اقدامات ممکن دارد نه تنها بر پاداش فوری بلکه در وضعیت بعدی محیط، و از طریق آن، بر کلیه پاداش های بعدی تأثیر بگذارد. این دو ویژگی (جستجوی آزمون و خطا و پاداش تأخیری) دو ویژگی مهم تمییز دهنده یادگیری تقویتی از روش های متداول یادگیری ماشین هستند.
اقدامات عامل هوشمند، می‌تواند بر وضعیت آینده محیط تأثیر بگذارد
یادگیری تقویتی یک رویکرد محاسباتی برای درک و خودکار کردن یادگیری و تصمیم‌گیری هدفمند است.
یادگیری تقویتی، یادگیری از  طریق تعامل است که چگونه می‌توان برای رسیدن به یک هدف رفتار کرد.
عامل یادگیری تقویتی و محیط، در طی مراحل زمانی گسسته یا پیوسته با یکدیگر تعامل دارند.

یادگیری تقویتی، روشی برای فهم و مدل سازی یادگیری هدف محور و تصمیم گیری است.



\قسمت{تعریف مسئله}
مسئله‌ی \مهم{یادگیری تقویتی} در اصل یک مسئله بهینه سازی است. هدف اصلی مسئله، به حداکثر رساندن پاداشی است که از محیط دریافت می‌شود

تعریف دقیق‌تر این مسئله را در فصل دوم خواهیم دید.

\قسمت{اهمیت موضوع}
یادگیری تقویتی
در بسیاری از رشته ها مانند نظریه بازی، نظریه کنترل، تحقیق در عملیات، نظریه اطلاعات، بهینه سازی مبتنی بر شبیه سازی، سیستم های چند عاملی، هوش انبوه و آمار مورد مطالعه قرار می‌گیرد. در ادبیات تحقیق و کنترل عملیات، یادگیری تقویتی را برنامه ریزی تقریبی پویا  \LTRfootnote{Approximate Dynamic Programming} یا برنامه ریزی عصبی پویا  \LTRfootnote{Neuro-dynamic Programming} می‌نامند. مسائل مورد بررسی در یادگیری تقویتی در نظریه کنترل بهینه \LTRfootnote{Optimal Control Theory} نیز مورد بررسی قرار گرفته است، که بیشتر مربوط به وجود و توصیف راه حل های بهینه و الگوریتم های محاسبه دقیق آنهاست، و کمتر مربوط به یادگیری یا تقریب، به ویژه در غیاب یک مدل ریاضی از محیط. در اقتصاد و نظریه بازی، ممکن است از یادگیری تقویتی برای توضیح چگونگی ایجاد تعادل، استفاده شود.


\subsection{تفاوت بین یادگیری تحت نظارت و یادگیری تقویتی}
یادگیری تحت نظارت ، یادگیری از مجموعه آموزش مثالهای برچسب دار است که توسط یک ناظر بیرونی آگاه ارائه می شود. هدف این نوع یادگیری این است که سیستم پاسخهای خود را برون یابی یا تعمیم دهد تا در موقعیت هایی که در مجموعه آموزش وجود ندارد ، به درستی عمل کند. در مسائل تعاملی ، به دست آوردن نمونه هایی از رفتار مطلوب که هم صحیح باشد و هم نمایانگر همه موقعیت هایی که عامل باید عمل کند، غیر عملی است. در قلمرو ثبت نشده (جایی که انتظار می رود یادگیری مفید باشد) یک عامل باید بتواند از تجربیات خود بیاموزد

\subsection{تفاوت بین یادگیری بدون نظارت و یادگیری تقویتی}
یادگیری بدون نظارت معمولاً یافتن ساختاری است که در مجموعه داده های بدون برچسب پنهان شده است. یادگیری تقویتی به جای تلاش برای یافتن ساختار پنهان در تلاش است تا یک سیگنال پاداش را به حداکثر برساند

\subsection{اکتشاف و بهره برداری}
یکی از چالش هایی که در یادگیری تقویتی برخلاف سایر روش‌های یادگیری وجود دارد ، رقابت بین اکتشاف و بهره برداری است. برای به دست آوردن پاداش زیاد ، یک عامل یادگیری تقویتی باید کارهایی را ترجیح دهد که در گذشته انجام داده و در تولید پاداش موثرتر بوده است. اما برای کشف چنین اعمالی، باید اقداماتی را امتحان کند که قبلاً انتخاب نکرده است. این عامل برای به دست آوردن پاداش مجبور است از آنچه قبلاً تجربه کرده است بهره برداری کند، اما همچنین برای انتخاب اقدامات بهتر در آینده باید به کاوش بپردازد. 
اکتشاف و بهره برداری هیچکدام به تنهایی در رسیدن به هدف، کارا نیست.
\subsection{چنگک k اهرمه}

مسئله یادگیری زیر را در نظر بگیرید. شما به طور مکرر با انتخاب در میان k گزینه ها یا اقدامات مختلف روبرو هستید. بعد از هر انتخاب پاداش عددی دریافت می کنید که از یک احتمال ثابت انتخاب شده است
توزیعی که به عملی که انتخاب کرده اید بستگی دارد. هدف شما به حداکثر رساندن کل مورد انتظار است
در طی یک دوره زمانی پاداش دهید ، به عنوان مثال ، بیش از 1000 انتخاب اقدام یا مراحل زمانی.
این شکل اصلی مشکل چنگک k اهرمه است ، بنابراین به قیاس با دستگاه اسلات نامیده می شود ، یا
"راهزن یک مسلح" ، با این تفاوت که به جای یک اهرم ، k اهرم دارد. هر انتخاب اکشن مانند یک بازی است
از اهرم های دستگاه اسلات ، و جوایز بازدهی برای ضربه زدن به برنده تمام پولها است. از طریق تکرار شده است
گزینه های انتخابی شما باید با تمرکز اقدامات خود بر روی بهترین اهرم ها ، برنده های خود را به حداکثر برسانید.
تشبیه دیگر این است که پزشک بین یک روش درمانی تجربی برای یک سری موارد جدی انتخاب می کند
بیماران بیمار هر عملی انتخاب یک درمان است و هر پاداش بقا یا رفاه است
از بیمار امروزه از اصطلاح "مسئله راهزن" برای تعمیم مسئله استفاده می شود
در بالا توضیح داده شد ، اما در این کتاب از آن استفاده می کنیم تا فقط به این مورد ساده اشاره کنیم.
در مسئله راهزن مسلح k ما ، هر یک از اقدامات k با توجه به آن پاداش مورد انتظار یا متوسطی دارد
آن عمل انتخاب شده است. بگذارید این را ارزش آن عمل بدانیم.
اگر ارزش هر اقدام را می دانستید ، حل مسئله راهزن مسلح به k: پیش پا افتاده است
همیشه عملکردی را با بالاترین مقدار انتخاب می کند. ما فرض می کنیم که شما مقادیر عمل را نمی دانید
با اطمینان ، اگرچه ممکن است تخمین بزنید.

اگر تخمین مقادیر عملکرد را حفظ کنید ، در هر مرحله هر مرحله حداقل یک اقدام وجود دارد
ارزش تخمینی بیشترین است. ما اینها را اقدامات حریصانه می نامیم. وقتی یکی از این اقدامات را انتخاب می کنید ،
ما می گوییم که شما از دانش فعلی خود در مورد ارزش اقدامات استفاده می کنید. اگر به جای شما
یکی از اقدامات غیرمجاز را انتخاب کنید ، سپس می گوییم در حال کاوش هستید ، زیرا این امر شما را قادر می سازد پیشرفت کنید
برآورد شما از ارزش اقدام غیرمنفردانه است. بهره برداری کار درستی است که باید برای به حداکثر رساندن
پاداش مورد انتظار در یک مرحله ، اما اکتشاف ممکن است در دراز مدت پاداش کل بیشتری را به بار آورد.
به عنوان مثال ، فرض کنید ارزش یک عمل حریصانه با قطعیت شناخته شده باشد ، در حالی که چندین عمل دیگر نیز مشخص هستند
تخمین زده می شود تقریبا به همان خوبی اما با عدم اطمینان قابل توجه است. عدم اطمینان به حدی است که حداقل
یکی از این اقدامات دیگر احتمالاً بهتر از عمل حریصانه است ، اما شما نمی دانید کدام یک است
یکی اگر گام های زمانی زیادی پیش رو دارید که می توانید انتخاب های عملی را انجام دهید ، پس بهتر است
اقدامات غیرمجاز را کشف کنید و کشف کنید کدام یک از آنها بهتر از عمل حریص است. پاداش است
در کوتاه مدت ، در حین اکتشاف ، پایین تر ، اما در بلند مدت بیشتر از این است که پس از کشف
اقدامات بهتر ، می توانید چندین بار از آنها بهره برداری کنید. زیرا هم کاوش و هم امکان پذیر نیست
برای بهره برداری با هر انتخاب اقدام واحد ، اغلب به "تعارض" بین اکتشاف و
بهره برداری.
در هر مورد خاص ، بهتر است اکتشاف یا بهره برداری به روش پیچیده ای به دقت بستگی دارد
مقادیر تخمین ها ، عدم قطعیت ها و تعداد مراحل باقی مانده. بسیاری از پیچیده وجود دارد
روشهای متعادل سازی اکتشاف و بهره برداری برای فرمولهای خاص ریاضی راهزن کارمند و مشکلات مربوطه. با این حال ، بیشتر این روشها فرضیات محکمی راجع به
ثابت بودن و دانش قبلی که نقض شده یا تأیید در برنامه ها و غیرممکن است
مسئله یادگیری تقویت کامل که در فصل های بعدی در نظر می گیریم. تضمین های
بهینه بودن یا از دست دادن محدود برای این روشها هنگام فرضیات نظریه آنها چندان راحت نیست
اعمال نمی شود


\subsection{ربات زباله جمع کن}
یک ربات موبایل وظیفه جمع آوری قوطی های نوشابه خالی در آن را دارد
یک محیط اداری دارای سنسورهایی برای تشخیص قوطی ها و بازو و گیرنده ای است که می تواند آنها را انتخاب کند
آنها را در سطل آشغال قرار دهید. با باتری قابل شارژ کار می کند. سیستم کنترل ربات
دارای م components لفه هایی برای تفسیر اطلاعات حسی ، جهت یابی و کنترل بازو و
گیرنده تصمیمات سطح بالا در مورد چگونگی جستجوی قوطی ها توسط یک عامل یادگیری تقویت کننده اتخاذ می شود
بر اساس میزان شارژ فعلی باتری. این نماینده باید تصمیم بگیرد که آیا ربات باید (1)
برای مدت زمان مشخصی به طور فعال به جستجو در یک قوطی بپردازید ، (2) ثابت بمانید و منتظر بمانید تا کسی چه کار کند
آن را قوطی بیاورید ، یا (3) برای شارژ مجدد باتری خود به پایه خانه خود برگردید. این تصمیم باید گرفته شود
به صورت دوره ای یا هر زمان که حوادث خاصی رخ می دهد ، مانند یافتن قوطی خالی. بنابراین نماینده
دارای سه عملکرد است و حالت در درجه اول با توجه به وضعیت باتری تعیین می شود. جوایز ممکن است
بیشتر اوقات صفر باشید ، اما وقتی ربات قوطی خالی یا بزرگ و بزرگ را محکم کرد ، مثبت می شوید
منفی است اگر باتری تمام کار کند. در این مثال ، عامل یادگیری تقویت کننده نیست
کل ربات ایالات که نظارت می کند شرایط موجود در خود ربات را توصیف می کند ، نه شرایط آن را
محیط خارجی ربات بنابراین محیط عامل شامل بقیه ربات است که
ممکن است شامل سایر سیستم های تصمیم گیری پیچیده و همچنین محیط خارجی ربات باشد.



\subsection{کارهای اپیزودیک و ادامه دار}
گاهی اوقات ، تعامل عامل و محیط به طور طبیعی به قسمت هایی تقسیم می شود که آنها را اپیزود می نامند. هر اپیزود، در یک حالت خاص به نام حالت ترمینال به پایان می رسد و به دنبال آن دوباره از یک حالت شروع استاندارد یا یک نمونه از یک توزیع استاندارد از حالت های شروع، آغاز می شود. هر اپیزود به طور مستقل از اپیزود قبلی آغاز می شود.
از طرف دیگر، در بسیاری از موارد تعامل عامل و محیط به طور طبیعی به قسمت های قابل شناسایی تقسیم نمی شود، بلکه به طور مداوم و بدون محدودیت ادامه دارد. ما این کارها را ادامه دار می‌نامیم.


\قسمت{فضای حالت\nf ها و عمل\nf ها}
\قسمت{خط مشی}
\قسمت{سیگنال پاداش}
%[Suttonbook]
%In reinforcement learning, the purpose or goal of the agent is formalized in terms of a special signal,
%called the reward, passing from the environment to the agent. At each time step, the reward is a simple
%number, Rt ∈ R. Informally, the agent’s goal is to maximize the total amount of reward it receives.
%This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly
%state this informal idea as the reward hypothesis

همه ی آنچه به عنوان هدف مدنظرداریم می‌تواند به صورت بیشینه سازی مقدار میانگین یک سیگنال عددی بیان شود.



\قسمت{تابع ارزش}
\قسمت{محیط}
\قسمت{مسئله اکتشاف و بهره برداری}
یکی از چالش هایی که در یادگیری تقویتی برخلاف سایر روش‌های یادگیری وجود دارد ، رقابت بین اکتشاف و بهره برداری است. برای به دست آوردن پاداش زیاد ، یک عامل یادگیری تقویتی باید کارهایی را ترجیح دهد که در گذشته انجام داده و در تولید پاداش موثرتر بوده است. اما برای کشف چنین اعمالی، باید اقداماتی را امتحان کند که قبلاً انتخاب نکرده است. این عامل برای به دست آوردن پاداش مجبور است از آنچه قبلاً تجربه کرده است بهره برداری کند، اما همچنین برای انتخاب اقدامات بهتر در آینده باید به کاوش بپردازد.
اکتشاف و بهره برداری هیچکدام به تنهایی در رسیدن به هدف، کارا نیست.




\قسمت{اهداف تحقیق}


\قسمت{ساختار پایان‌نامه}

این پایان‌نامه شامل --- فصل است. در فصل ...
