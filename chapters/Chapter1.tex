\فصل{مقدمه}

یادگیری ماشین کاربردی از هوش مصنوعی 
\LTRfootnote{Artificial Intelligence (AI)}
است که به کامپیوترها، توانایی یادگیری خودکار از طریق تجربه  را می‌دهد؛ بدون اینکه مستقیما برنامه‌ریزی شوند. حوزه یادگیری ماشین بر توسعه برنامه‌هایی متمرکز است که می‌توانند به داده\nf ها دسترسی پیدا کنند و از آن‌ها برای یادگیری استفاده کنند. فرایند یادگیری با مشاهده داده‌ها یا تجربه مستقیم،  به منظور کشف الگوهای نهفته در داده‌ها آغاز می‌شود.  سپس از الگوهای  استخراج شده برای اتخاذ تصمیمات بهتر در آینده استفاده می‌شود و هدف اصلی این است که به کامپیوترها اجازه دهیم بدون دخالت یا کمک انسان و به طور خودکار یاد بگیرند و اقدامات را بر اساس آن تنظیم کنند
\cite{mldef}.

\section{دسته‌بندی روش‌های یادگیری ماشین}
الگوریتم‌های یادگیری ماشین، به طور کلاسیک به دو دسته کلی 
\textit{یادگیری نظارت شده}
\LTRfootnote{Supervised Learning}
و 
\textit{یادگیری بدون نظارت}
\LTRfootnote{Unsupervised Learning}
تقسیم می‌شوند. بعدها دو دسته دیگر، یعنی 
\textit{یادگیری نیمه نظارت شده}
\LTRfootnote{Semisupervised Learning}
و
\textit{یادگیری تقویتی}
\LTRfootnote{Reinforcement Learning} 
  به این مجموعه اضافه شدند\cite{mldef}. در این پایان‌نامه، تمرکز ما روی مورد آخر، یعنی یادگیری تقویتی است، اما ابتدا بگذارید تعریفی اجمالی از این روش‌ها ارائه دهیم.
\subsubsection{یادگیری نظارت شده}

یکی از مهم‌ترین روش‌های یادگیری ماشین، یادگیری نظارت‌شده است. الگوریتم‌های یادگیری ماشین نظارت ‌شده می‌توانند  با استفاده از داده‌های برچسب‌دار، الگوها را فرابگیرند و سپس آنچه را که آموخته‌اند، برای پیش‌بینی برچسب داده‌های جدید به کار ببندند. آن‌ها با تجزیه و تحلیل مجموعه داده‌های برچسب‌دار، مدلی تقریبی برای پیش‌بینی مقادیر خروجی می‌‌آموزند. این الگوریتم‌ها می‌توانند بر اساس مقایسه خروجی خود با خروجی صحیح، مدل را اصلاح کنند.
%Supervised machine learning algorithms can apply what has been learned in the past to new data using labeled examples to predict future events. Starting from the analysis of a known training dataset, the learning algorithm produces an inferred function to make predictions about the output values. The system is able to provide targets for any new input after sufficient training. The learning algorithm can also compare its output with the correct, intended output and find errors in order to modify the model accordingly.

\subsubsection{یادگیری بدون نظارت}

در مقابل  یادگیری نظارت‌شده، الگوریتم‌های یادگیری ماشینِ بدونِ نظارت قرار دارند. این روش‌ها هنگامی‌استفاده می‌شوند که اطلاعات مورد استفاده برای آموزش نه طبقه‌بندی شده و نه دارای برچسب باشند. در یادگیری بدون نظارت، سیستم‌ها می‌توانند تابعی را برای توصیف ساختار پنهان داده‌های بدون برچسب بیاموزند. این سیستم‌ها خروجی صحیح را کشف نمی‌کنند؛ اما داده‌ها را کاوش می‌کنند و می‌توانند نتایجی را از مجموعه داده‌ها استخراج کنند.

%In contrast, unsupervised machine learning algorithms are used when the information used to train is neither classified nor labeled. Unsupervised learning studies how systems can infer a function to describe a hidden structure from unlabeled data. The system doesn’t figure out the right output, but it explores the data and can draw inferences from datasets to describe hidden structures from unlabeled data.

\subsubsection{یادگیری نیمه نظارت شده}

الگوریتم‌های یادگیری ماشین نیمه نظارت‌شده در جایی بین یادگیری نظارت‌شده و بدون نظارت قرار می‌گیرند. آن‌ها از هر دو داده برچسب‌دار و بدون برچسب، برای آموزش استفاده می‌کنند. داده‌های مورد استفاده، معمولاً شامل مقدار کمی از داده‌های برچسب‌دار و مقدار زیادی از داده‌های بدون برچسب است. معمولاً یادگیری نیمه نظارت‌شده زمانی استفاده می‌شود که یادگیری نظارت‌شده، به دلایل مختلف عملی نباشد. این دلایل می‌تواند کوچک یا ناکافی بودن داده‌های برچسب‌دار و یا پیچیدگی مدل باشد. سیستم‌هایی که از این روش استفاده می‌کنند، می‌توانند به میزان قابل توجهی دقت یادگیری را نسبت به روش نظارت‌شده بهبود بخشند. 
%Semi-supervised machine learning algorithms fall somewhere in between supervised and unsupervised learning, since they use both labeled and unlabeled data for training – typically a small amount of labeled data and a large amount of unlabeled data. The systems that use this method are able to considerably improve learning accuracy. Usually, semi-supervised learning is chosen when the acquired labeled data requires skilled and relevant resources in order to train it / learn from it. Otherwise, acquiring unlabeled data generally doesn’t require additional resources.	

\subsubsection{یادگیری تقویتی}
نظریه یادگیری تقویتی 
\lr{(RL)}،
به تدریج به یکی از فعال\nf ترین حوزه\nf های تحقیقاتی در یادگیری ماشین و هوش مصنوعی تبدیل شده، که ریشه در دیدگاه\nf های روانشناختی و علوم اعصاب درباره رفتار حیوانات و انسان دارد. یادگیری تقویتی تلاش می\nf کند به این سوال پاسخ دهد: چه کار کنیم که بیش\nf ترین پاداش یا کم\nf ترین هزینه نصیبمان شود؟ این که چگونه عوامل هوشمند می\nf توانند کنترل خود را روی محیط، بهینه کنند نیز در حوزه یادگیری تقویتی قرار می\nf گیرد. عوامل هوشمند با کاری دشوار روبرو می شوند: آنها باید مدل\nf های کارآمدی از محیط را با استفاده از ورودی\nf های حسی بدست آورند و از این مدل\nf ها برای تعمیم تجربه گذشته به موقعیت\nf های جدید استفاده کنند. به نظر می\nf رسد انسان و سایر حیوانات، این مشکل را از طریق ترکیب هماهنگ یادگیری تقویتی و سیستم\nf های حسی سلسله مراتبی حل می\nf کنند.
یادگیری تقویتی، یک روش یادگیری است که با الهام از یادگیری انسان و سایر موجودات زنده، شکل گرفته است. یادگیری تقویتی از طریق تعامل یک 
\textit{
\مهم{عامل یادگیری}
}
\LTRfootnote{Learning Agent}
هدفمند،
 با 
\textit{\مهم{محیط}}
\LTRfootnote{Environment}
تعریف می‌شود.  در این روش، عامل با مشاهده و تجربه محیط و آزمون و خطا، می‌‌آموزد و خطاها را کشف و اصلاح می‌کند.  
\textit{\مهم{سینگال پاداش}}
\LTRfootnote{Reward Signal}،
که توسط محیط تولید می‌شود، این امکان را برای عامل فراهم می‌کند که اقدامات خوب و بد را تشخیص دهد.  عامل یادگیری، می‌کوشد تا رفتارهای ایده‌آل، یعنی رفتارهایی که بیشترین پاداش را به همراه دارند را  پیدا کند. \مهم{آزمون و خطا} و \مهم{تأخیر در پاداش}، از مهم‌ترین ویژگی‌های یادگیری تقویتی هستند. در ادامه این پایان‌نامه، به بررسی دقیق‌تر روش‌های یادگیری تقویتی خواهیم پرداخت.
%Reinforcement machine learning algorithms is a learning method that interacts with its environment by producing actions and discovers errors or rewards. Trial and error search and delayed reward are the most relevant characteristics of reinforcement learning. This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance. Simple reward feedback is required for the agent to learn which action is best; this is known as the reinforcement signal.


\section{تاریخچه یادگیری تقویتی}
تاریخچه اولیه یادگیری تقویتی، دو مسیر اصلی دارد که هر دو طولانی و بسیار غنی هستند. یکی از این مسیرها به حوزه \مهم{یادگیری توسط
آزمون و خطا} مربوط می‌شود که پیدایش آن به حوزه روانشناسی یادگیری حیوانات برمی‌گردد. این مسیر در اولین مطالعات مربوط به هوش مصنوعی به وجود آمد و در اواخر دهه 1980 نیز منجر به احیای مجدد هوش مصنوعی شد. مسیر دیگر مربوط به مسئله 
\textit{\مهم{کنترل بهینه}}
\LTRfootnote{Optimal Control}
و حل آن با استفاده از
\textit{
\مهم{تابع ارزش}
}
\LTRfootnote{Value Function}
  و
\textit{\مهم{برنامه‌ریزی پویا}}
\LTRfootnote{Dynamic Programming}
    است. اگر چه این دو مسیر تا حد زیادی مستقل از یکدیگر توسعه یافته‌اند،  اما حول یک موضوع سوم و کمتر مشخص، یعنی روش‌های 
\textit{\مهم{اختلاف زمانی}}
 \LTRfootnote{Temporal Difference} با یکدیگر تلاقی می‌کنند. هر سه رشته در اواخر دهه 1980 گرد‌هم آمدند تا زمینه‌های یادگیری تقویتی مدرن را فراهم کنند
 \cite{suttonbook}.


%The early history of reinforcement learning has two main threads, both long and rich, that were pursued
%independently before intertwining in modern reinforcement learning. One thread concerns learning by
%trial and error that started in the psychology of animal learning. This thread runs through some of
%the earliest work in artificial intelligence and led to the revival of reinforcement learning in the early
%1980s. The other thread concerns the problem of optimal control and its solution using value functions
%and dynamic programming. For the most part, this thread did not involve learning. Although the
%two threads have been largely independent, the exceptions revolve around a third, less distinct thread
%concerning temporal-difference methods such as the one used in the tic-tac-toe example in this chapter.
%All three threads came together in the late 1980s to produce the modern field of reinforcement learning

اصطلاح کنترل بهینه
%\LTRfootnote{Optimal control}
در اواخر دهه 1950
برای توصیف مسئله طراحیِ یک کنترل‌گر به منظور کمینه‌سازی کمیتی مربوط به رفتار یک \textit{سیستم دینامیکی}\LTRfootnote{Dynamical system} در طول زمان، مورد استفاده قرارگرفت. یکی از رویکردهای این مسئله، در اواسط دهه 1950 توسط ریچارد بلمن\LTRfootnote{Richard Bellman} و همکاران از طریق گسترش نظریه قرن نوزدهم همیلتون و جاکوبی
\LTRfootnote{Hamilton and Jacobi}
 توسعه یافت. این رویکرد از مفاهیم 
\textit{\مهم{حالت}}
\LTRfootnote{State}
 یک سیستم دینامیکی و تابع ارزش، برای تعریف یک معادله تابعی استفاده می‌کند؛ که اکنون 
 \textit{
\مهم{معادله بلمن}
 }
  \LTRfootnote {Bellman equation}نامیده می‌شود. مجموعه روش‌های حل مسائل کنترل بهینه به کمک معادله بلمن به عنوان برنامه‌ریزی پویا شناخته می‌شود
  \cite{bellman1957dynamic}.
   همچنین بلمن نسخه گسسته از مسئله کنترل بهینه را که تحت عنوان 
  \textit{
\مهم{فرایندهای تصمیم‌گیری مارکوف}
  } 
  \LTRfootnote{Markov Decision Processes}
  \lr{(MDP)}
شناخته می‌شود، معرفی کرد
\cite{bellman1957markovian}.
 رونالد هوارد (1960) روش 
\textit{
	\مهم{تکرار خط‌مشی}
}
\LTRfootnote{Policy Iteration}
را برای MDP‌‌ ها طراحی کرد
\cite{howard1960dynamic}. 
همه این‌ها عناصر اساسی در نظریه و الگوریتم‌های \textit{یادگیری تقویتی}\LTRfootnote{Reinforcement learning (RL) } مدرن هستند
\cite{suttonbook}.


امروزه یادگیری تقویتی
در بسیاری از رشته‌ها مانند نظریه بازی، نظریه کنترل، تحقیق در عملیات، نظریه اطلاعات، بهینه سازی مبتنی بر شبیه سازی، سیستم‌های چند عاملی، هوش انبوه و آمار مورد مطالعه قرار می‌گیرد. در ادبیات تحقیق و کنترل عملیات، یادگیری تقویتی را برنامه‌ریزی تقریبیِ پویا\LTRfootnote{Approximate Dynamic Programming}
  یا برنامه‌ریزی عصبیِ پویا\LTRfootnote{Neuro-dynamic Programming}
       می‌نامند. مسائل مورد بررسی در یادگیری تقویتی در نظریه کنترل بهینه\LTRfootnote{Optimal Control Theory}
         نیز مورد بررسی قرار گرفته است. بیشتر این مسائل مربوط به وجود و توصیف راه‌حل‌های بهینه و الگوریتم‌های محاسبه دقیق آن‌هاست و کمتر مربوط به یادگیری یا تقریب؛ به ویژه زمانی که یک مدل ریاضی از محیط وجود ندارد. در اقتصاد و نظریه‌بازی، ممکن است از یادگیری تقویتی برای توضیح چگونگی ایجاد تعادل، استفاده شود. در دهه اخیر، الگوریتم‌های مدرن یادگیری تقویتی ژرف، مانند DQN با موفقیت‌های فراوان خود، توانسته‌اند انقلابی در حوزه هوش مصنوعی و یادگیری ماشین به‌وجود آورند. از نمونه‌های این موفقیت‌ها می‌توان به دو پروژه
 \lr{OpenAI Five}
 و
  \lr{AlphaZero}
   اشاره کرد.
  
  \paragraph{\lr{OpenAI Five}}
    
%\lr{OpenAI Five}
   نام یک پروژه یادگیری ماشین متعلق به شرکت 
   \lr{OpenAI} 
   است که به عنوان تیمی از ربات‌های هوش مصنوعی،  در بازی ویدئویی رقابتی پنج به پنج 
\lr{Dota2}
     در مقابل انسان‌ها بازی می‌کند.
\lr{OpenAI} 
 یک شرکت آمریکایی تحقیق و توسعه در زمینه هوش مصنوعی است که با هدف توسعه هوش مصنوعی ایمن به روشی که به نفع بشریت باشد، تاسیس شده است. اولین حضور عمومی
  \lr{OpenAI Five}
  در سال 2017 اتفاق افتاد، جایی که در یک بازی زنده مقابل یک بازیکن حرفه‌ای، معروف به Dendi بازی کرد و او را شکست داد. سال بعد، این سیستم تا جایی که به عنوان یک تیم 5 نفره کامل عمل کند، پیشرفت کرده بود. 
  \lr{OpenAI Five}
  بعد از پشت سر گذاشتن روند آزمایشی، شروع به بازی در برابر تیم‌های حرفه ای کرد و توانایی خود را در شکست دادن آن‌ها به نمایش گذاشت
  \cite{openai}. 
  \lr{Dota2}
  یک بازی ویدیویی چند نفره است که در آن دو تیم، هر یک شامل پنج بازیکن، با یکدیگر رقابت می‌کنند. همکاری تیمی، برنامه ریزی، ارزیابی موقعیت و تصمیم‌گیری از ملزومات اصلی پیروزی در این بازی است.
   
   
   

%
%OpenAI Five is the name of a machine learning project that performs as a team of video game bots playing against human players in the competitive five-on-five video game Dota 2. The system was developed by OpenAI, an American artificial intelligence (AI) research and development company founded with the mission to develop safe AI in a way that benefits humanity. OpenAI Five's first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against a professional player of the game known as Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.
  
  

\paragraph{\lr{AlphaZero}}
یک برنامه رایانه‌ای است که توسط شرکت تحقیقاتی هوش مصنوعی
\lr{DeepMind}
 برای تسلط بر بازی‌های شطرنج، شوگی و 
\lr{Go}
ساخته شده‌است. این الگوریتم، از روشی مشابه
\lr{AlphaGo Zero}
  استفاده می‌کند. در تاریخ 5 دسامبر 2017،  تیم
\lr{DeepMind}
  با معرفی
\lr{AlphaZero}
   نسخه اولیه‌ای را منتشر کرد که در طی 24 ساعت آموزش، با شکست دادن بهترین برنامه‌های جهان در این سه بازی، یعنی
    \lr{Stockfish}
    برای شطرنج،
\lr{elmo}
برای شوگی و نسخه 3 روزه
\lr{AlphaGo Zero}
برای بازی 
\lr{Go}،
به سطحی فوق بشری در این بازی‌ها دست‌یافت.  شرکت 
\lr{DeepMind}
برای اجرای الگوریتم یادگیری
\lr{AlphaZero}
از واحدهای پردازشی تنسور 
\LTRfootnote{Tensor Processing Unit}
یا 
\lr{TPU}
استفاده کرد. واحدهای پردازشی TPU برای اجرای برنامه‌های
\lr{Google}
بهینه شده‌اند
\cite{silver2017}.
\lr{AlphaZero}
  فقط از طریق 
  \مهم{
\textit{بازی با خود}
  }
\LTRfootnote{Self-Play}
و بدون استفاده از دانش بشری در این سه بازی، به سطحی باورنکردنی رسید. الگوریتم بازی با خود، یک روش یادگیری تقویتی برای بازی‌های دو یا چند نفره است که در آن عامل، در مقابل یک کپی از خودش بازی می‌کند و در روند این بازی‌ها یاد می‌گیرد.
شرکت
\lr{DeepMind}
  برای تولید  بازی‌ها از ۵ هزار TPU نسل اول و برای یادگیری شبکه‌های عصبی از ۶۴ TPU نسل دوم استفاده کرده‌است که همه به طور موازی آموزش می‌بینند.
پس از چهار ساعت آموزش،
\lr{DeepMind}
  تخمین زده بود که
\lr{AlphaZero}
    در رتبه بندی
    \lr{Elo}
\footnote{سیستم رتبه بندی
\lr{Elo}
	  روشی برای محاسبه سطح مهارت‌های نسبی بازیکنان در بازی‌های با جمع صفر مانند شطرنج است. نام آن به نام خالق آن
\lr{Arpad Elo}،
استاد فیزیک مجارستانی-آمریکایی برمی‌گردد.}
بالاتر از
\lr{Stockfish8}
  قرار می‌گیرد. پس از 9 ساعت آموزش، توانست
   \lr{Stockfish8}
را در یک مسابقه با 100 بازی و با کنترل زمان، شکست دهد (28 برد، 72 تساوی و بدون باخت) 
\cite{silver2017} \cite{knapton2017entire} \cite{superhuman2017}
درحالی‌که الگوریتم آموزش دیده 
\lr{AlphaZero}
بر روی یک ماشین واحد با چهار TPU بازی می‌کرد.
مقاله
\lr{DeepMind}
  در مورد
\lr{AlphaZero}
    در ژورنال
\lr{Science}
در 7 دسامبر 2018 منتشر شد
\cite{silver2018general}.
در سال 2019،
\lr{DeepMind}
مقاله جدیدی را منتشر کرد و در آن
\lr{MuZero}
را معرفی کرد. 
\lr{MuZero}
نسخه تعمیم یافته 
\lr{AlphaZero}
است که می‌تواند در بازی‌های آتاری و بازی‌های تخته‌ای 
\LTRfootnote{Board-Games}
 بدون اطلاع از قوانین بازی و صرفا با بازی کردن، مهارت کسب کند
 \cite{alphazerowiki}.



\section{یادگیری تقویتی چیست}
یادگیری تقویتی، یک روش یادگیری ماشین است که  به مسئله چگونگی عملکرد یک عامل هوشمند \LTRfootnote{Agent} در محیط، براساس هدفی مشخص می‌پردازد. یادگیری تقویتی عبارت است از یادگیری اینکه عامل، چه اقداماتی باید انجام دهد تا به حداکثر پاداش برسد. به بیان دیگر یادگیری تقویتی یک رویکرد محاسباتی برای درک و خودکار‌سازی یادگیری و تصمیم‌گیری هدفمند است. این روش، به بررسی تمامی‌ مشکلاتی که یک عامل هدفمند، در تعامل با یک محیط نامشخص با آن روبرو می‌شود، می‌پردازد. مثلا:

\شروع{فقرات}

\فقره{به عامل مستقیما گفته نمی‌شود که چه اقداماتی را انجام دهد؛او باید کشف کند که کدام اقدامات بیشترین پاداش را به همراه دارند.}

 
 
\فقره{ممکن است اقدامات عامل، نه تنها بر پاداش فوری بلکه در وضعیت بعدی محیط، و متعاقبا بر کلیه پاداش‌های آینده تأثیر بگذارد.}

\پایان{فقرات}
این دو ویژگی،  یعنی جستجوی آزمون و خطا و پاداش تأخیری ویژگی‌های مهم تمیز دهنده یادگیری تقویتی از روش‌های متداول یادگیری ماشین هستند \cite{suttonbook}.
 عامل یادگیری تقویتی و محیط، در طی مراحل زمانی گسسته یا پیوسته با یکدیگر تعامل دارند. به طور معمول، مسئله یادگیری تقویتی با استفاده از مدل سیستم دینامیکی به ویژه فرآیند تصمیم گیری مارکوف مدل سازی می‌شود. در فصل دوم با فرآیند تصمیم گیری مارکوف و ویژگی‌های آن بیشتر آشنا خواهیم شد.

واژه یادگیری تقویتی، همزمان به چند معنی اشاره می‌کند، اول مسئله یادگیری تقویتی، که صورت دقیق آن را در فصل دو تعریف خواهیم کرد. دوم، مجموعه راه‌حل‌ها و روش‌هایی که برای حل این مسئله ارائه شده و به خوبی کار می‌کنند. سوم، حوزه یادگیری تقویتی که به مطالعه مسئله یادگیری تقویتی و راه‌حل‌های آن می‌پردازد
\cite{suttonbook}.
 برای اینکه این چندمعنایی، ابهامی ایجاد نکند، از این به بعد هرگاه از واژه «یادگیری تقویتی» استفاده می‌کنیم، منظورمان  مورد دوم یا سوم است. برای اشاره به مورد اول از عبارت «مسئله یادگیری تقویتی» استفاده خواهیم کرد. 



\subsection*{تفاوت بین یادگیری تحت نظارت و یادگیری تقویتی}
یادگیری تحت نظارت، یادگیری از طریق مجموعه مثال‌های برچسب‌دار است که توسط یک ناظر بیرونی آگاه ارائه می‌شود. هدف این نوع یادگیری این است که سیستم، پاسخ‌های خود را برون‌یابی کند و یا تعمیم دهد؛ تا در موقعیت‌هایی که در مجموعه آموزش وجود ندارد، به درستی عمل کند. در مسائل تعاملی، به‌دست آوردن نمونه‌هایی از رفتار مطلوب، که هم صحیح باشد و هم نمایانگر تمام موقعیت‌هایی که عامل باید عمل کند، غیر عملی است. در قلمرو ثبت نشده (جایی که انتظار می‌رود یادگیری مفید باشد) یک عامل باید بتواند از تجربیات خود بیاموزد
\cite{suttonbook}.

\subsection*{تفاوت بین یادگیری بدون نظارت و یادگیری تقویتی}
یادگیری بدون نظارت، معمولاً یافتن ساختاری است که در مجموعه داده‌های بدون برچسب، پنهان شده است. یادگیری تقویتی به جای تلاش برای یافتن ساختار پنهان داده‌ها، در تلاش است تا سیگنال پاداش دریافتی از محیط را به حداکثر برساند
\cite{suttonbook}.


\subsection{آینده نگری و برنامه ریزی}
یک موضوع مهم در یادگیری تقویتی، آینده‌نگری است. عامل همواره باید تصمیم بگیرد که از میان اقدامات مختلفی که دراختیار دارد؛ کدام یک را انتخاب کند. ممکن است انتخاب یک عمل، پاداش آنی کمتری داشته باشد اما موجب کسب پاداش‌های بیشتر در آینده شود. در مقابل، عملی که پاداش آنی بیشتری دارد ممکن است در بلند‌مدت، به ضرر عامل تمام شود.  یک بستنی فروش عاقل، بستنی‌های خود را نمی‌خورد. اگرچه با خوردن بستنی، می‌تواند لذت  آنی بیشتری کسب کند، اما با خودداری کردن و مقاومت در برابر این وسوسه، سود بیشتری کسب ‌خواهد کرد و در آینده زندگی مرفه‌تری خواهد داشت.
ارزش یک عمل، تنها به پاداش آنی آن وابسته نیست، بلکه به شرایط بعد از انجام آن عمل و پاداش‌ها یا مجازات‌های بعدی نیز بستگی دارد. برای یک ماهی گرسنه در دل اقیانوس، هیچ چیز بهتر از این نیست که یک طعمه حاضر و آماده و لذیذ، بی حرکت سرراهش سبز شود. ماهی به دنبال کسب پاداش و لذت آنی، دهان خود را باز می‌کند تا طعمه را ببلعد؛ غافل از این‌که خودش طعمه ماهیگیر و قلابش خواهد شد. یک عامل یادگیری تقویتی هوشمند، می‌تواند قلاب‌های ماهیگیری موجود در محیط را تشخیص داده و به دام آن‌ها نیفتد.



\subsection{دینامیک عامل-محیط}
%\قسمت{دینامیک عامل-محیط}
حوزه یادگیری تقویتی  دو بازیگر اصلی دارد؛ عامل و محیط. موجود تصمیم گیرنده و آموزنده را عامل یادگیری، یا به اختصار، عامل می‌نامند. قسمتی که عامل با آن تعامل دارد (هر چیزی خارج از عامل)، محیط نامیده می‌شود. در ادبیات کنترل بهینه، معمولا به جای واژه‌های عامل و محیط، از  کنترل‌کننده 
\LTRfootnote{Controller}
و سیستم کنترل شده \LTRfootnote{Controlled Systen} استفاده می‌شود.
عامل و محیط، به طور مداوم با یکدیگر ارتباط برقرار می‌کنند. عامل، انتخاب می‌کند که چه اقدامی‌ انجام دهد، محیط به این اقدام پاسخ می‌دهد و موقعیت جدیدی را به عامل ارائه می‌دهد.
محیط، همچنین مقادیر عددی ویژه ای به نام \مهم{پاداش} را به عامل برمی‌گرداند؛ که عامل به دنبال بیشینه‌سازی آن است.
\\به طور خاص، عامل و محیط در یک توالی زمانی گسسته تعامل می‌کنند، 
$t = 0,1,2,3,...$،
در هر مرحله $t$، عامل، وضعیت محیط  
$S_t \in \EuScript{S}$
را دریافت می‌کند، و بر اساس آن یک عمل 
$A_t \in \EuScript{A}$
را انتخاب می‌کند. در گام بعدی، عامل به عنوان نتیجه عمل خود، یک پاداش عددی $R_{t+1} \in \EuScript{R}$ دریافت می‌کند و خود را در حالت جدید $S_{t+1}$ می‌یابد.
دینامیک عامل-محیط را می‌توان به شکل یک دنباله از حالت‌ها، عمل‌ها و پاداش‌ها به صورت زیر نمایش داد \cite{suttonbook}
$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3,...$$
\شروع{شکل}
\centerimg{agent-env.png}{10cm}
\شرح{دینامیک عامل-محیط در یادگیری تقویتی}
\برچسب{fig:dynamic}
\پایان{شکل}

%\قسمت{تعریف مسئله}
%مسئله‌ی \مهم{یادگیری تقویتی} در اصل یک مسئله بهینه سازی است. هدف اصلی مسئله، به حداکثر رساندن پاداشی است که از محیط دریافت می‌شود
%
%تعریف دقیق‌تر این مسئله را در فصل دوم خواهیم دید.
\subsection*{اکتشاف و بهره‌برداری}
یکی از چالش‌هایی که در یادگیری تقویتی برخلاف سایر روش‌های یادگیری وجود دارد، رقابت بین اکتشاف و بهره‌برداری است. برای بدست آوردن پاداش زیاد، یک عامل یادگیری تقویتی باید کارهایی را ترجیح دهد که در گذشته انجام داده و در تولید پاداش موثرتر بوده است. اما برای کشف چنین اعمالی، باید اقداماتی را امتحان کند که قبلاً انتخاب نکرده است. این عامل برای بدست آوردن پاداش مجبور است از آنچه قبلاً تجربه کرده است بهره‌برداری کند، اما همچنین برای انتخاب اقدامات بهتر در آینده، باید به کاوش بپردازد. 
اکتشاف و بهره‌برداری هیچکدام به تنهایی در رسیدن به هدف، کارا نیست. مثال زیر، رابطه میان اکتشاف و بهره برداری را روشن‌تر می‌کند.
\subsection{مثال: چنگک چند اهرمه}
مسئله چنگک چند اهرمه یکی از ساده‌ترین مسائل یادگیری تقویتی است که مفهوم اکتشاف و بهره‌برداری را به خوبی توضیح می‌دهد. مسئله زیر را در نظر بگیرید. شما به‌طور مکرر با انتخاب در میان k گزینه یا اقدام مختلف روبرو هستید که از این به بعد به آن‌ها عمل ۱ تا عمل k می‌گوییم. بعد از هر انتخاب، پاداش عددی دریافت می‌کنید. پاداش هر عمل از یک توزیع احتمال مجهول ولی ثابت پیروی می‌کند. ممکن است توزیع احتمال یک عمل با دیگری متفاوت باشد ولی توزیع احتمال‌ها در طول زمان تغییر نمی‌کنند. هدف شما کسب بیشترین پاداش، در یک دوره زمانی طولانی است (مثلا بعد از ۱۰۰۰ بار بازی کردن). 
 این مسئله یک استراتژی بدیهی دارد. هر کدام از توزیع‌ها، یک مقدار میانگین یا امیدریاضی دارند که مقدار پاداشی است که محیط، به طور متوسط به ازای هر بار انتخاب شدن، اهدا می‌کند. فرض کنید امیدریاضی پاداش هر عمل را \مهم{ارزش} آن عمل بنامیم. در این صورت استراتژی بدیهی، انتخاب مکرر عملی است که بیشتری ارزش را دارد. به این عمل، \مهم{عمل بهینه} 
\LTRfootnote{Greedy}
 گفته می‌شود. به عبارت دیگر در این مسئله، یک عمل بهینه وجود دارد و آن عملی است که نسبت به بقیه اعمال، ارزش بیشتری دارد. با انتخاب مکرر این عمل، به طور میانگین بیشترین پاداش را کسب خواهیم کرد. اما نکته مهم در این مسئله این است که ما ارزش هیج یک از عمل‌ها را نمی‌دانیم، بنابراین نمی‌دانیم که عمل بهینه کدام است. اما می‌توانیم با انتخاب اعمال مختلف و مشاهده پاداش‌ها، تخمینی از ارزش هر عمل کسب کنیم. به این کار، \مهم{اکتشاف} گفته می‌شود. فرض کنید تخمین ما از ارزش یک عمل، میانگین پاداش‌هایی است که تابه حال به ما داده‌است. دقت این تخمین، وابسته به دفعاتی است که آن عمل را انتخاب کرده‌ایم. هر چقدر بیشتر عملی را انتخاب و پاداش آن را مشاهده کنیم، تخمین ما از ارزش آن عمل، به ارزش واقعی آن نزدیک‌تر خواهد بود. به بیان دیگر، هر چه بیشتر اکتشاف کنیم، تخمین‌های دقیق‌تری خواهیم داشت.
 %
%این شکل اصلی مشکل چنگک k اهرمه است ، بنابراین به قیاس با دستگاه اسلات نامیده می‌شود ، یا
%"راهزن یک مسلح" ، با این تفاوت که به جای یک اهرم ، k اهرم دارد. هر انتخاب اکشن مانند یک بازی است
%از اهرم‌های دستگاه اسلات ، و جوایز بازدهی برای ضربه زدن به برنده تمام پولها است. از طریق تکرار شده است
%گزینه‌های انتخابی شما باید با تمرکز اقدامات خود بر روی بهترین اهرم‌ها ، برنده‌های خود را به حداکثر برسانید.
%تشبیه دیگر این است که پزشک بین یک روش درمانی تجربی برای یک سری موارد جدی انتخاب می‌کند
%بیماران بیمار هر عملی انتخاب یک درمان است و هر پاداش بقا یا رفاه است
%از بیمار امروزه از اصطلاح "مسئله راهزن" برای تعمیم مسئله استفاده می‌شود
در هر مرحله حداقل یک اقدام وجود دارد
که ارزش تخمینی آن بیشینه است. آن‌ها را اقدامات \مهم{حریصانه} می‌نامیم. وقتی یک اقدام حریصانه را انتخاب می‌کنیم،
درواقع از دانش فعلی خود در مورد ارزش اقدامات، \مهم{بهره‌برداری} می‌کنیم. در عوض اگر،
یکی از اقدامات کم ارزش‌تر را انتخاب کنیم، در حال \مهم{اکتشاف} هستیم، زیرا این کار به ما کمک می ‌کند تا برآوردهای بهتری از ارزش‌ها بدست آوریم. هدف بهره‌برداری، به حداکثر رساندن پاداش چشمداشتی در یک مرحله است، اما اکتشاف ممکن است در دراز مدت پاداش کل بیشتری را به‌بار‌ آورد.
به عنوان مثال، فرض کنید ارزش یک عمل حریصانه با قطعیت شناخته شده‌باشد، در‌حالی‌‌که چندین عمل دیگر نیز وجود دارند که
تقریبا به همان خوبی  هستند، اما در ارزش تخمینی آن‌ها عدم اطمینان قابل توجهی  وجود دارد. عدم اطمینان به حدی است که حداقل
یکی از این اقدامات دیگر احتمالاً بهتر از عمل حریصانه است، اما نمی‌دانیم کدام یک است.
اگر زمان زیادی پیش رو داریم، بهتر است
اقدامات دیگر را اکتشاف کنیم تا بفهمیم کدام یک از آن‌ها بهتر از عمل حریصانه است. با این کار، احتمالا در کوتاه مدت، یعنی در حین اکتشاف، پاداش کمتری کسب می‌کنیم، اما در بلندمدت، پاداش بیشتری دریافت خواهیم کرد، زیرا پس از کشف اقدامات بهتر، می‌توانیم چندین بار از آن‌ها بهره‌برداری کنیم. 
هیچ یک از اکتشاف و بهره‌برداری، به تنهایی نمی‌توانند ما را به بیشترین پاداش چشمداشتی در بلندمدت برسانند، در یک استراتژی مناسب، هر دو آن‌ها باید لحاظ شوند. از آن‌جایی که اکتشاف و بهره‌برداری به صورت هم‌زمان در یک گام امکان‌پذیر نیست، ناچاریم در هر مرحله تصمیم بگیریم که کدام یک را انتخاب کنیم. این چالش، به \مهم{تعارض} بین اکتشاف و بهره‌برداری معروف است.
%در هر مورد خاص ، بهتر است اکتشاف یا بهره برداری به روش پیچیده ای به دقت بستگی دارد
%مقادیر تخمین‌ها ، عدم قطعیت‌ها و تعداد مراحل باقی مانده. بسیاری از پیچیده وجود دارد
%روشهای متعادل سازی اکتشاف و بهره برداری برای فرمولهای خاص ریاضی راهزن کارمند و مشکلات مربوطه. با این حال ، بیشتر این روشها فرضیات محکمی‌راجع به
%ثابت بودن و دانش قبلی که نقض شده یا تأیید در برنامه‌ها و غیرممکن است
%مسئله یادگیری تقویت کامل که در فصل‌های بعدی در نظر می‌گیریم. تضمین‌های
%بهینه بودن یا از دست دادن محدود برای این روشها هنگام فرضیات نظریه آنها چندان راحت نیست
%اعمال نمی‌شود

% در این مثال ، عامل یادگیری تقویتی خود ربات نیست نظارت می‌کند شرایط موجود در خود ربات را توصیف می‌کند ، نه شرایط آن را
%محیط خارجی ربات بنابراین محیط عامل شامل بقیه ربات است که
%ممکن است شامل سایر سیستم‌های تصمیم گیری پیچیده و همچنین محیط خارجی ربات باشد.

%%%%%%%%%% This decision has to be made
%either periodically or whenever certain events occur, such as finding an empty can. The agent therefore
%has three actions, and the state is primarily determined by the state of the battery. The rewards might
%be zero most of the time, but then become positive when the robot secures an empty can, or large and
%negative if the battery runs all the way down. In this example, the reinforcement learning agent is not
%the entire robot. The states it monitors describe conditions within the robot itself, not conditions of the
%robot’s external environment. The agent’s environment therefore includes the rest of the robot, which
%might contain other complex decision-making systems, as well as the robot’s external environment.
%%%%%%%%

\subsection{کارهای اپیزودیک و مستمر}
در بعضی موارد، تعامل عامل و محیط به طور طبیعی به قسمت‌هایی تقسیم می‌شود که آن‌ها را 
\textit{\مهم{اپیزود}}\LTRfootnote{Episode}
می‌نامند. هر اپیزود، در یک حالت خاص، حالت پایانی، به پایان می‌رسد و به دنبال آن دوباره از یک حالت شروعِ استاندارد یا یک نمونه از یک توزیع استاندارد از حالت‌های شروع، آغاز می‌شود. هر اپیزود به طور مستقل از اپیزود قبلی آغاز می‌شود.
از طرف دیگر، در بسیاری از موارد تعامل عامل و محیط به طور طبیعی به قسمت‌های قابل شناسایی تقسیم نمی‌شود، بلکه به طور مداوم و بدون محدودیت ادامه دارد. این قبیل کارها 
\textit{
\مهم{مستمر}
}\LTRfootnote{Continuous}
 نامیده می‌شود.


\section{عناصر اصلی یادگیری تقویتی}
چهار عنصر کلیدی و اصلی مسائل یادگیری تقویتی عبارتند از:
\textit{خط‌مشی}\LTRfootnote{Policy}، 
\textit{سیگنال پاداش}\LTRfootnote{Reward Signal}،
\textit{تابع ارزش}\LTRfootnote{Value Function} 
و
\textit{محیط}\LTRfootnote{Environment}.
\subsection{خط‌مشی}
خط‌مشی\LTRfootnote{Policy} 
نحوه رفتار عاملِ یادگیری را در یک زمان خاص، مشخص می‌کند و هسته اصلی رفتار یک عامل یادگیریِ تقویتی است. به بیان دیگر، خط‌مشی، نگاشتی از حالت‌های محیط به عملی است که باید در آن حالت انجام شود. خط‌مشی به تنهایی برای تعیین رفتار عامل کافی است. ممکن است
خط‌مشی عامل یادگیری، یک تابع ساده یا جدول جستجو باشد و یا ممکن است شامل محاسبات پیچیده‌ای مانند فرآیند جستجو باشد؛ همچنین
خط‌مشی‌ها ممکن است احتمالاتی باشند، یعنی برای هر حالت، یک توزیع احتمال از عمل‌های مجاز در آن حالت ارائه شود
\cite{suttonbook}.


\subsection{سیگنال پاداش}

همهٔ آنچه به عنوان هدف مدنظرداریم می‌تواند به صورت بیشینه‌سازی  یک سیگنال عددی بیان شود. در مسئله یادگیریِ تقویتی، تصمیمات عاملِ یادگیری، توسط سیگنال پاداش جهت‌دهی می‌شود. در هر گام، محیط، یک عدد حقیقی به عنوان پاداش برای عامل یادگیریِ تقویتی ارسال می‌کند. تنها هدف عامل، به حداکثر رساندن کل پاداش دریافتی از محیط در طولانی مدت است. سیگنال پاداش اتفاقات خوب و بد را برای عامل مشخص می‌کند و مبنای اصلی تغییر خط‌مشی است
سیگنال پاداش می‌تواند تابعی تصادفی از وضعیت محیط و عمل انجام شده باشد
\cite{suttonbook}.

%In reinforcement learning, the purpose or goal of the agent is formalized in terms of a special signal,
%called the reward, passing from the environment to the agent. At each time step, the reward is a simple
%number, Rt ∈ R. Informally, the agent’s goal is to maximize the total amount of reward it receives.
%This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly
%state this informal idea as the reward hypothesis

\subsection{تابع ارزش}
همان‌طور که سیگنال پاداش نشان می‌دهد که انجام چه عملی در هر گام خوب است، تابع ارزش مشخص می‌کند که کدام عمل در طولانی مدت بهتر است. در واقع تابع ارزش نشانگر مطلوبیت طولانی مدت حالت‌ها با در نظر گرفتن حالت‌هایی است که در پی خواهند داشت.
\\پاداش‌ها به یک معنا اولیه هستند، در حالی که ارزش‌ها، به عنوان پیش‌بینی پاداش‌ها، ثانویه هستند. بدون پاداش هیچ ارزشی وجود ندارد و تنها هدفِ تخمین ارزش‌ها، دستیابی به پاداش بیشتر است. با این وجود، این تابع ارزش است که هنگام تصمیم‌گیری و ارزیابی به آن توجه می‌کنیم.
تعیین ارزش،‌ بسیار دشوارتر از تعیین پاداش است.
ارزش‌ها باید از توالی مشاهداتی که یک عامل در طول عمر خود انجام می‌دهد، برآورد شوند
\cite{suttonbook}.
مهمترین مولفه بیشتر الگوریتم‌های یادگیریِ تقویتی که در  فصل‌های آینده معرفی خواهیم کرد، روشی برای تخمین کارآمد تابع ارزش است.

\subsection{مدل محیط}

\قسمت{روش‌های مبتنی بر مدل و بدون مدل}
یکی از مهم‌ترین نقاط انشعاب در الگوریتم‌‌های یادگیری تقویتی این است که آیا عامل به یک مدل از محیط دسترسی دارد یا  توانایی آموختن مدلی از محیط را دارد یا خیر؟ منظور از مدل محیط، مدلی ریاضی است که رفتار محیط را تقلید می‌کند، یا به طور کلی‌تر، اجازه می‌دهد تا در مورد نحوه رفتار محیط، پیشبینی کارآمدی داشته‌باشیم. از مدل‌ها برای برنامه‌ریزی و انتخاب در روند تصمیم‌گیری  با در نظر گرفتن شرایط احتمالی آینده بدون تجربه واقعی آن‌ها استفاده می‌شود.
نقطه قوت داشتن مدل این است که به عامل اجازه می‌دهد با برآورد قبلی، طیف وسیعی از گزینه‌های ممکن را پیش‌بینی کند و پیشاپیش در مورد گزینه‌های خود تصمیم بگیرد. یک نمونه مشهور از روش‌های مبتنی بر مدل، 
\lr{AlphaZero}
 است. در عمل، چنانچه دستیابی به مدلی از محیط امکان‌پذیر و عملی باشد، معمولا از روش‌های مبتنی بر مدل استفاده می‌شود. زیرا می‌تواند باعث بهبود قابل توجه‌ای در روند یادگیری نسبت به روش‌های بدون مدل شود. اصلی‌ترین نقطه ضعف این روش‌ها این است که معمولا یک مدل کامل از محیط  در دسترس عامل نیست و باید کاملاً از طریق تجربه آموخته ‌شود. الگوریتم‌هایی که از مدل استفاده می‌کنند، روش‌های مبتنی بر مدل و آن‌هایی که از مدلی استفاده نمی‌کنند، \textit{بدون مدل} نامیده می‌شوند. روش‌های بدون مدل از دستاوردهای بالقوه روش‌های مبتنی بر مدل چشم‌پوشی می‌کنند‌، اما پیاده‌سازی و تنظیم آن‌ها آسان‌تر است. به همین خاطر، روش‌های بدون مدل از محبوبیت بیشتری برخوردار بوده و به طور گسترده‌تری توسعه و آزمایش شده‌اند.  معمولا روند یادگیری روش‌های بدون مدل، بر اساس تجربه واقعی در محیط و آزمون و خطا است \cite{suttonbook}. در این پایان‌نامه تمرکز ما بر روش‌های بدون مدل خواهد بود.



%\قسمت{فضای حالت\nf‌ها و عمل\nf‌ها}

\قسمت{ساختار پایان‌نامه}

این پایان‌نامه شامل 4 فصل است. در فصل ...
