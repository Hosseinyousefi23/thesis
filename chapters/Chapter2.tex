
\فصل{مفاهیم اولیه یادگیری تقویتی}
  در این فصل، با فرایندهای تصمیم‌گیری مارکوف آشنا خواهیم‌شد و چهار عنصر اصلی یادگیری تقویتی، یعنی \textit{خط‌مشی}\LTRfootnote{Policy}، \textit{سیگنال پاداش}\LTRfootnote{Reward signal}، تابع ارزش و محیط\LTRfootnote{Environment} را دقیقا تعریف خواهیم‌کرد. همچنین برخی روش‌های کلاسیک در یادگیری تقویتی را معرفی می‌کنیم.

%----------------------------- مقدمه ----------------------------------


\section{فرایند تصمیم‌گیری مارکوف}

یادگیری تقویتی از چارچوب رسمی \textit{فرایندهای تصمیم‌گیری مارکوف} 
\LTRfootnote{Marcov Decision Process} (MDP)
 برای تعریف تعامل بین یک عامل یادگیری و محیط توسط حالت‌ها، اقدامات و پاداش استفاده می‌کند. مدل MDP یک مدل کلاسیک از تصمیم‌گیری متوالی است، جایی که اقدامات نه تنها بر پاداش‌های فوری، بلکه بر موقعیت‌ها و حالت‌های بعدی و به تبع آن بر پاداش‌های آینده تأثیر می‌گذارد.
MDP
 یک فرم ایده‌آل ریاضی از مسئله یادگیری تقویتی است که برای آن تئوری‌های دقیقی بیان شده‌است.
MDP متناهی،
 یک MDP با مجموعه حالت‌های متناهی است. 
بیشتر نظریه‌های فعلی یادگیری تقویتی، محدود به MDP  های متناهی است، اما روش‌ها و ایده‌ها به طور کلی بیان می‌شوند.


%[Szepesvari2010]
\شروع{تعریف}[فرایند تصمیم‌گیری مارکوف] 
 فرایند تصمیم‌گیری مارکوف، یک 4 تایی 
$\EuScript{M} = \seq{\EuScript{S},\EuScript{A},\EuScript{R},\EuScript{P}}$
است که
\شروع{فقرات}
\فقره{$\EuScript{S}$ بیانگر مجموعه تمام \مهم{حالت‌ها}ست،}
\فقره{$\EuScript{A}$ بیانگر مجموعه تمام \مهم{عمل‌}\nf‌ هاست،}
\فقره{
	$\EuScript{R} \subseteq \IR$
	بیانگر مجموعه \مهم{پاداش}‌هاست،
}
\فقره{ 
$\EuScript{P}$
\مهم{هسته احتمال انتقال\LTRfootnote 
	{Transition Probability Kernel}} ،
$ \EuScript{P}: \EuScript{S} \times \EuScript{A} \to \Pi(\EuScript{S} \times \EuScript{R})$، 
تابعی است که دینامیک MDP را مشخص می‌کند.
}

\پایان{فقرات}

\پایان{تعریف}
هسته احتمال انتقال یا تابع انتقال $\EuScript{P}$، هر دوتایی حالت-عمل
$(s,a)$ که 
$s \in \EuScript{S}$
و
$a \in \EuScript{A}$، را به یک توزیع احتمال روی دوتایی‌هایی به شکل 
$(s',r)$
نسبت می‌دهد که $s'$ بیانگر حالت بعدی و $r$ بیانگر پاداش این انتقال است. به ازای هر دو حالت 
$s,s' \in \EuScript{S}$
 و هر عمل 
 $a \in \EuScript{A}$
  و هر پاداش 
  $r \in \EuScript{R}$
  احتمال رسیدن به حالت $s'$ و دریافت پاداش $r$ با انتخاب عمل $a$ در حالت $s$، یک عدد حقیقی متعلق به  بازه $[0,1]$ است که آن را به شکل
$p(s',r|s,a)$
نمایش می‌دهیم:
$$p(s',r|s,a) \triangleq Pr\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\}.$$ 
نامگذاری فرایند تصمیم‌گیری مارکوف اشاره به این موضوع دارد که این سیستم‌ها دارای \مهم{ویژگی مارکوف \LTRfootnote{Marcov Property}}  هستند، بدین معنا که تابع انتقال آن تنها به حالت فعلی سیستم و آخرین عمل انجام شده وابسته است، و نسبت به حالت‌ها و اعمال قبلی مستقل است.

\section{یک مثال ساده: ربات قوطی یاب}
یک ربات قابل حمل، وظیفه جمع‌آوری قوطی‌های نوشابه خالی در 
یک محیط اداری دارد. این ربات دارای سنسورهایی برای تشخیص قوطی‌ها است و بازو و گیره‌ای دارد که می‌تواند آن\nf ها را بردارد و در سطل آشغال قرار دهد. این ربات  با باتری قابل شارژ کار می‌کند. سیستم کنترلی ربات
دارای مولفه‌هایی برای تفسیر اطلاعات حسی، جهت یابی و کنترل بازو و گیره است. تصمیم در مورد چگونگی جستجوی قوطی‌ها توسط یک عامل یادگیری تقویتی بر اساس میزان شارژ باتری اتخاذ می‌شود. این عامل باید تصمیم بگیرد که: 
\begin{itemize}
	\item  ربات باید برای مدت زمان مشخصی به طور فعال به جستجوی  یک قوطی بپردازد؟
	\item ثابت باشد و منتظر کسی بماند که قوطی را برایش بیاورد!
	\item  برای شارژ مجدد باتری به جایگاه اختصاصی‌اش  برود!
\end{itemize}
این تصمیم‌ها نیز مانند یافتن قوطی خالی باید به صورت مکرر و در مواقع خاص گرفته شود. مواقع تصمیم گیری توسط رویدادهای بیرونی یا سایر قسمت\nf های سیستم کنترلی ربات تعیین می\nf شود. بنابراین عامل
دارای سه عمل است و با توجه به وضعیت باتری تصمیم‌گیری می\nf کند. ممکن است در ابتدا پاداشی در کار نباشد ولی با یافتن هر قوطی خالی عامل پاداش مثبتی دریافت می\nf کند. اگر باتری ربات، قبل از  اینکه به جایگاه شارژ مجدد برسد، تمام شود، پاداش منفی و بزرگی نیز بعنوان مجازات تمام شدن باتری به عامل داده می\nf شود.

تعامل بین این عامل یادگیری و محیط، می\nf تواند به شکل یک فرآیند تصمیم گیری مارکوف ساده با دو حالت سطح انرژی \مهم{بالا} و سطح انرژی \مهم{پایین}، 
${\EuScript{S} = \{High, Low\}}$ 
و سه عمل \مهم{جستجو} برای یافتن قوطی، \مهم{منتظر ماندن} برای جلوگیری از اتلاف انرژی و \مهم{بازگشت} به جایگاه شارژ مجدد 
$\EuScript{A}=\{Search, Wait, Recharge\}$
بیان شود. بهترین راه برای یافتن قوطی\nf ها، جستجو کردن آن\nf هاست، اما عملیات جستجو موجب مصرف باتری می\nf شود و ریسک تمام شدن باتری در حین جستجو وجود دارد. در عوض، منتظر ماندن انرژی چندانی مصرف نمی کند، اما شانس پیدا کردن قوطی را نیز کاهش می\nf دهد و از این رو، پاداش کمتری به همراه دارد. درصورت خالی شدن باتری، ربات باید توسط متصدی شرکت، به جایگاه شارژ مجدد منتقل شود. این اتفاق، مطلوب نیست و در صورت وقوع، پاداش منفی برای عامل به همراه دارد. 
اگر سطح انرژی عامل، بالا باشد، عامل همیشه می تواند یک گام، جستجو انجام دهد بدون اینکه خطر تخلیه شدن باتری وجود داشته باشد.
پس از یک گام جستجو که با سطح انرژی بالا شروع شود، عامل با احتمال
 $\alpha$
 در سطح بالا باقی می ماند و با احتمال 
 $1-\alpha$
 به سطح انرژی پایین منتقل می شود. از سوی دیگر یک دوره
جستجوی انجام شده هنگامی که سطح انرژی پایین است، آن را با احتمال $\beta$ در سطح پایین نگاه می\nf دارد ولی خطر تخلیه باتری با احتمال $1-\beta$ وجود دارد. در حالت دوم، ربات و باتری باید نجات یابند و سپس باتری دوباره شارژ می شود و به سطح انرژی بالا بازمی گردد. 
   همچنین فرض کنید پاداش پیدا کردن هر قوطی، یک واحد باشد. اگر ربات با هر عمل جستجوی موفق به طور میانگین $r_{search}$ قوطی پیدا کند و هر عمل منتظر ماندن به طور میانگین $r_{wait}$ قوطی به همراه داشته باشد که 
      $r_{search} > r_{wait}$؛
 پاداش چشمداشتی اعمال جستجو (اگر موفقیت آمیز باشد)  و منتظرماندن به ترتیب برابر $r_{search}$  و 
$r_{wait}$
    خواهد بود. همچنین پاداش چشمداشتی عمل 
   \lr{Recharge}
   
   برابر صفر و پاداش تخلیه باتری 
   $-3$
است. جدول 
\ref{tab:recycling}
 احتمالات انتقال و پاداش های چشمداشتی در این فرآیند تصمیم گیری مارکوف را نشان می\nf دهد.
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

%\شرح{عملگرهای مقایسه‌ای}
%\برچسب{جدول:عملگرهای مقایسه‌ای}

\begin{table}[]
	\centering
	\lr{
	\begin{tabular}{@{}ccc|c|c@{}}
		$s$    & $a$        & $s'$   & $p(s'|s,a)$ & $\mathbb{E}[r(s,a,s')]$  \\ \midrule
		$high$ & $search$   & $high$ & $\alpha$    & $r_{search}$ \\
		$high$ & $search$   & $low$  & $1- \alpha$  & $r_{search}$ \\
		$low$  & $search$   & $low$  & $\beta$     & $r_{search}$ \\
		$low$  & $search$   & $high$ & $1-\beta$   & $-3$         \\
		$high$ & $wait$     & $high$ & $1$         & $r_{wait}$   \\
		$high$ & $wait$     & $low$  & $0$         & $r_{wait}$   \\
		$low$  & $wait$     & $low$  & $1$         & $r_{wait}$   \\
		$low$  & $wait$     & $high$ & $0$         & $r_{wait}$   \\
		$high$  & $recharge$ & $high$ & $1$         & $0$          \\
		$high$  & $recharge$ & $low$ & $0$         & $0$          \\
		$low$  & $recharge$ & $high$ & $1$         & $0$          \\
		$low$  & $recharge$ & $low$  & $0$         & $0$         
	\end{tabular}
	}
	\شرح{جدول توزیع انتقال و پاداش های چشمداشتی فرآیند تصمیم گیری مارکوف مربوط به ربات قوطی\nf یاب}
\برچسب{tab:recycling}
\end{table}

\شروع{شکل}[]
\centerimg{recycle.png}{10cm}
\شرح{گراف انتقال فرآیند تصمیم\nf گیری مارکوف مربوط به ربات قوطی\nf یاب}
\برچسب{fig:recycle}
\پایان{شکل}


\section{عناصر اصلی یادگیری تقویتی}
چهار عنصر کلیدی و اصلی مسائل یادگیری تقویتی عبارت است از:
\textit{خط‌مشی} \LTRfootnote{Policy}، 
\textit{سیگنال پاداش} \LTRfootnote{Reward Signal}،
\textit{تابع ارزش} \LTRfootnote{Value Function} 
و
\textit{محیط} \LTRfootnote{Environment}
\subsection{خط‌مشی}

\شروع{تعریف}[خط\nf مشی احتمالاتی ثابت]
%[Szepesvari2010]
یک خط\nf مشی احتمالاتی ثابت\LTRfootnote{Stationary probabilistic policy} (یا به طور خلاصه خط\nf مشی ثابت) 
$\pi: S \to \Pi(A)$
تابعی است که هر حالت را به یک توزیع احتمال روی فضای عمل
$\EuScript{A}$
 می‌نگارد.
برای سادگی، احتمال انتخاب عمل $a$ در حالت $s$ تحت خط‌مشی $\pi$ را به شکل
$\pi(a|s)$
نشان می\nf دهیم.
اگر به ازای هر حالت $s$، عمل $a$ موجود باشد به طوری که 
$\pi(a|s) = 1$
در این صورت به $\pi$،
\textit{خط مشی قطعی}
\LTRfootnote{Deterministic Policy}
گفته می‌شود. مرسوم است که برای خط‌مشی‌های قطعی به جای اینکه بنویسیم 
$\pi(a|s) = 1$،
می نویسیم
$\pi(s) = a$

\پایان{تعریف} 

\شروع{تعریف}
می‌گوییم خط‌مشی $\pi$ در یک
 MDP
  \textit{دنبال می
  	\nf
  	 شود}
   هرگاه
$$A_t \sim \pi(. |S_t),	 \quad t \in \IN.$$

\پایان{تعریف}



\subsection{عایدی و تابع ارزش}

ارزش حالت $s$، تحت خط‌مشی $\pi$، یا
 $v_\pi(s)$،
مجموع میزان پاداشی است که عامل، با شروع از $s$ و دنبال کردن خط‌مشی $\pi$، می‌تواند انتظار داشته باشد در آینده کسب کند.

\شروع{تعریف}
\textit{عایدی}
\LTRfootnote{Future Discounted Return}
 یا به اختصار، عایدی \LTRfootnote{Return}، در زمان $t$ به شکل

$$G_t \triangleq \sum_{t'=t}^{T} \gamma^{t'-t} R_{t'}$$
تعریف می‌شود که $T$ زمانی است که اپیزود به اتمام می‌رسد. اگر مسئله مستمر باشد آنگاه 
$T=\infty$
\پایان{تعریف}

از تعریف بالا نتیجه می‌شود 
\begin{align}
G_t =& R_t + \sum_{t'=t+1}^{T} \gamma^{t'-t} R_{t'} \nonumber \\
=& R_t + \gamma \sum_{t'=t+1}^{T} \gamma^{t' - (t+1)} r_{t'} \nonumber \\
=& R_t + \gamma G_{t+1}. \numberthis
\end{align}

\شروع{تعریف}[تابع ارزش حالت]

 ارزش حالت $s$ تحت خط‌مشی $\pi$ یا $v_\pi(s)$ به شکل امیدریاضی عایدی، با شروع از $s$ و دنبال کردن خط‌مشی $\pi$ تعریف می‌شود.
$$v_\pi(s) \triangleq \mathbb{E}_\pi\left[G_t| S_t=s\right] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t =s \right]$$
تابع $v_\pi$ را 
\textit{تابع ارزش حالت}
\LTRfootnote{State Value Function} مربوط به خط‌مشی 
$\pi$
می‌نامیم. اگر $s$ یک حالت نهایی باشد آنگاه 
$v_\pi(s) = 0$

\label{statevaluedef}
\پایان{تعریف}

\شروع{تعریف}[تابع ارزش عمل]

ارزش عمل  $a$ در حالت
$s$
 تحت خط‌مشی $\pi$ یا 
 $q_\pi(s,a)$
  به شکل امیدریاضی عایدی، با شروع از $s$ و انتخاب عمل $a$  و سپس دنبال کردن خط‌مشی $\pi$ تعریف می‌شود.
$$q_\pi(s,a) = \mathbb{E}_\pi\left[G_t| S_t=s, A_t=a\right] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t =s, A_t=a \right]$$
تابع $q_\pi$ را 
\textit{تابع ارزش عمل}
\LTRfootnote{Action Value Function} مربوط به خط‌مشی 
$\pi$
می‌نامیم.
\پایان{تعریف}
\section{برنامه‌ریزی پویا}
در بخش قبل، با مفاهیم کلیدی یادگیری تقویتی آشنا شدیم. در این بخش به یکی از مهم ترین روش‌های کلاسیک در حل مسئله یادگیری تقویتی، یعنی 
\textit{برنامه ریزی پویا}
\LTRfootnote{ (DP)Dynamic Programming}خواهیم پرداخت. برنامه‌ریزی پویا
 مجموعه‌ای از الگوریتم‌ها است در صورت وجود مدل کاملی از محیط در قالب یک 
MDP
 می‌توانند برای محاسبه بهترین خط‌مشی (خط‌مشی‌ای که بیشترین عایدی را به دست می‌دهد) استفاده شوند.
الگوریتم‌های کلاسیک برنامه‌ریزی پویا به دلیل فرض
مدل کاملی از محیط و همچنین هزینه محاسباتی زیادشان، به لحاظ عملی چندان قابل استفاده نیستند اما به لحاظ نظری مهم هستند. قبل از آن که به روش‌های برنامه ریزی پویا بپردازیم، لازم است با مفهوم خط‌مشی بهینه و تابع ارزش بهینه آشنا شویم.
\subsection{خط‌مشی و تابع ارزش بهینه}
حل کردن مسئله یادگیری تقویتی، به معنی پیدا کردن خط\nf مشی ای است که بیشترین پاداش را در طول زمان موجب می‌شود.	به چنین خط\nf مشی ای، 
\textit{خط\nf مشی بهینه} 
\LTRfootnote{Optimal Policy}
گفته می‌شود. برای
 MDP‌ 
 های متناهی، می‌توانیم خط‌مشی بهینه را به صورت زیر تعریف کنیم:
\شروع{تعریف}
می‌گوییم خط‌مشی $\pi$ \مهم{بهتر یا مساوی} خط‌مشی 
$\pi'$
است یا
$\pi \ge \pi'$
هرگاه برای هر 
$s \in \mathcal{S}$

$$v_\pi(s) \ge v_{\pi'}(s).$$
\پایان{تعریف} می\nf توان نشان داد که برای هر MDP متناهی، حداقل یک خط\nf مشی وجود دارد که بهتر یا مساوی هر خط\nf مشی دیگری باشد
\cite{suttonbook}.
 به چنین خط\nf مشی\nf ای \مهم{خط\nf مشی بهینه} گفته می\nf  شود. ممکن است بیش از یک خط\nf مشی بهینه وجود داشته باشد. تمام خط\nf مشی\nf‌ های بهینه را با نماد $\pi_*$  نمایش می\nf دهیم. تابع ارزش متناظر با همه خط\nf مشی\nf‌ های بهینه یکسان است و برابر با 
\textit{تابع ارزش بهینه}
\LTRfootnote{Optimal Value Function}
 است که با نماد $v_*$ نمایش داده شده و به شکل زیر تعریف می\nf شود:
$$v_*(s) \triangleq \max_{\pi} v_\pi(s).$$
همچنین تمام خط\nf مشی\nf ‌های بهینه تابع ارزشِ عمل مشترکی دارند که آن را با نماد $q_*$ نمایش می\nf دهیم و به شکل زیر تعریف می\nf کنیم:
$$q_*(s,a) \triangleq max_{\pi} q_\pi(s,a).$$
می\nf توانیم $q_*$ را برحسب $v_*$ به شکل زیر بنویسیم
\begin{equation}
q_* (s,a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1})| S_t=s, A_t=a].
%\label{•}
\end{equation}

\subsection{معادله بلمن}

ویژگی اساسی توابع ارزشی که در برنامه‌ریزی پویا استفاده می‌شوند،  این است که آن‌ها در یک رابطه بازگشتی موسوم به 
\textit{معادله بلمن} 
\LTRfootnote{Bellman Equation}
صدق می\nf کنند. معادله بلمن رابطه‌ای بین ارزش یک حالت و ارزش‌ حالت‌های بعدی آن را بیان می‌کند.

فرض کنید $\pi$ یک خط‌مشی دلخواه باشد و
 $s \in \EuScript{S}$.
  با استفاده مستقیم از تعریف
 \ref{statevaluedef}
 داریم
\begin{align}
v_\pi (s) \triangleq& \mathbb{E}_\pi [G_t | S_t = s] \nonumber \\
=& \mathbb{E}_\pi [R_{t+1}+ \gamma G_{t+1}|S_t=s] \nonumber \\
=&\sum_{a} \pi(a|s) \sum_{s'}\sum_{r} p(s',r|s,a)[r+\gamma \mathbb{E}_\pi[G_{t+1}|S_{t+1}=s']] \nonumber\\
=&\sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a)[r+\gamma v_\pi(s')] %\quad \forall s \in \mathbb(S) \numerhis 
\label{bellman}.
\end{align}

به رابطه 
 \ref{bellman}
\مهم{معادله بلمن} گفته می\nf شود. این معادله، رابطه ای بین ارزش یک حالت و ارزش حالت‌های بعدی آن را مشخص می‌کند. مشابه این رابطه را می‌توان برای تابع ارزش عمل نیز تحقیق کرد. فرض کنید 
$s \in \EuScript{S}$
 و 
 $a \in \EuScript{A}$.
 داریم
 
\begin{align}
q_\pi (s,a) \triangleq& \mathbb{E}_\pi [G_t | S_t = s, A_t = a] \nonumber \\
=& \mathbb{E}_\pi [R_{t+1}+ \gamma G_{t+1}|S_t=s, A_t = a] \nonumber \\
=&\sum_{s',r} p(s',r|s,a) \sum_{a'} \pi(a'|s')[r+\gamma \mathbb{E}_\pi[G_{t+1}|S_{t+1}=s', A_{t+1} = a']] \nonumber\\
=&\sum_{s',r}  p(s',r|s,a) \sum_{a'} \pi(a'|s')[r+\gamma q_\pi(s',a')] \quad \forall s \in \mathbb(S) \numerhis 
\label{bellmanaction}
\end{align}

رابطه \ref{bellmanaction} به معادله بلمن مربوط به تابع ارزش عمل معروف است.
\subsection{بهینگی و معادله بهینگی بلمن}
%Because v∗ is the value function for a policy, it must satisfy the self-consistency condition given by
%the Bellman equation for state values (3.14). Because it is the optimal value function, however, v∗’s
%consistency condition can be written in a special form without reference to any specific policy. This is
%the Bellman equation for v∗, or the Bellman optimality equation. Intuitively, the Bellman optimality
%equation expresses the fact that the value of a state under an optimal policy must equal the expected
%return for the best action from that state

از آنجا که $v_*$ تابع ارزش یک خط‌مشی است، بنابراین باید در شرایط معادله \ref{bellman} صدق کند.
در این حالت خاص، معادله بلمن را می‌توان به فرم ویژه ای نوشت که با نام
 \textit{معادله بهینگی بلمن}
 \LTRfootnote{Bellman Optimality Equation}
شناخته می‌شود.
\begin{align}
  v_{*}(s)= &\max_{a \in \EuScript{A}(s)} q_{\pi_*}(s,a) \nonumber \\
      =& \max_{a} \mathbb{E}_{\pi_*} [G_t|S_t=s, A_t=a] \nonumber \\ 
      =& \max_{a} \mathbb{E}_{\pi_*} [R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a] \nonumber \\ 
      =& \max_{a} \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t= s, A_t = a] \nonumber \\
      =& \max_{a} \sum_{s',r} p(s',r|s,a) [r + \gamma v_*(s')]. \numberthis
\label{eq:bellman-opti}
\end{align}
معادله 
\ref{eq:bellman-opti}
 بیانگر این واقعیت است که ارزش یک حالت تحت یک خط\nf مشی بهینه، برابر با امیدریاضی عایدی، برای بهترین عمل از آن حالت است. به طور مشابه برای تابع ارزش عمل بهینه، معادله بهینگی بلمن به شکل زیر خواهد بود:
\begin{align}
q_*(s,a) =& \mathbb{E}\left[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1},a')| S_t=s, A_t=a \right] \nonumber\\
=& \sum_{s',r} p(s',r|s,a) \left[r + \gamma \max_{a'} q_*(s',a')\right] 
\label{eq:2}.
\end{align}

\subsection{مثال: معادله بهینگی بلمن برای ربات قوطی یاب}
با استفاده از 
\ref{eq:bellman-opti}
می\nf توانیم معادله بهینگی بلمن را برای مثال ربات قوطی\nf یاب، ارائه دهیم. همانطور که دیدیم، فرآیند تصمیم گیری مارکوف ارائه شده برای این مثال، شامل حالت\nf های 
$\EuScript{S} = {high, low}$
و اعمال 
$\EuScript{A} = {search, wait, recharge}$
است.
برای سادگی، حالت\nf های $high$ و $low$ را به ترتیب با $h$ و $l$ و اعمال $search$،
$wait$
و
$recharge$
را به ترتیب با $s$ و $w$ و $re$ نشان می\nf دهیم. از آنجایی که تنها دو حالت وجود دارد، معادله بهینگی بلمن شامل دو معادله است:


!!!!!!!!!!!!!!!!!!!!!!! معادله اول


به طریق مشابه برای 
$v_*(l)$
معادله زیر به\nf دست می\nf آید:


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! معادله دوم
\subsection{برنامه‌ریزی پویا}

ایده اصلی DP و بیشتر الگوریتم یادگیری تقویتی‌، استفاده از تابع ارزش حالت یا عمل برای سازماندهی یک الگوریتم جستجو برای پیدا کردن خط‌مشی بهینه
$v_*$
یا
$q_*$
است
که در معادلات بهینگی بلمن صدق می‌کند:
\begin{equation}
v_{*}(s) = \max_{a} \sum_{s',r} p(s',r | s,a)[r + \gamma v_*(s')]
\label{bellman_opt_state}
\end{equation}
\begin{equation}
q_{*}(s,a) = \sum_{s',r} p(s',r | s,a)[r + \gamma \max_{a'} q_* (s',a')]
\label{bellman_opt_action}
\end{equation}
%\section{ارزیابی خط‌مشی}
اگر دینامیک محیط کاملاً معلوم باشد معادلات
\ref{bellman_opt_state}
و
\ref{bellman_opt_action}
به ترتیب یک دستگاه معادلات خطی با اندازه
 $|\EuScript{S}|$
 و
 $|\EuScript{S} \times \EuScript{A}|$
  هستند.
در ساده ترین حالت، باحل این دستگاه معادلات  خطی می توان  مقادیر 
$v_*(s)$
و
$q_*(s,a)$
را به ازای هر حالت $s$ و عمل $a$ به دست آورد. اما اگر تعداد حالت‌ها زیاد باشد، ممکن است حل چنین دستگاهی به لحاظ محاسباتی عملی نباشد. برای اهداف ما، روش‌های تکراری مناسب‌ترین روش‌ها هستند. یک دنباله $ v_0 , v_1 , v_2 , ...$ 
از توابع ارزش تقریبی
را در نظر بگیرید که هرکدام نگاشتی از
$\EuScript{S}$ 
به 
$\IR$
 هستند.
تقریب اولیه $v_0$
در همه حالت\nf ها
به طور دلخواه انتخاب می‌شود(به جز در حالت‌های پایانی که باید صفر باشد). هر تقریب به عنوان یک قانون بروزرسانی به وسیله معادله بلمن از روی تقریب قبلی بدست می‌آید. برای هر 
$s \in \EuScript{S}$:
%If the environment’s dynamics are completely known, then (4.4) is a system of |S| simultaneous linear
%equations in |S| unknowns (the vπ(s), s ∈ S). In principle, its solution is a straightforward, if tedious,
%computation. For our purposes, iterative solution methods are most suitable. Consider a sequence
%of approximate value functions v0, v1, v2, . . ., each mapping S
%+ to R (the real numbers). The initial
%approximation, v0, is chosen arbitrarily (except that the terminal state, if any, must be given value 0),
%and each successive approximation is obtained by using the Bellman equation for vπ (4.4) as an update
%rule:
\begin{align}
v_{k+1}(s) \triangleq & \mathbb{E}_{\pi} [R_{t+1} + \gamma v_k(S_{t+1}) | S_t=s]  \nonumber \\
=& \sum_{a} \pi(a|s) \sum_{s',r} p(s',r | s,a)[r + \gamma v_k(s')].
\end{align}
بر اساس معادله بلمن می‌توان درستی این تساوی را برای 
$v_{\pi}$
نوشت و واضح است که 
 $v_k = v_{\pi}$
 نقطه ثابتی برای این قانون بروز رسانی است.
 در واقع، می‌توان نشان داد که در حالت کلی دنباله 
 $\{ v_k \}$
وقتی که 
$ k \rightarrow \infty $
و تحت همان شرایط که وجود 
$v_{\pi}$
را تضمین می‌کند، به مقدار 
$v_{\pi}$
همگرا می‌شود. با دانستن این نکته می توان الگوریتمی تکراری برای  تقریب زدن تابع ارزش حالت، تحت خط مشی $\pi$، طراحی کرد. این الگوریتم را 
\textit{ارزیابی خط‌مشی گام به گام}
\LTRfootnote{Iterative policy evaluation} یا به طور خلاصه 
ارزیابی خط‌مشی
\LTRfootnote{Policy Evaluation}
 می‌نامند.


\شروع{الگوریتم}
{الگوریتم ارزیابی خط مشی}
\ورودی{خط مشی $\pi$}
\به‌ازای{تا رسیدن به همگرایی تکرار کن}
\به‌ازای{برای هر $s \in \EuScript{S}$}
\دستور{
	$V(s) \longleftarrow \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a) \left[r + \gamma V(s')\right]$
}
\پایان‌به‌ازای
\پایان‌به‌ازای
\خروجی{$V \approx v_\pi$}
\پایان{الگوریتم}
روشن است که همگرایی تنها در حد اتفاق می‌افتد، اما می توان تقریبی به اندازه کافی نزدیک از تابع ارزش به دست آورد، کافیست عملیات به روزرسانی را تا جایی ادامه دهیم که تغییرات تابع ارزش از یک
$\epsilon > 0$
کوچکتر شود. در این صورت می توانیم الگوریتم را متوقف کرده و آخرین تابع ارزش دنباله را به عنوان خروجی برگردانیم.

% 
%for all s ∈ S. Clearly, vk = vπ is a fixed point for this update rule because the Bellman equation for vπ
%assures us of equality in this case. Indeed, the sequence {vk} can be shown in general to converge to
%vπ as k → ∞ under the same conditions that guarantee the existence of vπ. This algorithm is called
%iterative policy evaluation.
%

\subsection{بهبود خط‌مشی}

هدف ما از محاسبه تابع ارزش یک خط‌مشی کمک به یافتن خط‌مشی‌های بهتر است.
فرض کنید ما تابع ارزش 
$v_{\pi}$
 را برای یک خط‌مشی معین \LTRfootnote{Deterministic} دلخواه 
 $\pi$
  محاسبه کرده ایم. برای یک حالت 
  $s$
 می‌خواهیم بدانیم که آیا باید خط‌مشی را به انتخاب قطعی یک عمل 
 $a \neq \pi(s)$
  تغییر دهیم یا خیر.
ما می‌دانیم که با پیروی از خط‌مشی فعلی از حالت
 $s$
  چقدر عایدی خواهیم داشت، اما آیا تغییر به یک خط‌مشی جدید بهتر است یا بدتر؟
  یکی از راه‌های پاسخ به این سوال در نظر گرفتن انتخاب عمل  
  $a$
در حالت 
$s$
و پس از آن پیروی از خط‌مشی
$\pi$
 است. 
 ارزش این شیوه عملکرد برابر است با:
 
 \begin{align}
q_\pi(s,a) = \sum_{s',r} p(s',r|s,a)\left[r + \gamma v_\pi(s')\right]	
 	\label{eq:policybetter}
 \end{align}

 اگر عبارت 
 \ref{eq:policybetter}
 از $v_\pi(s)$ بزرگتر باشد، بدین معناست که انتخاب قطعی عمل $a$ در حالت $s$ و سپس دنبال کردن خط‌مشی $\pi$ بهتر از این است که همواره خط‌مشی $\pi$ را دنبال کنیم. این گزاره را می‌توان به فرم کلی تر در قالب یک قضیه بیان کرد.

%Our reason for computing the value function for a policy is to help find better policies. Suppose we
%have determined the value function vπ for an arbitrary deterministic policy π. For some state s we
%would like to know whether or not we should change the policy to deterministically choose an action
%a 6= π(s). We know how good it is to follow the current policy from s—that is vπ(s)—but would it be better or worse to change to the new policy? One way to answer this question is to consider selecting a in s and thereafter following the existing policy, π. The value of this way of behaving is
\شروع{قضیه}[قضیه‌ی بهبود خط‌مشی]
فرض کنید $\pi$ و 
$\pi_0$
دو خط‌مشی قطعی باشند که برای هر 
$s \in S$
\begin{align}
q_\pi(s, \pi'(s)) \ge \v_{\pi}(s) \numberthis
\label{action-state}
\end{align}
در این صورت 
\begin{align}
v_{\pi'}(s) \ge v_\pi(s)
\label{state-state}
\end{align}
همچنین اگر نامساوی
\ref{action-state}
برای یکی از حالت ها، به صورت اکید برقرار باشد آنگاه
\ref{state-state}
نیز به صورت اکید برقرار است.
\label{policyimptheorem}
\پایان{قضیه}
درستی قضیه بالا را می\nf توان با استفاده از تعاریف به روشنی بررسی کرد؛
\begin{align*}
& v_\pi (s) \le q_\pi (s,\pi'(s)) = & \mathbb{E}\left[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t = \pi'(a)\right] \\
= & \mathbb{E}_{\pi'}\left[R_{t+1}+\gamma v_\pi(s_{t+1}) | S_t=s\right] \\
 \le &  \mathbb{E}_{\pi'}[R_{t+1}+ \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t=s] \\
= & \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2})] | S_t=s] \\
\le & \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_\pi(s_{t+3}) | S_t=s] \\
\vdots \\
\le & \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots | S_t=s] \\
\le & v_{\pi'}(s)
\end{align*}


قضیه \ref{policyimptheorem} یک شرط کافی برای اینکه خط‌مشی $\pi'$ بهتر یا مساوی خط‌مشی $\pi$ باشد را بیان می‌کند. این قضیه می‌تواند ما را به روشی برای بهبود یک خط‌مشی، سوق دهد. منظور ما از بهبود خط‌مشی، روشی برای دستیابی به یک خط‌مشی بهتر، از روی یک خط‌مشی ضعیف‌تر است.

فرض کنید $\pi'$ یک خط مشی قطعی باشد که نسبت به تابع ارزش عمل 
$q_\pi$
حریصانه عمل می کند. به عبارت دیگر

\begin{align*}
	\pi'(s) \triangleq& arg\max_{a} q_\pi(s,a) \\
	= & arg\max_{a} \mathbb{E}\left[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a\right] \\
	= & arg \max_{a} \sum_{s',r} p(s',r|s,a) \left[r + \gamma v_\pi(s')\right].\end{align*}

با استفاده از قضیه \ref{policyimptheorem} می توان نشان داد برای هر
$s \in \EuScript{S}$:
$$v_{\pi'}(s) \ge v_{\pi}(s).$$
بنابراین با حریصانه عمل کردن نسبت به تابع ارزش خط‌مشی $\pi$ می‌توان به یک خط‌مشی بهتر رسید \cite{suttonbook}.


\شروع{الگوریتم}
{الگوریتم بهبود خط‌مشی}
\ورودی{ تابع  
	$V \approx v_\pi$
}
\به‌ازای{برای هر $s \in \EuScript{S}$}
\دستور{
	$\pi'(s) \longleftarrow arg \max_{a} \sum_{s',r} p(s',r|s,a) \left[r + \gamma V(s')\right]$
}
\پایان‌به‌ازای
\خروجی{$\pi'$}
\پایان{الگوریتم}

\subsection{الگوریتم تکرار خط‌مشی}
تا اینجا روشی برای محاسبه تقریبی تابع ارزش حالت و همینطور روشی دستیابی به یک خط‌مشی بهتر با استفاده از تابع ارزش، ارائه دادیم. اما هدف ما دستیابی به خط‌مشی بهینه است. 
برای این کار، با استفاده از ارزیابی خط‌مشی، روی یک خط‌مشی دلخواه $\pi_0$ به تخمینی از 
$v_{\pi_0}$
 برسیم؛ سپس با استفاده از الگوریتم بهبود خط‌مشی روی 
$v_{\pi_0}$،
خط‌مشی جدید 
$\pi_1$
را می سازیم. با تکرار این عملیات روی خط‌مشی 
$\pi_1$
به خط‌مشی 
$\pi_2$
خواهیم رسید. با ادامه دادن این روند، می‌توانیم دنباله‌ای از خط‌مشی‌ها و تابع ارزش‌ها به دست آوریم که به صورت یکنوا در حال بهبود هستند. می‌توان نشان داد که این دنباله از خط‌مشی‌ها و توابع ارزش، به خط‌مشی و تابع ارزش بهینه همگرا می‌شود:
%Once a policy, π, has been improved using vπ to yield a better policy, π0, we can then compute vπ0 and
%improve it again to yield an even better π00. We can thus obtain a sequence of monotonically improving
%policies and value functions:
$$\pi_0 \longrightarrow v_{\pi_0} \longrightarrow \pi_1 \longrightarrow v_{\pi{1}} \longrightarrow \pi_2 \longrightarrow \dots \longrightarrow \pi_* \longrightarrow v_*$$

به این الگوریتم، الگوریتم \textit{تکرار خط‌مشی}  \LTRfootnote{Policy iteration}
گفته می\nf شود.

\شروع{الگوریتم}
{الگوریتم تکرار خط‌مشی}
\دستور{تابع ارزش 
	$V(s)$
	 و خط‌مشی 
	 $\pi$
	 را به دلخواه مقداردهی اولیه کن.
}

\به‌ازای{تا رسیدن به خط‌مشی و تابع ارزش بهینه تکرار کن}

(ارزیابی خط‌مشی)
\به‌ازای{تا رسیدن به همگرایی تکرار کن}
\به‌ازای{برای هر $s \in \EuScript{S}$}
\دستور{
	$V(s) \longleftarrow \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a) \left[r + \gamma V(s')\right]$
}

\پایان‌به‌ازای
\پایان‌به‌ازای
(بهبود خط‌مشی)
\به‌ازای{برای هر $s \in \EuScript{S}$}
\دستور{
	$\pi(s) \longleftarrow arg \max_{a} \sum_{s',r} p(s',r|s,a) \left[r + \gamma V(s')\right]$
}
\پایان‌به‌ازای
\پایان‌به‌ازای
\خروجی{
	$V_*, \pi_*$
}
\پایان{الگوریتم}


\subsection{الگوریتم تکرار ارزش}
یک اشکال در روش تکرار خط‌مشی این است که هر یک از تکرارهای آن، شامل ارزیابی خط‌مشی است که
به خودی خود یک محاسبه زمان\nf بر است که نیاز به رفت و برگشت‌های متعدد روی مجموعه حالت دارد. اگر
ارزیابی خط‌مشی بصورت گام به گام تکراری انجام شود، همگرایی فقط در حد اتفاق می‌افتد. بنابراین یا باید منتظر همگرایی دقیق بمانیم و یا می‌توانیم قبل تر از آن متوقف شویم. 
در واقع، عملیات ارزیابی خط‌مشی را می‌توان پس از یک بار رفت و برگشت روی فضای حالت (یک بروزرسانی در هر حالت)،  بدون از دست دادن تضمین همگرایی به خط‌مشی بهینه، متوقف کرد. 
این الگوریتم،
\textit{تکرار ارزش}
\LTRfootnote{Value Iteration}
 نامیده می‌شود. الگوریتم تکرار ارزش نسبت به تکرار خط‌مشی، گام های بیشتری تا رسیدن به همگرایی می پیماید اما به دلیل آنکه در هر گام تنها یک بار رفت و برگشت روی فضای حالت های اتفاق می افتد، در عمل سریع تر از تکرار خط‌مشی به همگرایی می رسد. این الگوریتم
را می‌توان به عنوان یک عملیات بروزرسانی ساده نوشت که ترکیبی از بهبود خط‌مشی و ارزیابی خط‌مشی کوتاه شده است.



\شروع{الگوریتم}
{الگوریتم تکرار ارزش}
\دستور{تابع ارزش 
	$V(s)$
	را به دلخواه مقداردهی اولیه کن.
}

\به‌ازای{تا رسیدن به تابع ارزش بهینه تکرار کن}

\به‌ازای{برای هر $s \in \EuScript{S}$}
\دستور{
	$V(s) \longleftarrow \max_{a} \sum_{s',r} p(s',r|s,a) \left[r + \gamma V(s')\right]$
}
\پایان‌به‌ازای
\پایان‌به‌ازای
\دستور{خط مشی قطعی $\pi$  را تولید کن به طوری که: 
	$$\pi(s) = arg \max_{a} \sum_{s',r} p(s',r|s,a)\left[r + \gamma V(s')\right]$$
}
\خروجی{
	$V \approx V_*, \pi \approx \pi_*$
}
\پایان{الگوریتم}
%One drawback to policy iteration is that each of its iterations involves policy evaluation, which may
%itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy
%evaluation is done iteratively, then convergence exactly to vπ occurs only in the limit. Must we wait
%for exact convergence, or can we stop short of that? The example in Figure 4.1 certainly suggests that
%it may be possible to truncate policy evaluation. In that example, policy evaluation iterations beyond
%the first three have no effect on the corresponding greedy policy.
%In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing
%the convergence guarantees of policy iteration. One important special case is when policy evaluation
%is stopped after just one sweep (one update of each state). This algorithm is called value iteration. It
%can be written as a particularly simple update operation that combines the policy improvement and
%truncated policy evaluation steps:


%
%for all s ∈ S. For arbitrary v0, the sequence {vk} can be shown to converge to v∗ under the same
%conditions that guarantee the existence of v∗.

\subsection{الگوریتم های تکرار خط‌مشی تعمیم یافته}


%A major drawback to the DP methods that we have discussed so far is that they involve operations
%over the entire state set of the MDP, that is, they require sweeps of the state set. If the state set is very
%large, then even a single sweep can be prohibitively expensive
%
%
%Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms
%of systematic sweeps of the state set. These algorithms update the values of states in any order
%whatsoever, using whatever values of other states happen to be available. The values of some states
%may be updated several times before the values of others are updated once. To converge correctly,
%however, an asynchronous algorithm must continue to update the values of all the states: it can’t ignore
%any state after some point in the computation. Asynchronous DP algorithms allow great flexibility in
%selecting states to update.
%
یک اشکال عمده در روشهای برنامه ریزی پویا که تا کنون بحث شده‌است، این است که آن ها شامل عملیات‌هایی روی تمام فضای حالت‌ها هستند.
اگر فضای حالت بسیار بزرگ باشد،
حتی یک بار رفت و برگشت می‌تواند بسیار پرهزینه باشد.
الگوریتم‌های ناهمزمان برنامه ریزی پویا، الگوریتم‌های تکرار شونده درجا هستند که مقید به رفت و برگشت سیستماتیک روی تمام مجموعه حالت‌ها نیستند. این الگوریتم‌ها ارزش حالت‌ها را بدون هیچ ترتیب منظمی ‌و با استفاده از هر تخمینی از ارزش حالت‌های دیگر که در دسترس باشد، به روز می‌کنند.
 ارزش برخی حالت‌ها ممکن است قبل از اینکه ارزش دیگران یکبار به روز شود، چندین بار به روز شوند. با این حال، برای همگرایی صحیح، یک الگوریتم ناهمزمان باید بروزرسانی مقادیر همه حالت‌ها را ادامه دهد و نمی‌تواند حالتی را نادیده بگیرد.
الگوریتم‌های ناهمزمان DP امکان انعطاف‌پذیری زیادی را در سیستم فراهم می‌کنند و در بسیاری از مسائل، تنها گزینه قابل اجرا به حساب می‌آیند.

\section{روش های یادگیری تفاوت زمانی}
برخلاف روش های برنامه\nf ریزی پویا، روش های 
{یادگیری تفاوت زمانی} (TD) می توانند مستقیماً از تجربه خام بدون در اختیار داشتن مدلی از دینامیک محیط، یاد بگیرند. مانند DP، روش های TD، ارزش ها حالت  (حالت-عمل) را بر اساس تخمین های حالت (حالت-عمل)  های دیگر به روز می کنند.

%TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based
%in part on other learned estimates, without waiting for a final outcome 


% the value of a state is the expected return—expected cumulative future discounted reward—
%starting from that state. An obvious way to estimate it from experience, then, is simply to average the
%returns observed after visits to that state. As more returns are observed, the average should converge
%to the expected value.

همانطور که دیدیم، ارزش یک حالت، امید ریاضی عایدی، با شروع از آن حالت است.
بنابراین ، یک روش آشکار برای تخمین ارزش حالت $s$ از طریق تجربه، میانگین گرفتن تمام عایدی های تجربه شده در هر بار عبور از $s$ است.
با تجربه بیشتر و مشاهده عایدی های بیشتر، میانگین آن ها باید به $v_\pi(s)$ همگرا شود.

روش های تفاوت زمانی در ساده ترین حالت، بعد از عبور از هر حالت، ارزش آن حالت را با استفاده از ارزش حالت بعدی به عنوان تخمینی از عایدی، بلافاصله به روزرسانی می کنند
$$V(S_t) \longleftarrow V(S_t) + \alpha\left[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right]$$

\شروع{الگوریتم}{الگوریتم TD(0) برای تخمین $v_\pi$}
\ورودی{خط‌مشی $\pi$}
\دستور{تابع $V(s)$ را با مقادیر دلخواه مقداردهی اولیه کن.}
\‌به‌ازای{برای هر گام در اپیزود}
\دستور{عمل $a$ را با دنبال کردن خط‌مشی $\pi$ انتخاب کن}
\دستور{پاداش $r$ و حالت جدید $s'$ را مشاهده کن}
\دستور{
$V(s) \longleftarrow V(s) + \alpha\left[r + \gamma V(s') - V(s)\right]$
}
\دستور{
	$s \longleftarrow s'$
}

\پایان‌به‌ازای

\پایان{الگوریتم}

\subsection{Q-learning}
ایده اصلی در روش 
Q-learning
، تخمین تابع مقدار عمل  
$q^*(s,a)$ 
با استفاده از معادله بلمن به عنوان یک بروزرسانی تکراری،
$$q_{i+1}(s,a) = \mathbb{E}[r+ \gamma \max_{a'} q_i(s',a')|s,a]$$
چنین الگوریتم‌های تکرار مقداری به تابع ارزشِ عمل بهینه همگرا می‌شوند، 
$q_i \longrightarrow q^*$
وقتی
$i \longrightarrow \infty$
، این رویکرد کلی کاملا غیر عملی است.
زیرا تابع ارزشِ عمل برای هر دنباله، به طور جداگانه و بدون هیچ گونه تعمیم برآورد می‌شود. در عوض، معمولاً از یک تقریب‌گر توابع (مثل شبکه عصبی) برای تخمین تابع ارزشِ عمل استفاده می‌شود. در فصل سوم با این رویکرد بیشتر آشنا خواهیم شد.