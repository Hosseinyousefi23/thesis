
\فصل{مفاهیم اولیه یادگیری تقویتی}
 \textit{کنترل بهینه}
\LTRfootnote{Optimal control}
در اواخر دهه 1950 برای توصیف مسئله طراحی یک کنترل‌گر برای به حداقل رساندن اندازه‌گیری رفتار \textit{سیستم دینامیکی}\LTRfootnote{Dynamical system} در طول زمان مورد استفاده قرارگرفت. یکی از رویکردهای این مسئله در اواسط دهه 1950 توسط ریچارد بلمن\LTRfootnote{Richard Bellman} و دیگران از طریق گسترش نظریه قرن نوزدهم همیلتون\LTRfootnote{Hamilton} و جاکوبی\LTRfootnote{Jacobi} توسعه یافت. این رویکرد از مفاهیم حالت یک سیستم دینامیکی و یک \textit{تابع ارزش}\LTRfootnote{Value function} برای تعریف یک معادله تابعی استفاده می‌کند؛ که اکنون \textit{معادله بلمن} \LTRfootnote {Bellman equation}نامیده می‌شود. مجموعه روش‌های حل مسائل کنترل بهینه به کمک معادله بلمن به عنوان برنامه‌ریزی پویا شناخته می‌شود. همچنین بلمن نسخه گسسته از مسئله کنترل بهینه را که تحت عنوان \textit{فرایندهای تصمیم‌گیری مارکوف} 
\LTRfootnote{Markov decision process(MDP)}
 شناخته می‌شود، معرفی کرد. رونالد هوارد (1960) روش
\lr{Policy Iteration}
  را برای MDP‌ها طراحی کرد. همه این‌ها عناصر اساسی در تئوری و الگوریتم‌های \textit{یادگیری تقویتی}\LTRfootnote{Reinforcement learning (RL) } مدرن هستند.
  در این فصل، با فرایندهای تصمیم‌گیری مارکوف آشنا خواهیم‌شد و چهار عنصر اصلی یادگیری تقویتی، یعنی \textit{خط‌مشی}\LTRfootnote{Policy}، \textit{سیگنال پاداش}\LTRfootnote{Reward signal}، تابع ارزش و محیط\LTRfootnote{Environment} را دقیقا تعریف خواهیم‌کرد. همچنین برخی روش‌های کلاسیک در یادگیری تقویتی را معرفی می‌کنیم.

%----------------------------- مقدمه ----------------------------------
\section{دینامیک عامل-محیط}
%\قسمت{دینامیک عامل-محیط}
حوزه یادگیری تقویتی  دو بازیگر اصلی دارد؛ عامل و محیط. موجود تصمیم گیرنده و آموزنده را عامل یادگیری، یا به اختصار، عامل می‌نامند. قسمتی که عامل با آن تعامل دارد(
هر چیز خارج از عامل)، محیط نامیده می‌شود. در ادبیات کنترل بهینه، معمولا به جای واژه‌های عامل و محیط، از کنترل کننده و سیستم کنترل شده استفاده می‌شود.
 عامل و محیط به طور مداوم با یکدیگر ارتباط برقرار می‌کنند، عامل انتخاب می‌کند که چه اقدامی‌انجام دهد، محیط به این اقدامات پاسخ می‌دهد و موقعیت جدیدی را به عامل ارائه می‌دهد.
محیط همچنین مقادیر عددی ویژه ای به نام پاداش  به عامل برمی‌گرداند، که عامل به دنبال به حداکثر رساندن آن است. \\به طور خاص، عامل و محیط در یک توالی زمانی گسسته تعامل می‌کنند، 
$t = 0,1,2,3,...$،
در هر مرحله $t$، عامل وضعیت محیط  
$S_t \in \EuScript{S}$
را دریافت می‌کند، و بر اساس آن یک عمل 
$A_t \in \EuScript{A}$
را انتخاب می‌کند. در گام بعدی، عامل به عنوان نتیجه عمل خود، یک پاداش عددی $R_{t+1} \in \EuScript{R}$ دریافت می‌کند و خود را در حالت جدید $S_{t+1}$ می‌یابد.
دینامیک عامل-محیط را می‌توان به شکل یک دنباله از حالت‌ها، عمل‌ها و پاداش‌ها به شکل زیر نمایش داد:
$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ...$$

\section{فرایند تصمیم‌گیری مارکوف}

یادگیری تقویتی از چارچوب رسمی‌فرایندهای تصمیم‌گیری مارکوف (MDP) برای تعریف تعامل بین یک عامل یادگیری و محیط آن توسط حالت‌ها، اقدامات و پاداش استفاده می‌کند. مدل MDP یک مدل کلاسیک از تصمیم‌گیری متوالی است، جایی که اقدامات نه تنها بر پاداش‌های فوری، بلکه بر موقعیت‌ها و حالت‌های بعدی و به تبع آن بر پاداش‌های آینده تأثیر می‌گذارد.
MDP
 یک فرم ایده‌آل ریاضی از مسئله یادگیری تقویتی است که برای آن تئوری‌های دقیقی بیان شده‌است.
MDP متناهی، یک MDP با مجموعه حالت‌های محدود است. 
بیشتر نظریه‌های فعلی یادگیری تقویتی، محدود به MDP متناهی است، اما روش‌ها و ایده‌ها به طور کلی بیان می‌شوند.


%[Szepesvari2010]
\شروع{تعریف}[فرایند تصمیم‌گیری مارکوف] 
 فرایند تصمیم‌گیری مارکوف، یک 4 تایی 
$$\EuScript{M} = \seq{\EuScript{S},\EuScript{A},\EuScript{R},\EuScript{P}}$$
است که
\شروع{فقرات}
\فقره $\EuScript{S}$ بیانگر مجموعه تمام \مهم{حالت‌ها}\nf‌هاست،
\فقره $\EuScript{A}$ بیانگر مجموعه تمام \مهم{عمل‌ها}\nf‌هاست،
\فقره{
	$\EuScript{R} \subseteq \IR$
	بیانگر مجموعه \مهم{پاداش}‌هاست،
}
\فقره 
$\EuScript{P}$
\مهم{هسته احتمال انتقال\LTRfootnote 
	{Transition Probability Kernel}} ،
$ \EuScript{P}: \EuScript{S} \times \EuScript{A} \to \Pi(\EuScript{S} \times \EuScript{R})$، 
تابعی است که دینامیک MDP را مشخص می‌کند.

\پایان{فقرات}

\پایان{تعریف}
هسته احتمال انتقال یا تابع انتقال $\EuScript{P}$، هر دوتایی حالت-عمل
$(s,a)$، که 
$s \in \EuScript{S}$
و
$a \in \EuScript{A}$، را به یک توزیع احتمال روی دوتایی‌هایی به شکل 
$(s',r)$
نسبت می‌دهد. $s'$ بیانگر حالت بعدی و $r$ بیانگر پاداش این انتقال است. به ازای هر دو حالت 
$s,s' \in \EuScript{S}$
 و هر عمل 
 $a \in \EuScript{A}$
  و هر پاداش 
  $r \in \EuScript{R}$
  احتمال رسیدن به حالت $s'$ و دریافت پاداش $r$ با انتخاب عمل $a$ در حالت $s$، یک عدد حقیقی متعلق به  بازه $[0,1]$ است که آن را به شکل
$p(s',r|s,a)$
نمایش می‌دهیم:
$$p(s',r|s,a) \triangleq Pr\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\}.$$ 
نامگذاری فرایند تصمیم‌گیری مارکوف اشاره به این موضوع دارد که این سیستم‌ها دارای \مهم{ویژگی مارکوف \LTRfootnote{Marcov Property}}  هستند، بدین معنا که تابع انتقال آن تنها به حالت فعلی سیستم و آخرین عمل انجام شده وابسته است، و نسبت به حالت‌ها و اعمال قبلی مستقل است.
\section{عناصر اصلی یادگیری تقویتی}
\subsection{خط‌مشی}
خط‌مشی\LTRfootnote{Policy} نحوه رفتار عامل یادگیری را در یک زمان خاص، مشخص می‌کند و هسته اصلی رفتار یک عامل یادگیری تقویتی است. خط‌مشی به تنهایی برای تعیین رفتار کافی است. به عنوان یک تعریف غیر دقیق، خط‌مشی، نگاشتی از حالت‌های مدل شده از محیط به عملی است که باید در آن حالت انجام شود.
خط‌مشی ممکن است یک عملکرد ساده یا جدول جستجو باشد، یا ممکن است شامل محاسبات پیچیده‌ای مانند فرآیند جستجو باشد؛ همچنین
خط‌مشی‌ها ممکن است احتمالاتی باشند.

\شروع{تعریف}[خط\nf مشی احتمالاتی ثابت]
%[Szepesvari2010]
یک خط\nf مشی احتمالاتی ثابت\LTRfootnote{Stationary probabilistic policy} (یا به طور خلاصه خط\nf مشی ثابت) 
$\pi: S \to \Pi(A)$
تابعی است که هر حالت را به یک توزیع احتمال روی فضای عمل
$\EuScript{A}$
 می\nf نگارد.
به طور خلاصه احتمال انتخاب عمل $a$ در حالت $s$ تحت خط\nf مشی $\pi$ را به شکل
$\pi(a|s)$
نشان می\nf دهیم.

\پایان{تعریف} 

\شروع{تعریف}
می\nf گوییم خط\nf مشی $\pi$ در یک MDP \مهم{دنبال می\nf شود} هرگاه
$$A_t \sim \pi(. |S_t),	 \quad t \in \IN.$$

\پایان{تعریف}

\subsection{سیگنال پاداش}

در مسئله یادگیری تقویتی، تصمیمات عامل یادگیری، توسط سیگنال پاداش جهت‌دهی می‌شود. در هر گام، محیط، یک عدد حقیقی به عنوان پاداش برای عامل یادگیری تقویتی ارسال می‌کند. تنها هدف عامل، به حداکثر رساندن کل پاداش دریافتی در طولانی مدت است. سیگنال پاداش اتفاقات خوب و بد را برای عامل مشخص می‌کند و مبنای اصلی تغییر خط‌مشی است
سیگنال پاداش می‌تواند تابعی تصادفی از وضعیت محیط و اقدام انجام شده باشد.


\subsection{عایدی و تابع ارزش}
همانطور که سیگنال پاداش نشان می‌دهد که انجام چه عملی در هر گام خوب است، تابع ارزش مشخص می\nf کند که کدام عمل در طولانی مدت بهتر است. در واقع تابع ارزش نشانگر مطلوبیت طولانی مدت حالت‌ها با در نظر گرفتن حالت‌هایی است که در پی خواهند داشت.
ارزش حالت $s$، تحت خط\nf مشی $\pi$، یا
 $v_\pi(s)$،
مجموع میزان پاداشی است که عامل، با شروع از $s$ و دنبال کردن خط\nf مشی $\pi$، می‌تواند انتظار داشته باشد در آینده کسب کند.
پاداش‌ها به یک معنا اولیه هستند، در حالی که ارزش‌ها، به عنوان پیش‌بینی پاداش‌ها، ثانویه هستند. بدون پاداش هیچ ارزشی وجود ندارد و تنها هدفِ تخمین ارزش‌ها، دستیابی به پاداش بیشتر است. با این وجود، این تابع ارزش است که هنگام تصمیم‌گیری و ارزیابی به آن توجه می‌کنیم.
تعیین ارزش،‌ بسیار دشوارتر از تعیین پاداش است.
ارزش‌ها باید از توالی مشاهداتی که یک عامل در طول عمر خود انجام می‌دهد، برآورد شوند.
مهمترین مولفه بیشتر الگوریتم‌های یادگیریِ تقویتی که در  این فصل و فصل آینده معرفی خواهیم کرد، روشی برای تخمین کارآمد تابع ارزش است.

\شروع{تعریف}
\textit{عایدی آینده کاهشی}
\LTRfootnote{Future Discounted Return}
 یا به اختصار، عایدی \LTRfootnote{Return}، در زمان $t$ به شکل

$$G_t \triangleq \sum_{t'=t}^{T} \gamma^{t'-t} R_{t'}$$
تعریف می‌شود که $T$ زمانی است که اپیزود به اتمام می‌رسد. اگر مسئله مستمر باشد آنگاه 
$T=\infty$
\پایان{تعریف}

از تعریف بالا نتیجه می‌شود 
\begin{align}
G_t =& R_t + \sum_{t'=t+1}^{T} \gamma^{t'-t} R_{t'} \nonumber \\
=& R_t + \gamma \sum_{t'=t+1}^{T} \gamma^{t' - (t+1)} r_{t'} \nonumber \\
=& R_t + \gamma G_{t+1}. \numberthis
\end{align}

\شروع{تعریف}[تابع ارزش حالت]

 ارزش حالت $s$ تحت خط‌مشی $\pi$ یا $v_\pi(s)$ به شکل امیدریاضی عایدی، با شروع از $s$ و دنبال کردن خط‌مشی $\pi$ تعریف می‌شود.
$$v_\pi(s) \triangleq \mathbb{E}_\pi\left[G_t| S_t=s\right] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t =s \right]$$
تابع $v_\pi$ را 
\textit{تابع ارزش حالت}
\LTRfootnote{State Value Function} مربوط به خط‌مشی 
$\pi$
می‌نامیم. اگر $s$ یک حالت نهایی باشد آنگاه 
$v_\pi(s) = 0$

\label{statevaluedef}
\پایان{تعریف}

\شروع{تعریف}[تابع ارزش عمل]

ارزش عمل  $a$ در حالت
$s$
 تحت خط‌مشی $\pi$ یا 
 $q_\pi(s,a)$
  به شکل امیدریاضی عایدی، با شروع از $s$ و انتخاب عمل $a$  و سپس دنبال کردن خط‌مشی $\pi$ تعریف می‌شود.
$$q_\pi(s,a) = \mathbb{E}_\pi\left[G_t| S_t=s, A_t=a\right] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t =s, A_t=a \right]$$
تابع $q_\pi$ را 
\textit{تابع ارزش عمل}
\LTRfootnote{Action Value Function} مربوط به خط‌مشی 
$\pi$
می‌نامیم.
\پایان{تعریف}
\subsection{مدل محیط}

مدلِ محیط رفتار محیط را تقلید می‌کند، یا به طور کلی‌تر، اجازه می‌دهد تا در مورد نحوه رفتار محیط، پیشبینی کارآمدی داشته‌باشیم. از مدل‌ها برای برنامه‌ریزی و انتخاب در روند تصمیم‌گیری  با در نظر گرفتن شرایط احتمالی آینده بدون تجربه واقعی آن‌ها استفاده می‌شود.
روش‌هایی که برای حل مشکلات یادگیری تقویتی از مدل‌ها و برنامه‌ریزی‌ها استفاده می‌کنند، روش‌های 
\textit{مبتنی بر مدل}
\LTRfootnote{Model Based}
 نامیده می‌شوند. در مقابل این روش‌ها، روش‌های 
\textit{بدون مدل}
\LTRfootnote{Model Free}
  هستند که از هیچ گونه شبیه سازی  یا ابزاری برای پیشبینی رفتار محیط استفاده نمی‌کنند و معمولا روند یادگیری آن‌ها بر اساس تجربه واقعی در محیط و آزمون و خطا است.
\section{برنامه‌ریزی پویا}
در بخش قبل، با مفاهیم کلیدی یادگیری تقویتی آشنا شدیم. در این بخش به یکی از مهم ترین روش‌های کلاسیک در حل مسئله یادگیری تقویتی، یعنی 
\textit{برنامه ریزی پویا}
\LTRfootnote{ (DP)Dynamic Programming}خواهیم پرداخت. برنامه‌ریزی پویا
 مجموعه‌ای از الگوریتم‌ها است در صورت وجود مدل کاملی از محیط در قالب یک 
MDP
 می‌توانند برای محاسبه بهترین خط\nf مشی (خط\nf مشی‌ای که بیشترین عایدی را به دست می‌دهد)   استفاده شوند.
الگوریتم‌های کلاسیک برنامه‌ریزی پویا به دلیل فرض
مدل کاملی از محیط و همچنین هزینه محاسباتی زیادشان، به لحاظ عملی چندان قابل استفاده نیستند اما به لحاظ نظری مهم هستند. قبل از آن که به روش‌های برنامه ریزی پویا بپردازیم، لازم است با مفهوم خط‌مشی بهینه و تابع ارزش بهینه آشنا شویم.
\subsection{خط‌مشی و تابع ارزش بهینه}
حل کردن مسئله یادگیری تقویتی، به معنی پیدا کردن خط\nf مشی ای است که بیشترین پاداش را در طول زمان موجب می‌شود.	به چنین خط\nf مشی ای، 
\textit{خط\nf مشی بهینه} 
\LTRfootnote{Optimal Policy}
گفته می‌شود. برای
 MDP‌ 
 های متناهی، می‌توانیم خط‌مشی بهینه را به صورت زیر تعریف کنیم:
\شروع{تعریف}
می‌گوییم خط‌مشی $\pi$ \مهم{بهتر یا مساوی} خط‌مشی 
$\pi'$
است یا
$\pi \ge \pi'$
هرگاه برای هر 
$s \in \mathcal{S}$

$$v_\pi(s) \ge v_{\pi'}(s).$$
\پایان{تعریف} می\nf توان نشان داد که برای هر MDP متناهی، حداقل یک خط\nf مشی وجود دارد که بهتر یا مساوی هر خط\nf مشی دیگری باشد
\cite{suttonbook}.
 به چنین خط\nf مشی\nf ای \مهم{خط\nf مشی بهینه} گفته می\nf شود. ممکن است بیش از یک خط\nf مشی بهینه وجود داشته باشد. تمام خط\nf مشی\nf‌های بهینه را با نماد $\pi_*$  نمایش می\nf دهیم. تابع ارزش متناظر با همه خط\nf مشی\nf‌های بهینه یکسان است و برابر با 
\textit{تابع ارزش بهینه}
\LTRfootnote{Optimal Value Function}
 است که با نماد $v_*$ نمایش داده شده و به شکل زیر تعریف می\nf شود:
$$v_*(s) \triangleq \max_{\pi} v_\pi(s).$$
همچنین تمام خط\nf مشی\nf‌های بهینه تابع عمل-ارزش مشترکی دارند که آن را با نماد $q_*$ نمایش می\nf دهیم و به شکل زیر تعریف می\nf کنیم:
$$q_*(s,a) \triangleq max_{\pi} q_\pi(s,a).$$
می\nf توانیم $q_*$ را برحسب $v_*$ به شکل زیر بنویسیم
\begin{equation}
q_* (s,a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1})| S_t=s, A_t=a].
%\label{•}
\end{equation}

\subsection{معادله بلمن}

ویژگی اساسی توابع ارزش که در طول یادگیری تقویتی و برنامه‌ریزی پویا استفاده می‌شوند،  این است که آن‌ها در یک رابطه بازگشتی موسوم به 
\textit{معادله بلمن} 
\LTRfootnote{Bellman Equation}
صدق می\nf کنند. معادله بلمن رابطه‌ای بین ارزش یک حالت و ارزش‌های حالت‌های بعدی آن را بیان می‌کند.

فرض کنید $\pi$ یک خط‌مشی دلخواه باشد و
 $s \in \EuScript{S}$.
  با استفاده مستقیم از تعریف
 \ref{statevaluedef}
 داریم
\begin{align}
v_\pi (s) \triangleq& \mathbb{E}_\pi [G_t | S_t = s] \nonumber \\
=& \mathbb{E}_\pi [R_{t+1}+ \gamma G_{t+1}|S_t=s] \nonumber \\
=&\sum_{a} \pi(a|s) \sum_{s'}\sum_{r} p(s',r|s,a)[r+\gamma \mathbb{E}_\pi[G_{t+1}|S_{t+1}=s']] \nonumber\\
=&\sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a)[r+\gamma v_\pi(s')] %\quad \forall s \in \mathbb(S) \numerhis 
\label{bellman}.
\end{align}

به رابطه 
 \ref{bellman}
\مهم{معادله بلمن} گفته می\nf شود. این معادله، رابطه ای بین ارزش یک حالت و ارزش حالت‌های بعدی آن را مشخص می‌کند. مشابه این رابطه را می‌توان برای تابع ارزش عمل نیز تحقیق کرد. فرض کنید 
$s \in \EuScript{S}$
 و 
 $a \in \EuScript{A}$.
 داریم
 
\begin{align}
q_\pi (s,a) \triangleq& \mathbb{E}_\pi [G_t | S_t = s, A_t = a] \nonumber \\
=& \mathbb{E}_\pi [R_{t+1}+ \gamma G_{t+1}|S_t=s, A_t = a] \nonumber \\
=&\sum_{s',r} p(s',r|s,a) \sum_{a'} \pi(a'|s')[r+\gamma \mathbb{E}_\pi[G_{t+1}|S_{t+1}=s', A_{t+1} = a']] \nonumber\\
=&\sum_{s',r}  p(s',r|s,a) \sum_{a'} \pi(a'|s')[r+\gamma q_\pi(s',a')] \quad \forall s \in \mathbb(S) \numerhis 
\label{bellmanaction}
\end{align}

رابطه \ref{bellmanaction} به معادله بلمن مربوط به تابع ارزش عمل معروف است.
\subsection{بهینگی و معادله بهینگی بلمن}
%Because v∗ is the value function for a policy, it must satisfy the self-consistency condition given by
%the Bellman equation for state values (3.14). Because it is the optimal value function, however, v∗’s
%consistency condition can be written in a special form without reference to any specific policy. This is
%the Bellman equation for v∗, or the Bellman optimality equation. Intuitively, the Bellman optimality
%equation expresses the fact that the value of a state under an optimal policy must equal the expected
%return for the best action from that state

از آنجا که $v_*$ تابع ارزش یک خط‌مشی است، بنابراین باید در شرایط معادله \ref{bellman} صدق کند.
در این حالت خاص، معادله بلمن را می‌توان به فرم ویژه ای نوشت که با نام
 \textit{معادله بهینگی بلمن}
 \LTRfootnote{Bellman Optimality Equation}
شناخته می‌شود.

\begin{align}
  v_{*}(s)= &\max_{a \in \EuScript{A}(s)} q_{\pi_*}(s,a) \nonumber \\
      =& \max_{a} \mathbb{E}_{\pi_*} [G_t|S_t=s, A_t=a] \nonumber \\ 
      =& \max_{a} \mathbb{E}_{\pi_*} [R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a] \nonumber \\ 
      =& \max_{a} \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t= s, A_t = a] \nonumber \\
      =& \max_{a} \sum_{s',r} p(s',r|s,a) [r + \gamma v_*(s')]. \numberthis
\label{eq:bellman-opti}
\end{align}
معادله 
\ref{eq:bellman-opti}
(معادله بهینگی بلمن) بیانگر این واقعیت است که ارزش یک حالت تحت یک خط\nf مشی بهینه ،برابر با امیدریاضی عایدی، برای بهترین عمل از آن حالت است. به طور مشابه برای تابع ارزش عمل بهینه، معادله بهینگی بلمن به شکل زیر خواهد بود:
\begin{align}
q_*(s,a) =& \mathbb{E}\left[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1},a')| S_t=s, A_t=a \right] \nonumber\\
=& \sum_{s',r} p(s',r|s,a) \left[r + \gamma \max_{a'} q_*(s',a')\right] 
\label{eq:2}.
\end{align}

\subsection{برنامه‌ریزی پویا}


ایده اصلی DP و به طور کلی یادگیری تقویتی‌، استفاده از تابع ارزش حالت یا عمل برای سازماندهی یک الگوریتم جستجو برای پیدا کردن خط‌مشی بهینه است.
$v_*$
یا
$q_*$
است
که در معادلات بهینگی بلمن صدق می‌کند:
\begin{equation}
v_{*}(s) = \max_{a} \sum_{s',r} p(s',r | s,a)[r + \gamma v_*(s')]
\label{bellman_opt_state}
\end{equation}
\begin{equation}
q_{*}(s,a) = \sum_{s',r} p(s',r | s,a)[r + \gamma \max_{a'} q_* (s',a')]
\label{bellman_opt_action}
\end{equation}
%\section{ارزیابی خط‌مشی}
اگر دینامیک محیط کاملاً معلوم باشد معادله
\ref{bellman_opt_state}
و
یک دستگاه معادلات خطی با اندازه
 $|\EuScript{S}|$
  است
 که راه‌حل آن سرراست است. اما اگر تعداد حالت‌ها زیاد پاید، ممکن است این روش عملی نباشد
 برای اهداف ما، روش‌های تکراری مناسب‌ترین روش‌ها هستند. یک دنباله 

$ v_0 , v_1 , v_2 , ...$ 
از توابع ارزش تقریبی
را در نظر بگیرید که هرکدام نگاشتی از
$\EuScript{S}$ 
به 
$\IR$
 هستند.
تقریب اولیه $v_0$
به طور دلخواه انتخاب می‌شود(به جز در حالت‌های پایانی که باید صفر باشد).

 هر تقریب به عنوان یک قانون به‌روز رسانی به وسیله معادله بلمن از روی تقریب قبلی برای
$v_\pi$
بدست می‌آید. 
%If the environment’s dynamics are completely known, then (4.4) is a system of |S| simultaneous linear
%equations in |S| unknowns (the vπ(s), s ∈ S). In principle, its solution is a straightforward, if tedious,
%computation. For our purposes, iterative solution methods are most suitable. Consider a sequence
%of approximate value functions v0, v1, v2, . . ., each mapping S
%+ to R (the real numbers). The initial
%approximation, v0, is chosen arbitrarily (except that the terminal state, if any, must be given value 0),
%and each successive approximation is obtained by using the Bellman equation for vπ (4.4) as an update
%rule:
\begin{align}
v_{k+1}(s) \triangleq & \mathbb{E}_{\pi} [R_{t+1} + \gamma v_k(S_{t+1}) | S_t=s]  \nonumber \\
=& \sum_{a} \pi(a|s) \sum_{s',r} p(s',r | s,a)[r + \gamma v_k(s')]
\end{align}
 برای هر 
 $s \in \EuScript{S}$.
 بر اساس معادله بلمن می‌توان درستی این تساوی را برای 
$v_{\pi}$
نوشت و واضح است که 
 $v_k = v_{\pi}$
 نقطه ثابتی برای این قانون بروز رسانی است.
 در واقع، می‌توان نشان داد که در حالت کلی دنباله 
 $\{ v_k \}$
وفتی که 
$ k \rightarrow \infty $
و تحت همان شرایط که وجود 
$v_{\pi}$
را تضمین می‌کند، به مقدار 
$v_{\pi}$
همگرا می‌شود.  این الگوریتم را 
\textit{ارزیابی خط‌مشی رفت و برگشتی}
\LTRfootnote{Iterative policy evaluation} یا به طور خلاصه 
ارزیابی خط‌مشی \LTRfootnote{Policy Evaluation} می‌نامند.
 
% 
%for all s ∈ S. Clearly, vk = vπ is a fixed point for this update rule because the Bellman equation for vπ
%assures us of equality in this case. Indeed, the sequence {vk} can be shown in general to converge to
%vπ as k → ∞ under the same conditions that guarantee the existence of vπ. This algorithm is called
%iterative policy evaluation.
%

\subsection{بهبود خط‌مشی}

هدف ما برای محاسبه تابع ارزش یک خط‌مشی کمک به یافتن خط‌مشی‌های بهتر است.
فرض کنید ما تابع ارزش 
$v_{\pi}$
 را برای یک خط‌مشی معین \LTRfootnote{Deterministic} دلخواه 
 $\pi$
  محاسبه کرده ایم. برای یک حالت 
  $s$
 می‌خواهیم بدانیم که آیا باید خط‌مشی را برای انتخاب قطعی یک عمل 
 $a \neq \pi(s)$
  تغییر دهیم یا خیر.
ما می‌دانیم که پیروی از خط‌مشی فعلی از حالت
 $s$
  چقدر خوب است، اما آیا تغییر به یک خط‌مشی جدید بهتر است یا بدتر؟
  یکی از راه‌های پاسخ به این سوال در نظر گرفتن انتخاب عمل  
  $a$
در حالت 
$s$
و پس از آن پیروی از خط‌مشی
$\pi$
 است. 
 ارزش این شیوه عملکرد برابر است با:
 
 $$q_\pi(s,a) = \sum_{s',r} p(s',r|s,a)\left[r + \gamma v_\pi(s')\right]$$
 اگر این عبارت از $v_\pi(s)$ بزرگتر باشد، بدین معناست که انتخاب قطعی عمل $a$ در حالت $s$ و سپس دنبال کردن خط‌مشی $\pi$ بهتر از این است که همواره خط‌مشی $\pi$ را دنبال کنیم. این گزاره را می‌توان به فرم کلی تر در قالب یک قضیه بیان کرد.

%Our reason for computing the value function for a policy is to help find better policies. Suppose we
%have determined the value function vπ for an arbitrary deterministic policy π. For some state s we
%would like to know whether or not we should change the policy to deterministically choose an action
%a 6= π(s). We know how good it is to follow the current policy from s—that is vπ(s)—but would it be better or worse to change to the new policy? One way to answer this question is to consider selecting a in s and thereafter following the existing policy, π. The value of this way of behaving is
\شروع{قضیه}[قضیه‌ی بهبود خط‌مشی]
فرض کنید $\pi$ و 
$\pi_0$
دو خط‌مشی معین باشند که برای هر 
$s \in S$
\begin{align}
q_\pi(s, \pi'(s)) \ge \pi'(s) \numberthis
\end{align}
در این صورت 
\begin{align}
v_{\pi'}(s) \ge v_\pi(s)
\end{align}
\label{policyimptheorem}
\پایان{قضیه}
درستی قضیه بالا را می\nf توان با استفاده از تعاریف به روشنی بررسی کرد؛
\begin{align*}
v_\pi (s) \le q_\pi (s,\pi'(s)) = & \mathbb{E}\left[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t = \pi'(a)\right] \\
= & \mathbb{E}_{\pi'}\left[R_{t+1}+\gamma v_\pi(s_{t+1}) | S_t=s\right] \\
 \le &  \mathbb{E}_{\pi'}[R_{t+1}+ \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t=s] \\
= & \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2})] | S_t=s] \\
\le & \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_\pi(s_{t+3}) | S_t=s] \\
\vdots \\
\le & \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots | S_t=s] \\
\le & v_{\pi'}(s)
\end{align*}
\subsection{الگوریتم  خط‌مشی تکراری}
قضیه \ref{policyimptheorem} یک شرط کافی برای اینکه خط‌مشی $\pi'$ بهتر یا مساوی خط‌مشی $p$ باشد را بیان می‌کند. این قضیه می‌تواند ما را به روشی برای بهبود یک خط‌مشی، سوق دهد. منظور ما از بهبود خط‌مشی، روشی برای دستیابی به یک خط‌مشی بهتر، از روی یک خط‌مشی ضعیف تر است.
در الگوریتم تکرار خط‌مشی  \LTRfootnote{Policy iteration}هنگامی‌که یک خط‌مشی
$\pi$
 با استفاده از 
$v_{\pi}$
  بهبود یافته است تا به خط‌مشی بهتری
$\pi_0$
    برسیم، سپس می‌توانیم
$v_{\pi_0}$
    را محاسبه کرده و مجدداً آن را بهبود بخشیم تا
$\pi_{00}$
بهتر داشته باشیم.
بنابراین ما می‌توانیم دنباله‌ای از خط‌مشی‌ها و تابع ارزش‌هایی بدست آوریم که به صورت یکنوا در حال بهبود هستند:
%Once a policy, π, has been improved using vπ to yield a better policy, π0, we can then compute vπ0 and
%improve it again to yield an even better π00. We can thus obtain a sequence of monotonically improving
%policies and value functions:
$$\pi_0 \longrightarrow v_{\pi_0} \longrightarrow \pi_1 \longrightarrow v_{\pi{1}} \longrightarrow \pi_2 \longrightarrow \dots \longrightarrow \pi_* \longrightarrow v_*$$

\subsection{الگوریتم Value Iteration}
یک اشکال در تکرار خط‌مشی این است که هر یک از تکرارهای آن شامل ارزیابی خط‌مشی است که
به خودی خود یک محاسبه تکراری طولانی مدت است که نیاز به رفت و برگشت‌های متعدد روی مجموعه حالت دارد. اگر
ارزیابی خط‌مشی بصورت گام به گام تکراری انجام می‌شود، همگرایی فقط در حد اتفاق می‌افتد. بنابراین یا باید منتظر همگرایی دقیق بمانیم و یا می‌توانیم زود تر از آن متوقف شویم. 
در واقع، عملیات ارزیابی خط‌مشی را می‌توان پس از یک بار رفت و برگشت روی فضای حالت (یک بروزرسانی در هر حالت)،  بدون از دست دادن تضمین همگرایی به خط‌مشی بهینه متوقف کرد. 
این الگوریتم،
\textit{تکرار ارزش}
\LTRfootnote{Value Iteration}
 نامیده می‌شود. این الگوریتم
را می‌توان به عنوان یک عملیات بروزرسانی ساده نوشت که ترکیبی از بهبود خط‌مشی و
مراحل ارزیابی خط‌مشی کوتاه شده است.
%One drawback to policy iteration is that each of its iterations involves policy evaluation, which may
%itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy
%evaluation is done iteratively, then convergence exactly to vπ occurs only in the limit. Must we wait
%for exact convergence, or can we stop short of that? The example in Figure 4.1 certainly suggests that
%it may be possible to truncate policy evaluation. In that example, policy evaluation iterations beyond
%the first three have no effect on the corresponding greedy policy.
%In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing
%the convergence guarantees of policy iteration. One important special case is when policy evaluation
%is stopped after just one sweep (one update of each state). This algorithm is called value iteration. It
%can be written as a particularly simple update operation that combines the policy improvement and
%truncated policy evaluation steps:


%
%for all s ∈ S. For arbitrary v0, the sequence {vk} can be shown to converge to v∗ under the same
%conditions that guarantee the existence of v∗.

\subsection{درهم تنیدگی PE و PI و الگوریتم‌های GPI}


%A major drawback to the DP methods that we have discussed so far is that they involve operations
%over the entire state set of the MDP, that is, they require sweeps of the state set. If the state set is very
%large, then even a single sweep can be prohibitively expensive
%
%
%Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms
%of systematic sweeps of the state set. These algorithms update the values of states in any order
%whatsoever, using whatever values of other states happen to be available. The values of some states
%may be updated several times before the values of others are updated once. To converge correctly,
%however, an asynchronous algorithm must continue to update the values of all the states: it can’t ignore
%any state after some point in the computation. Asynchronous DP algorithms allow great flexibility in
%selecting states to update.
%
یک اشکال عمده در روشهای DP که تا کنون بحث شده‌است، این است که آنها شامل عملیات‌هایی روی تمام مجموعه حالت‌ها هستند.
اگر فضای حالت بسیار بزرگ باشد،
حتی یک بار رفت و برگشت می‌تواند بسیار پرهزینه باشد.
الگوریتم‌های ناهمزمان DP الگوریتم‌های تکرار شونده درجا هستند که مقید به رفت و برگشت سیستماتیک روی تمام مجموعه حالت‌ها نیستند. این الگوریتم‌ها ارزش حالت‌ها را بدون هیچ ترتیب منظمی ‌و با استفاده از هر تخمینی از ارزش حالت‌های دیگر که در دسترس باشد، به روز می‌کنند.
 ارزش برخی حالت‌ها ممکن است قبل از اینکه ارزش دیگران یکبار به روز شود، چندین بار به روز شوند. با این حال، برای همگرایی صحیح، یک الگوریتم ناهمزمان باید بروزرسانی مقادیر همه حالت‌ها را ادامه دهد و نمی‌تواند حالتی را نادیده بگیرد.
الگوریتم‌های ناهمزمان DP امکان انعطاف‌پذیری زیادی را در سیستم فراهم می‌کنند و در بسیاری از مسائل، تنها گزینه قابل اجرا به حساب می‌آیند.

\section{Q-learning}
ایده اصلی در روش 
Q-learning
، تخمین تابع مقدار عمل  
$Q^*(s,a)$ 
با استفاده از معادله بلمن به عنوان یک بروزرسانی تکراری،
$$Q_{i+1}(s,a) = \mathbb{E}[r+ \gamma \max_{a'} Q_i(s',a')|s,a]$$
چنین الگوریتم‌های تکرار مقداری به تابع عمل-ارزش بهینه همگرا می‌شوند، 
$Q_i \longrightarrow Q^*$
وقتی
$i \longrightarrow \infty$
، این رویکرد کلی کاملا غیر عملی است.
زیرا تابع عمل-ارزش برای هر دنباله، به طور جداگانه و بدون هیچ گونه تعمیم برآورد می‌شود. در عوض، معمولاً از یک تخمین‌گر توابع (مثل شبکه عصبی) برای تخمین تابع عمل-ارزش استفاده می‌شود. در فصل سوم با این روش بیشتر آشنا خواهیم شد.