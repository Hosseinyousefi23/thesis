
\فصل{مفاهیم اولیه یادگیری تقویتی}

اصطلاح \textit{کنترل بهینه}
\پاورقی{Optimal control}
در اواخر دهه 1950 برای توصیف مسئله طراحی یک کنترل‌گر برای به حداقل رساندن اندازه‌گیری رفتار \textit{سیستم دینامیکی}\پاورقی{Dynamical system} در طول زمان مورد استفاده قرارگرفت. یکی از رویکردهای این مسئله در اواسط دهه 1950 توسط ریچارد بلمن\پاورقی{Richard Bellman} و دیگران از طریق گسترش نظریه قرن نوزدهم همیلتون\پاورقی{Hamilton} و جاکوبی\پاورقی{Jacobi} توسعه یافت. این رویکرد از مفاهیم حالت یک سیستم دینامیکی و یک \textit{تابع ارزش}\پاورقی{Value function} برای تعریف یک معادله تابعی استفاده می‌کند؛ که اکنون \textit{معادله بلمن} \پاورقی {Bellman equation}نامیده می‌شود. مجموعه روش های حل مسائل کنترل بهینه به کمک معادله بلمن به عنوان برنامه‌ریزی پویا شناخته می‌شود. همچنین بلمن نسخه گسسته از مسئله کنترل بهینه را که تحت عنوان \textit{فرایندهای تصمیم‌گیری مارکوف} 
\پاورقی{Markov decision process(MDP)}
 شناخته می‌شود، معرفی کرد. رونالد هوارد (1960) روش
\lr{Policy Iteration}
  را برای MDP ها طراحی کرد. همه این‌ها عناصر اساسی در تئوری و الگوریتم‌های \textit{یادگیری تقویتی}\پاورقی{Reinforcement learning (RL) } مدرن هستند.
  در این فصل، با فرایندهای تصمیم‌گیری مارکوف آشنا خواهیم‌شد و چهار عنصر اصلی یادگیری تقویتی، یعنی \textit{خط‌مشی}\پاورقی{Policy}، \textit{سیگنال پاداش}\پاورقی{Reward signal}، تابع ارزش و محیط\پاورقی{Environment} را دقیقا تعریف خواهیم‌کرد. همچنین برخی روش‌های کلاسیک در یادگیری تقویتی را معرفی می‌کنیم.

%----------------------------- مقدمه ----------------------------------
\قسمت{دینامیک عامل-محیط}
حوزه یادگیری تقویتی  دو بازیگر اصلی دارد: عامل و محیط. موجود تصمیم گیرنده و آموزنده، عامل یادگیری یا به اختصار، عامل نامیده می شود. قسمتی که عامل با آن تعامل دارد ، شامل
هر چیز خارج از عامل ، محیط نامیده می شود.  در ادبیات کنترل بهینه، معمولا به  جای واژه های عامل و محیط، از کنترل کننده و سیستم کنترل شده استفاده می شود.
 عامل و محیط به طور مداوم با یکدیگر ارتباط برقرار می کنند ، عامل انتخاب می کند که چه اقدامی انجام دهد و محیط، به این اقدامات پاسخ می دهد و موقعیت جدیدی را به عامل ارائه می دهد.
محیط همچنین مقادیر عددی ویژه ای به نام پاداش  به عامل برمیگرداند، که عامل به دنبال به حداکثر رساندن آن است

به طور خاص ، عامل و محیط در یک توالی زمانی گسسته تعامل می کنند 
$t = 0,1,2,3,...$
در هر مرحله $t$ ، عامل وضعیت محیط  
$S_t \in \EuScript{S}$
را دریافت می کند، و بر اساس آن یک عمل 
$A_t \in \EuScript{A}$
را انتخاب می کند. در گام بعدی، عامل به عنوان نتیجه عمل خود، یک پاداش عددی $R_{t+1} \in \EuScript{R}$ دریافت می کند و خود را در حالت جدید $S_{t+1}$ می یابد.
دینامیک عامل-محیط را می توان به شکل یک دنباله از حالت ها، عمل ها و پاداش ها به شکل زیر نمایش داد:
$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ...$$

\قسمت{فرایند تصمیم\nf گیری مارکوف}

یادگیری تقویتی از چارچوب رسمی فرایندهای تصمیم‌گیری مارکوف (MDP) برای تعریف تعامل بین یک عامل یادگیری و محیط آن توسط حالت‌ها، اقدامات و پاداش استفاده می‌کند. مدل MDP یک مدل کلاسیک از تصمیم‌گیری متوالی است، جایی که اقدامات نه تنها بر پاداش‌های فوری، بلکه بر موقعیت‌ها و حالت‌های بعدی و به تبع آن بر پاداش های آینده تأثیر می‌گذارد.
MDP
 یک فرم ایده‌آل ریاضی از مسئله یادگیری تقویتی است که برای آن تئوری‌های دقیقی بیان شده‌است.
MDP متناهی ، یک MDP با مجموعه حالت‌های محدود است. 
بیشتر نظریه‌های فعلی یادگیری تقویتی، محدود به MDP متناهی است، اما روش‌ها و ایده‌ها به طور کلی بیان می‌شوند.


%[Szepesvari2010]
\شروع{تعریف}[فرایند تصمیم‌گیری مارکوف] 
 فرایند تصمیم‌گیری مارکوف

،یک 4 تایی 
$$\EuScript{M} = \seq{\EuScript{S},\EuScript{A},\EuScript{R},\EuScript{P}}$$
است که
\شروع{فقرات}
\فقره $\EuScript{S}$ بیانگر مجموعه تمام \مهم{حالت}\nf هاست،
\فقره $\EuScript{A}$ بیانگر مجموعه تمام \مهم{عمل}\nf هاست،
\فقره{
	$\EuScript{R} \subseteq \IR$
	بیانگر مجموعه پاداش هاست،
}
\فقره 
$\EuScript{P}$
\مهم{هسته احتمال انتقال\پاورقی{Probability transition kernel}}
$ \EuScript{P}: \EuScript{S} \times \EuScript{A} \to \Pi(\EuScript{S} \times \EuScript{R})$
تابعی است که دینامیک MDP را مشخص می‌کند.

\پایان{فقرات}

\پایان{تعریف}
هسته احتمال انتقال یا تابع انتقال $\EuScript{P}$، هر دوتایی حالت-عمل
$(s,a)$، که 
$s \in \EuScript{S}$
و
$a \in \EuScript{A}$، را به یک توزیع احتمال روی دوتایی‌هایی به شکل 
$(s',r)$
نسبت می‌دهد که $s'$ بیانگر حالت بعدی و $r$ بیانگر پاداش این انتقال است. به ازای هر دو حالت 
$s,s' \in \EuScript{S}$
 و هر عمل 
 $a \in \EuScript{A}$
  و هر پاداش 
  $r \in \EuScript{R}$
  احتمال رسیدن به حالت $s'$ و دریافت پاداش $r$ با انتخاب عمل $a$ در حالت $s$، یک عدد حقیقی عضو $[0,1]$ است که آن را به شکل
$p(s',r|s,a)$
نمایش می‌دهیم:
$$p(s',r|s,a) \triangleq Pr\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\}$$

نامگذاری فرایند تصمیم‌گیری مارکوف اشاره به این موضوع دارد که این سیستم\nf ها دارای \مهم{ویژگی مارکوف} هستند، بدین معنا که تابع انتقال تنها به حالت فعلی سیستم و آخرین عمل وابسته است، و نسبت به حالت\nf ها و اعمال قبل از آن مستقل است.
\قسمت{خط‌مشی}
خط‌مشی\پاورقی{Policy} نحوه رفتار عامل یادگیری را در یک زمان خاص، مشخص می‌کند و هسته اصلی رفتار یک عامل یادگیری تقویتی است. خط‌مشی به تنهایی برای تعیین رفتار کافی است. به عنوان یک تعریف غیر دقیق، خط‌مشی، نگاشتی از حالت‌های مدل شده از محیط به اقداماتی است که باید در آن حالت انجام شود.
خط‌مشی ممکن است یک عملکرد ساده یا جدول جستجو باشد، یا ممکن است شامل محاسبات پیچیده‌ای مانند فرآیند جستجو باشد؛همچنین
خط‌مشی‌ها ممکن است تصادفی باشند.

\شروع{تعریف}[خط\nf مشی احتمالاتی ثابت]
%[Szepesvari2010]
یک خط\nf مشی احتمالاتی ثابت\پاورقی{Stationary probabilistic policy} (یا به طور خلاصه خط\nf مشی ثابت) 
$\pi: S \to \Pi(A)$
حالت\nf ها را به توزیع احتمال روی فضای عمل می\nf نگارد.
به طور خلاصه احتمال انتخاب عمل $a$ از حالت $s$ را با
$\pi(a|s)$
نشان می\nf دهیم.

\پایان{تعریف} می\nf گوییم خط\nf مشی $\pi$ در یک MDP \مهم{دنبال می\nf شود} هرگاه
$$A_t \sim \pi(. |X_t),	 \quad t \in \IN.$$

\قسمت{سیگنال پاداش}

یک سیگنال پاداش هدف را در یک مسئله یادگیری تقویتی تعریف می‌کند. در هر گام، محیط یک عدد حقیقی به نام پاداش برای عامل یادگیری تقویتی ارسال می‌کند. تنها هدف عامل، به حداکثر رساندن کل پاداش دریافتی در طولانی زمان است. سیگنال پاداش اتفاقات خوب و بد برای عامل را مشخص می‌کند و مبنای اصلی تغییر خط‌مشی است.
سیگنال پاداش می‌تواند تابعی تصادفی از وضعیت محیط و اقدام انجام شده باشد.


\قسمت{عایدی\پاورقی{Return} و تابع ارزش}
 سیگنال پاداش نشان می‌دهد که انجام چه عملی در هر گام خوب است، در حالی که تابع ارزش مشخص می\nf کند که کدام خط‌مشی در طولانی مدت بهتر است. در واقع تابع ارزش نشانگر مطلوبیت طولانی مدت حالت‌ها پس از در نظر گرفتن حالت‌هایی است که احتمالاً در پی خواهند داشت.
ارزش حالت $s$، مجموع میزان پاداشی است که عامل می‌تواند انتظار داشته باشد با شروع از $s$ در آینده کسب کند.
پاداش‌ها به یک معنا اولیه هستند، در حالی که ارزش‌ها، به عنوان پیش‌بینی پاداش ها، ثانویه هستند. بدون پاداش هیچ ارزشی وجود ندارد و تنها هدفِ تخمین ارزش‌ها، دستیابی به پاداش بیشتر است. با این وجود، این تابع ارزش است که هنگام تصمیم‌گیری و ارزیابی بیشتر به آن توجه می‌کنیم.
تعیین ارزش‌ بسیار دشوارتر از تعیین پاداش است
ارزش‌ها باید از توالی مشاهداتی که یک عامل در طول عمر خود انجام می‌دهد، تخمین زده و مجدد برآورد شوند.
مهمترین مولفه بیشتر الگوریتم‌های یادگیریِ تقویتی که در نظر می‌گیریم، روشی برای تخمین کارآمد تابع ارزش است.

\شروع{تعریف}
\مهم{عایدی تخفیف دار آینده}\پاورقی{Future discounted Return} یا به اختصار، عایدی، در زمان $t$ به شکل

$$G_t \triangleq \sum_{t'=t}^{T} \gamma^{t'-t} R_{t'}$$
تعریف می‌شود که $T$ زمانی است که اپیزود به اتمام می‌رسد. اگر مسئله مستمر باشد آنگاه 
$T=\infty$
\پایان{تعریف}

از تعریف بالا نتیجه می‌شود 
\begin{align}
G_t =& R_t + \sum_{t'=t+1}^{T} \gamma^{t'-t} R_{t'} \nonumber \\
=& R_t + \gamma \sum_{t'=t+1}^{T} \gamma^{t' - (t+1)} r_{t'} \nonumber \\
=& R_t + \gamma G_{t+1}. \numberthis
\end{align}

\شروع{تعریف}[تابع ارزش حالت]

 ارزش حالت $s$ تحت خط‌مشی $\pi$ یا $v_\pi(s)$ به شکل امیدریاضی عایدی، با شروع از $s$ و دنبال کردن خط‌مشی $\pi$ تعریف می‌شود.
$$v_\pi(s) = \mathbb{E}_\pi\left[G_t| S_t=s\right] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t =s \right]$$
تابع $v_\pi$ را 
\textit{تابع ارزش حالت}
\پاورقی{State Value Function} مربوط به خط‌مشی 
$\pi$
می‌نامیم.
\پایان{تعریف}

\شروع{تعریف}[تابع ارزش عمل]

ارزش عمل  $a$ در حالت
$s$
 تحت خط‌مشی $\pi$ یا 
 $q_\pi(s,a)$
  به شکل امیدریاضی عایدی، با شروع از $s$ و انتخاب عمل $a$  و سپس دنبال کردن خط‌مشی $\pi$ تعریف می‌شود.
$$q_\pi(s,a) = \mathbb{E}_\pi\left[G_t| S_t=s, A_t=a\right] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t =s, A_t=a \right]$$
تابع $q_\pi$ را 
\textit{تابع ارزش عمل}
\پاورقی{Action Value Function} مربوط به خط‌مشی 
$\pi$
می‌نامیم.
\پایان{تعریف}

\قسمت{محیط}
مدلِ محیط رفتار محیط را تقلید می‌کند، یا به طور کلی‌تر، اجازه می‌دهد تا در مورد نحوه رفتار محیط، پیشبینی کارآمدی داشته‌باشیم. از مدل‌ها برای برنامه‌ریزی و انتخاب در روند تصمیم‌گیری  با در نظر گرفتن شرایط احتمالی آینده بدون تجربه واقعی آنها استفاده می‌شود.
روش‌هایی که برای حل مشکلات یادگیری تقویتی از مدل‌ها و برنامه‌ریزی‌ها استفاده می‌کنند، روش‌های مبتنی بر مدل نامیده می‌شوند. این روش‌ها در مقابل روش‌های بدون مدل هستند که همگی آزمون و خطا هستند.

\قسمت{خط\nf مشی و تابع ارزش بهینه}
برای MDP های متناهی، می\nf توانیم خط\nf مشی بهینه\پاورقی{Optimal policy} را به صورت زیر تعریف کنیم
\شروع{تعریف}
می\nf گوییم خط\nf مشی $\pi$ \مهم{بهتر از} خط\nf مشی 
$\pi'$
است و می\nf نویسیم 
$\pi \ge \pi'$
هرگاه برای هر 
$s \in \mathcal{S}$

$$v_\pi(s) \ge v_{\pi'}(s).$$
\پایان{تعریف} می\nf توان نشان داد که حداقل یک خط\nf مشی وجود دارد که بهتر یا مساوی هر خط\nf مشی دیگری باشد

\cite{suttonbook}
. به چنین خط\nf مشی\nf ای \مهم{خط\nf مشی بهینه} گفته می\nf شود. ممکن است بیش از یک خط\nf مشی بهینه وجود داشته باشد ولی تابع ارزش متناظر با همه خط\nf مشی \nf های بهینه یکسان است و برابر با تابع ارزش بهینه است که با نماد $v_*$ نمایش داده شده و به شکل زیر تعریف می\nf شود. برای هر $s \in S$
$$v_*(s) \triangleq \max_{\pi} v_\pi(s)$$
همچنین تمام خط\nf مشی\nf های بهینه تابع عمل-ارزش مشترکی دارند که آن را با نماد $q_*$ نمایش می\nf دهیم و به شکل زیر تعریف می\nf شود. برای هر دوتایی حالت عمل $(s,a)$ که $s \in S$ و $a \in A(s)$
$$q_*(s,a) \triangleq max_{\pi} q_\pi(s,a)$$
می\nf توانیم $q_*$ را برحسب $v_*$ به شکل زیر بنویسیم
\begin{equation}
q_* (s,a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1})| S_t=s, A_t=a]
%\label{•}
\end{equation}

\قسمت{معادله بلمن}

ویژگی اساسی توابع ارزش که در طول یادگیری تقویتی و برنامه‌ریزی پویا استفاده می‌شوندصدق کردن در روابط بازگشتی است. معادلات بلمن رابطه‌ای بین ارزش یک حالت و ارزش‌های حالت‌های بعدی آن را بیان می‌کند
\begin{align}
v_\pi (s) \triangleq& \mathbb{E}_\pi [G_t | S_t = s] \nonumber \\
=& \mathbb{E}_\pi [R_{t+1}+ \gamma G_{t+1}|S_t=s] \nonumber \\
=&\sum_{a} \pi(a|s) \sum_{s'}\sum_{r} p(s',r|s,a)[r+\gamma \mathbb{E}_\pi[G_{t+1}|S_{t+1}=s']] \nonumber\\
=&\sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a)[r+\gamma v_\pi(s')] \quad \forall s \in \mathbb(S) \numerhis 
\label{bellman}
\end{align}
\قسمت{بهینگی و معادله بهینگی بلمن}

\begin{align}
  v_{*}(s)= &\max_{a \in \mathbb{A}(s)} q_{\pi_*}(s,a) \nonumber \\
      =& \max_{a} \mathbb{E}_{\pi_*} [G_t|S_t=s, A_t=a] \nonumber \\ 
      =& \max_{a} \mathbb{E}_{\pi_*} [R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a] \nonumber \\ 
      =& \max_{a} \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t= s, A_t = a] \nonumber \\
      =& \max_{a} \sum_{s',r} p(s',r|s,a) [r + \gamma v_*(s')]. \numberthis
\label{eq:bellman-opti}
\end{align}




$q_*(s,a) = \mathbb{E}\left[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1},a')| S_t=s, A_t=a \right] \\
= \sum_{s',r} p(s',r|s,a) \left[r + \gamma \max_{a'} q_*(s',a')\right] \\
$
\قسمت{برنامه‌ریزی پویا}
\textit{برنامه‌ریزی پویا}  \پاورقی{Dynamic Programing(DP)} مجموعه‌ای از الگوریتم‌ها است که برای محاسبه خط‌مشی بهینه استفاده می‌شود.
الگوریتم‌های کلاسیک برنامه‌ریزی پویا به دلیل فرض
مدل کاملی از محیط و همچنین هزینه محاسباتی زیادشان، به لحاظ عملی چندان قابل استفاده نیستند اما به لحاظ نظری مهم هستند.

ایده اصلی DP و به طور کلی یادگیری تقویتی‌، استفاده از تابع ارزش حالت یا عمل برای سازماندهی یک الگوریتم جستجو برای خط‌مشی بهینه است.
هدف، تخمین تابع ارزش بهینه،
$v_*$
یا
$q_*$
است
که در معادلات بهینگی بلمن صدق می‌کند:
\begin{equation}
v_{*}(s) = \max_{a} \sum_{s',r} p(s',r | s,a)[r + \gamma v_*(s')]
\label{bellman_opt_state}
\end{equation}
\begin{equation}
q_{*}(s,a) = \sum_{s',r} p(s',r | s,a)[r + \gamma \max_{a'} q_* (s',a')]
\label{bellman_opt_action}
\end{equation}
%\قسمت{ارزیابی خط‌مشی}
اگر دینامیک محیط کاملاً مشخص باشد معادلات 
\ref{bellman_opt_state}
و
\ref{bellman_opt_action}
به ترتیب یک دستگاه معادلات خطی با
 $|\EuScript{S}|$
  است
 که راه‌حل آن سرراست است.
 برای اهداف ما، روش‌های تکراری مناسب‌ترین روش‌ها هستند. یک دنباله 
از توابع ارزش تقریبی
$ v_0 , v_1 , v_2 , ...$ 
را در نظر بگیرید که هرکدام نگاشتی از
$\EuScript{S}$ 
به 
$\mathbb{R}$
 هستند.
تقریب اولیه $v_0$
به طور دلخواه انتخاب می‌شود(به جز در حالت های پایانی که باید صفر باشد).

 هر تقریب موفقی به عنوان یک قانون به‌روز رسانی به وسیله معادله بلمن برای
$v_\pi$
بدست می‌آید. 
%If the environment’s dynamics are completely known, then (4.4) is a system of |S| simultaneous linear
%equations in |S| unknowns (the vπ(s), s ∈ S). In principle, its solution is a straightforward, if tedious,
%computation. For our purposes, iterative solution methods are most suitable. Consider a sequence
%of approximate value functions v0, v1, v2, . . ., each mapping S
%+ to R (the real numbers). The initial
%approximation, v0, is chosen arbitrarily (except that the terminal state, if any, must be given value 0),
%and each successive approximation is obtained by using the Bellman equation for vπ (4.4) as an update
%rule:
\begin{align}
v_{k+1}(s) \triangleq & \mathbb{E}_{\pi} [R_{t+1} + \gamma v_k(S_{t+1}) | S_t=s]  \nonumber \\
=& \sum_{a} \pi(a|s) \sum_{s',r} p(s',r | s,a)[r + \gamma v_k(s')]
\end{align}
 برای هر 
 $s \in \EuScript{S}$.
 بر اساس معادله بلمن می‌توان درستی این تساوی را برای 
$v_{\pi}$
نوشت و واضح است که 
 $v_k = v_{\pi}$
 نقطه ثابتی برای این قانون بروز رسانی است.
 در واقع، می‌توان نشان داد که در حالت کلی دنباله 
 $\{ v_k \}$
وفتی که 
$ k \rightarrow \infty $
و تحت همان شرایط که وجود 
$v_{\pi}$
را تضمین می‌کند، به مقدار 
$v_{\pi}$
همگرا می‌شود.  این الگوریتم را ارزیابی خط‌مشی تکراری
\پاورقی{Iterative policy evaluation}
 می‌نامند.
 
% 
%for all s ∈ S. Clearly, vk = vπ is a fixed point for this update rule because the Bellman equation for vπ
%assures us of equality in this case. Indeed, the sequence {vk} can be shown in general to converge to
%vπ as k → ∞ under the same conditions that guarantee the existence of vπ. This algorithm is called
%iterative policy evaluation.
%

\قسمت{بهبود خط‌مشی}

دلیل ما برای محاسبه تابع ارزش یک خط‌مشی کمک به یافتن خط‌مشی‌های بهتر است.
فرض کنید ما تابع ارزش 
$v_{\pi}$
 را برای یک خط‌مشی تعیین‌گرایانه دلخواه 
 $\pi$
  تعیین کرده‌ایم. برای یک حالت 
  $s$
 ما می‌خواهیم بدانیم که آیا باید خط‌مشی را برای انتخاب قطعی یک عمل 
 $a$
  تغییر دهیم یا خیر.
ما می‌دانیم که پیروی از خط‌مشی فعلی از حالت
 $s$
  چقدر خوب است، اما آیا تغییر به سیاست جدید بهتر است یا بدتر؟
  یکی از راه های پاسخ به این سوال در نظر گرفتن انتخاب عمل  $a$
در حالت 
$s$
و پس از آن پیروی از خط مشی موجود
($\pi$)
 است. 
 ارزش این شیوه رفتار این است که ؟؟؟؟؟؟

%Our reason for computing the value function for a policy is to help find better policies. Suppose we
%have determined the value function vπ for an arbitrary deterministic policy π. For some state s we
%would like to know whether or not we should change the policy to deterministically choose an action
%a 6= π(s). We know how good it is to follow the current policy from s—that is vπ(s)—but would it be better or worse to change to the new policy? One way to answer this question is to consider selecting a in s and thereafter following the existing policy, π. The value of this way of behaving is


\شروع{قضیه}[قضیه‌ی بهبود خط‌مشی]
فرض کنید $\pi$ و 
$\pi_0$
دو خط‌مشی معین باشند که برای هر 
$s \in S$

$$q_\pi(s, \pi'(s)) \ge \pi'(s)$$
در این صورت 
$v_{\pi'}(s) \ge v_\pi(s)$
\پایان{قضیه}

درستی قضیه بالا را می\nf توان با استفاده از تعاریف به روشنی بررسی کرد

\begin{align*}
v_\pi (s) \le q_\pi (s,\pi'(s)) = & \mathbb{E}\left[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t = \pi'(a)\right] \\
= & \mathbb{E}_{\pi'}\left[R_{t+1}+\gamma v_\pi(s_{t+1}) | S_t=s\right] \\
 \le &  \mathbb{E}_{\pi'}[R_{t+1}+ \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t=s] \\
= & \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2})] | S_t=s] \\
\le & \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_\pi(s_{t+3}) | S_t=s] \\
\vdots \\
\le & \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots | S_t=s] \\
\le & v_{\pi'}(s)
\end{align*}


\قسمت{الگوریتم  خط‌مشی تکراری}
در الگوریتم خط‌مشی تکراری  \پاورقی{Policy iteration}هنگامی‌که یک خط‌مشی
$\pi$
 با استفاده از 
$v_{\pi}$
  بهبود یافته است تا به خط‌مشی بهتری
$\pi_0$
    برسیم، سپس می‌توانیم
$v_{\pi_0}$
    را محاسبه کرده و مجدداً آن را بهبود بخشیم تا
$\pi_{00}$
بهتر داشته باشیم.
بنابراین ما می توانیم دنباله‌ای از خط‌مشی‌ها و تابع ارزش‌هایی بدست آوریم که به صورت یکنوا در حال بهبود هستند:
%
%Once a policy, π, has been improved using vπ to yield a better policy, π0, we can then compute vπ0 and
%improve it again to yield an even better π00. We can thus obtain a sequence of monotonically improving
%policies and value functions:

$$\pi_0 \longrightarrow v_{\pi_0} \longrightarrow \pi_1 \longrightarrow v_{\pi{1}} \longrightarrow \pi_2 \longrightarrow \dots \longrightarrow \pi_* \longrightarrow v_*$$

\قسمت{الگوریتم Value Iteration}
یک اشکال در تکرار خط مشی این است که هر یک از تکرارهای آن شامل ارزیابی سیاست است که ممکن است باشد
به خودی خود یک محاسبه تکراری طولانی مدت است که نیاز به رفت و برگشت های متعدد از طریق مجموعه حالت دارد. اگر سیاست
ارزیابی بصورت تکراری انجام می شود ، سپس همگرایی دقیقاً به vπ فقط در حد مجاز اتفاق می افتد. باید منتظر بمانیم
برای همگرایی دقیق ، یا می توانیم کوتاه تر از آن متوقف شویم؟ مثالی که در شکل 4.1 وجود دارد قطعاً این را نشان می دهد
کوتاه کردن ارزیابی سیاست ممکن است باشد. در آن مثال ، تکرار ارزیابی سیاست فراتر از آن است
سه مورد اول هیچ تاثیری در سیاست حریص مربوطه ندارند.
در واقع ، مرحله ارزیابی سیاست تکرار سیاست می تواند از چند طریق بدون ضرر کوتاه شود
همگرایی تضمین سیاست را تضمین می کند. یک مورد خاص مهم ، ارزیابی سیاسی است
فقط پس از یک بار جابجایی متوقف می شود (یک به روزرسانی از هر حالت). این الگوریتم تکرار مقدار نامیده می شود. آی تی
را می توان به عنوان یک عملیات به روزرسانی ساده نوشت که ترکیبی از بهبود سیاست و
مراحل ارزیابی سیاست کوتاه شده:
One drawback to policy iteration is that each of its iterations involves policy evaluation, which may
itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy
evaluation is done iteratively, then convergence exactly to vπ occurs only in the limit. Must we wait
for exact convergence, or can we stop short of that? The example in Figure 4.1 certainly suggests that
it may be possible to truncate policy evaluation. In that example, policy evaluation iterations beyond
the first three have no effect on the corresponding greedy policy.
In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing
the convergence guarantees of policy iteration. One important special case is when policy evaluation
is stopped after just one sweep (one update of each state). This algorithm is called value iteration. It
can be written as a particularly simple update operation that combines the policy improvement and
truncated policy evaluation steps:



for all s ∈ S. For arbitrary v0, the sequence {vk} can be shown to converge to v∗ under the same
conditions that guarantee the existence of v∗.

\قسمت{درهم تنیدگی PE و PI و الگوریتم های GPI}


A major drawback to the DP methods that we have discussed so far is that they involve operations
over the entire state set of the MDP, that is, they require sweeps of the state set. If the state set is very
large, then even a single sweep can be prohibitively expensive


Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms
of systematic sweeps of the state set. These algorithms update the values of states in any order
whatsoever, using whatever values of other states happen to be available. The values of some states
may be updated several times before the values of others are updated once. To converge correctly,
however, an asynchronous algorithm must continue to update the values of all the states: it can’t ignore
any state after some point in the computation. Asynchronous DP algorithms allow great flexibility in
selecting states to update.

\قسمت{Q-learning}
ایده اصلی در روش Q-learning، تخمین تابع مقدار عمل  
$Q^*(s,a)$ 
با استفاده از معادله بلمن به عنوان یک به روزرسانی تکراری ،
$$Q_{i+1}(s,a) = \mathbb{E}[r+ \gamma \max_{a'} Q_i(s',a')|s,a]$$
چنین الگوریتم های تکرار مقداری به تابع عمل-ارزش بهینه همگرا می‌شوند، 
$Q_i \longrightarrow Q^*$
وقتی
$i \longrightarrow \infty$

در عمل ، این رویکرد کلی کاملا غیر عملی است ،
زیرا تابع عمل-ارزش برای هر دنباله، به طور جداگانه و بدون هیچ گونه تعمیم برآورد می‌شود. در عوض، معمولاً از یک تخمین گر توابع (مثل شبکه عصبی) برای تخمین تابع عمل-ارزش استفاده می‌شود، در فصل سوم با این روش بیشتر آشنا خواهیم شد.