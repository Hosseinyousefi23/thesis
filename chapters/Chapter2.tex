
\فصل{مفاهیم اولیه و الگوریتم‌های کلاسیک}
  در این فصل، با فرایندهای تصمیم‌گیری مارکوف آشنا خواهیم‌شد و چهار عنصر اصلی یادگیری تقویتی، یعنی \textit{خط‌مشی}\LTRfootnote{Policy}، \textit{سیگنال پاداش}\LTRfootnote{Reward signal}، تابع ارزش و محیط\LTRfootnote{Environment} را دقیقا تعریف خواهیم‌کرد. همچنین برخی روش‌های کلاسیک در یادگیری تقویتی مانند برنامه ریزی پویا را معرفی می‌کنیم.

%----------------------------- مقدمه ----------------------------------


\section{فرایند تصمیم‌گیری مارکوف}

یادگیری تقویتی گسسته، از چارچوب رسمی \textit{فرایندهای تصمیم‌گیری مارکوف} 
\LTRfootnote{Marcov Decision Process} (MDP)
 برای تعریف تعامل بین یک عامل یادگیری و محیط توسط حالت‌ها، اقدامات و پاداش استفاده می‌کند. مدل MDP یک مدل کلاسیک از تصمیم‌گیری متوالی است، جایی که اقدامات نه تنها بر پاداش‌های فوری، بلکه بر موقعیت‌ها و حالت‌های بعدی و به تبع آن بر پاداش‌های آینده تأثیر می‌گذارد
 \cite{suttonbook}.
MDP
 یک فرم ایده‌آل ریاضی از مسئله یادگیری تقویتی است که برای آن تئوری‌های دقیقی بیان شده‌است.
MDP متناهی،
 یک MDP با مجموعه حالت‌های متناهی است. 
بیشتر نظریه‌های فعلی یادگیری تقویتی، محدود به MDP  های متناهی است، اما روش‌ها و ایده‌ها به طور کلی بیان می‌شوند.
%[Szepesvari2010]
\شروع{تعریف}[فرایند تصمیم‌گیری مارکوف] 
یک فرایند تصمیم‌گیری مارکوف، 5 تایی 
$\EuScript{M} = \seq{\EuScript{S},\EuScript{A},\EuScript{R},\EuScript{P}, \lambda}$
است که
\شروع{فقرات}
\فقره{$\EuScript{S}$ بیانگر مجموعه تمام \مهم{حالت‌ها}ست،}
\فقره{$\EuScript{A}$ بیانگر مجموعه تمام \مهم{عمل‌}\nf‌ هاست،}
\فقره{
	$\EuScript{R} \subseteq \IR$
	بیانگر مجموعه \مهم{پاداش}‌هاست،
}
\فقره{ 
$\EuScript{P}$
\مهم{هسته احتمال انتقال\LTRfootnote 
	{Transition Probability Kernel}} ،
$ \EuScript{P}: \EuScript{S} \times \EuScript{A} \to P(\EuScript{S} \times \EuScript{R})$، 
تابعی است که دینامیک MDP را مشخص می‌کند.
}
\فقره{
	$0 \le \lambda < 1$
\textit{\مهم{ضریب کاهش}}
\LTRfootnote{Discounting Factor}
نامیده می‌شود.
}
\پایان{فقرات}

\پایان{تعریف}
هسته احتمال انتقال یا تابع انتقال $\EuScript{P}$، هر دوتایی حالت-عمل
$(s,a)$ که 
$s \in \EuScript{S}$
و
$a \in \EuScript{A}$، را به یک توزیع احتمال روی دوتایی‌هایی به شکل 
$(s',r)$
نسبت می‌دهد که $s'$ بیانگر حالت بعدی و $r$ بیانگر پاداش این انتقال است. به ازای هر دو حالت 
$s,s' \in \EuScript{S}$
 و هر عمل 
 $a \in \EuScript{A}$
  و هر پاداش 
  $r \in \EuScript{R}$
  احتمال رسیدن به حالت $s'$ و دریافت پاداش $r$ با انتخاب عمل $a$ در حالت $s$، یک عدد حقیقی متعلق به  بازه $[0,1]$ است که آن را به شکل
$p(s',r|s,a)$
نمایش می‌دهیم:
$$p(s',r|s,a) \triangleq Pr\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\}.$$ 
نام‌گذاری فرآیند تصمیم‌گیری مارکوف اشاره به این موضوع دارد که این سیستم‌ دارای \مهم{ویژگی مارکوف \LTRfootnote{Marcov Property}}  است، یعنی تابع انتقال آن تنها به حالت فعلی سیستم و آخرین عمل انجام شده وابسته است، و نسبت به حالت‌ها و اعمال قبلی مستقل است.

\section{یک مثال ساده: ربات قوطی یاب}
%The recycling robot example was inspired by the can-collecting robot built by Jonathan Connell
%(1989).
یک ربات قابل حمل، وظیفه جمع‌آوری قوطی‌های نوشابه خالی در 
یک محیط اداری دارد. این ربات، دارای سنسورهایی برای تشخیص قوطی‌ها است و بازو و گیره‌ای دارد که می‌تواند آن‌ها را بردارد و در سطل آشغال قرار دهد. این ربات  با باتری قابل شارژ کار می‌کند. سیستم کنترلی ربات
دارای مولفه‌هایی برای تفسیر اطلاعات حسی، جهت یابی و کنترل بازو و گیره است. تصمیم در مورد چگونگی جستجوی قوطی‌ها توسط یک عامل یادگیری تقویتی بر اساس میزان شارژ باتری اتخاذ می‌شود. این عامل باید تصمیم بگیرد که: 
\begin{itemize}
	\item  ربات باید برای مدت زمان مشخصی به طور فعال به جستجوی  یک قوطی بپردازد؟
	\item ثابت باشد و منتظر کسی بماند که قوطی را برایش بیاورد!
	\item  برای شارژ مجدد باتری به جایگاه اختصاصی‌اش  برود!
\end{itemize}
این تصمیم‌ها نیز مانند یافتن قوطی خالی باید به صورت مکرر و در مواقع خاص گرفته شود. مواقع تصمیم‌گیری، توسط رویدادهای بیرونی یا سایر قسمت‌های سیستم کنترلی ربات، تعیین می‌شود. بنابراین عامل
دارای سه عمل است و با توجه به وضعیت باتری تصمیم‌گیری می‌کند. ممکن است در ابتدا پاداشی در کار نباشد ولی عامل با یافتن هر قوطی خالی،  پاداش مثبتی دریافت می‌کند. اگر باتری ربات، قبل از رسیدن به جایگاه شارژ مجدد، تمام شود، پاداش منفی و بزرگی نیز بعنوان مجازات به عامل ارسال می\nf شود.

در ساده‌ترین شکل ممکن، تعامل بین این عامل یادگیری و محیط، می‌تواند به شکل یک فرآیند تصمیم‌گیری مارکوف ساده با دو حالت «سطح انرژی \مهم{بالا}» و «سطح انرژی \مهم{پایین}»، 
${\EuScript{S} = \{high, low\}}$ 
و سه عمل \مهم{جستجو} برای یافتن قوطی، \مهم{منتظر ماندن} برای جلوگیری از اتلاف انرژی و \مهم{بازگشت} به جایگاه شارژ مجدد 
$\EuScript{A}=\{search, wait, recharge\}$
بیان شود. بهترین راه برای یافتن قوطی‌ها، جستجو کردن آن‌هاست؛ اما عملیات جستجو موجب مصرف باتری می‌شود و ریسک تمام شدن باتری در حین جستجو وجود دارد. در عوض، منتظر ماندن، انرژی چندانی مصرف نمی کند، اما شانس پیدا کردن قوطی را نیز کاهش می‌دهد و از این رو، پاداش کمتری به همراه دارد. درصورت خالی شدن باتری، ربات باید توسط متصدی شرکت به جایگاه شارژ مجدد منتقل شود. این اتفاق، مطلوب نیست و در صورت وقوع، پاداش منفی برای عامل به همراه دارد. 
اگر سطح انرژی عامل، $high$  باشد، عامل همیشه می تواند یک گام، جستجو انجام دهد بدون اینکه خطر اتمام باتری وجود داشته باشد.
پس از یک گام جستجو با سطح انرژی $high$، انرژی عامل با احتمال
 $\alpha$
 در سطح $high$ باقی می ماند و با احتمال 
 $1-\alpha$
 به سطح انرژی $low$ کاهش می‌یابد. از سوی دیگر یک دوره
جستجو در سطح انرژی $low$، آن را با احتمال $\beta$ در سطح $low$ نگاه می‌دارد ولی خطر تخلیه باتری با احتمال $1-\beta$ وجود دارد. در صورت تخلیه باتری، ربات و باتری باید نجات یابند و سپس باتری دوباره شارژ می‌شود و به سطح انرژی $high$ بازمی‌گردد. 
فرض کنید پاداش پیدا کردن هر قوطی، یک واحد باشد. اگر ربات با هر عمل جستجوی موفق (بدون تخلیه باتری) به طور میانگین $r_{search}$ قوطی پیدا کند و هر عمل «منتظر ماندن» به طور میانگین $r_{wait}$ قوطی به همراه داشته باشد، 
 پاداش چشمداشتی اعمال $search$ (اگر موفقیت آمیز باشد)  و $wait$ به ترتیب برابر $r_{search}$  و 
$r_{wait}$
خواهد بود و البته $r_{search} > r_{wait}$. همچنین پاداش چشمداشتی عمل 
   $recharge$
   برابر صفر و پاداش تخلیه باتری در حین جستجو، 
   $-3$
است. جدول 
\ref{tab:recycling}
 احتمالات انتقال و پاداش‌های چشمداشتی در فرآیند تصمیم‌گیری مارکوف مربوط به این مثال را نشان می‌دهد\cite{suttonbook}.
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

%\شرح{عملگرهای مقایسه‌ای}
%\برچسب{جدول:عملگرهای مقایسه‌ای}

\begin{table}[]
	\centering
	\lr{
	\begin{tabular}{@{}ccc|c|c@{}}
		$s$    & $a$        & $s'$   & $p(s'|s,a)$ & $\mathbb{E}[r(s,a,s')]$  \\ \midrule
		$high$ & $search$   & $high$ & $\alpha$    & $r_{search}$ \\
		$high$ & $search$   & $low$  & $1- \alpha$  & $r_{search}$ \\
		$low$  & $search$   & $low$  & $\beta$     & $r_{search}$ \\
		$low$  & $search$   & $high$ & $1-\beta$   & $-3$         \\
		$high$ & $wait$     & $high$ & $1$         & $r_{wait}$   \\
		$high$ & $wait$     & $low$  & $0$         & $r_{wait}$   \\
		$low$  & $wait$     & $low$  & $1$         & $r_{wait}$   \\
		$low$  & $wait$     & $high$ & $0$         & $r_{wait}$   \\
		$high$  & $recharge$ & $high$ & $1$         & $0$          \\
		$high$  & $recharge$ & $low$ & $0$         & $0$          \\
		$low$  & $recharge$ & $high$ & $1$         & $0$          \\
		$low$  & $recharge$ & $low$  & $0$         & $0$         
	\end{tabular}
	}
	\شرح{جدول توزیع انتقال و پاداش‌های چشمداشتی فرآیند تصمیم‌گیری مارکوف مربوط به ربات قوطی‌یاب، برگرفته از \cite{suttonbook}}
\برچسب{tab:recycling}
\end{table}

\شروع{شکل}[]
\centerimg{recycle.png}{10cm}
\شرح{گراف انتقال فرآیند تصمیم‌گیری مارکوف مربوط به ربات قوطی‌یاب برگرفته از \cite{suttonbook}}
\برچسب{fig:recycle}
\پایان{شکل}


\section{عناصر اصلی یادگیری تقویتی}
چهار عنصر کلیدی و اصلی مسائل یادگیری تقویتی عبارت است از:
\textit{خط‌مشی} \LTRfootnote{Policy}، 
\textit{سیگنال پاداش} \LTRfootnote{Reward Signal}،
\textit{تابع ارزش} \LTRfootnote{Value Function} 
و
\textit{محیط} \LTRfootnote{Environment}.
\subsection{خط‌مشی}

\شروع{تعریف}[خط‌مشی احتمالاتی ثابت]
%[Szepesvari2010]
یک خط‌مشی احتمالاتی ثابت\LTRfootnote{Stationary probabilistic policy} (یا به طور خلاصه خط‌مشی ثابت) 
$\pi: S \to \Pi(A)$
تابعی است که هر حالت را به یک توزیع احتمال روی فضای عمل
$\EuScript{A}$
 می‌نگارد.
برای سادگی، احتمال انتخاب عمل $a$ در حالت $s$ تحت خط‌مشی $\pi$ را به شکل
$\pi(a|s)$
نشان می‌دهیم.
اگر به ازای هر حالت $s$، عمل $a$ موجود باشد به طوری که 
$\pi(a|s) = 1$
در این صورت به $\pi$،
\textit{
	خط‌مشی قطعی
}
\LTRfootnote{Deterministic Policy}
گفته می‌شود. مرسوم است که برای خط‌مشی‌های قطعی به جای 
$\pi(a|s) = 1$،
می نویسیم
$\pi(s) = a$.

\پایان{تعریف} 

\شروع{تعریف}
می‌گوییم خط‌مشی $\pi$ در یک
 MDP
  \textit{
\مهم{دنبال 
  	می‌شود}
  }
   هرگاه
$$A_t \sim \pi(. |S_t),	 \quad t \in \IN.$$

\پایان{تعریف}



\subsection{عایدی و تابع ارزش}

ارزش حالت $s$، تحت خط‌مشی $\pi$، یا
 $v_\pi(s)$،
مجموع میزان پاداشی است که عامل، با شروع از $s$ و دنبال کردن خط‌مشی $\pi$ می‌تواند انتظار داشته باشد در آینده کسب کند.

\شروع{تعریف}
\textit{عایدی کاهشی آینده}
\LTRfootnote{Future Discounted Return}
 یا به اختصار، عایدی \LTRfootnote{Return}، در زمان $t$ به شکل

\begin{align}
G_t \doteq \sum_{k=t+1}^{T} \gamma^{k-t-1} R_{k}
\label{def:return}
\end{align}

تعریف می‌شود که $T$ زمانی است که دوره به اتمام می‌رسد\cite{suttonbook}. اگر MDP، مستمر باشد آنگاه 
$T=\infty$ و
در این صورت 
\ref{def:return}
 را می‌توان به فرم زیر نیز نوشت
 
\begin{align}
	G_t \doteq \sum_{k=0}^{\infty} \gamma^{k} R_{k+t+1}.
	\label{def:returninf}
\end{align}
 
\پایان{تعریف}

از
 \ref{def:return}
  نتیجه می‌شود 
\begin{flalign}
G_t =& \ R_{t+1} + \sum_{k=t+2}^{T} \gamma^{k-t-1} R_{k}  & \nonumber \\
=& \ R_{t+1} + \gamma \sum_{k=t+2}^{T} \gamma^{k - (t+1) - 1} R_{k} & \nonumber \\
=& \ R_{t+1} + \gamma G_{t+1}. \numberthis
\label{eq:returnres}
\end{flalign}

در ادامه این پایان نامه، از تعریف 
\ref{def:returninf}
در حالت کلی، بدون اینکه فرض کنیم MDP مستمر است، استفاده خواهیم کرد. برای اینکه این کار مشکلی ایجاد نکند، فرض می‌کنیم MDP های دوره‌ای بعد از رسیدن به حالت نهایی در زمان $T$، متوقف نمی‌شوند؛ اما در گام‌های 
$T+1, T+2, T+3, ...$
پاداش صفر تولید می‌شود
$$R_{T+1} = R_{T+2} = R_{T+3} = ... = 0$$
\شروع{تعریف}[تابع ارزش حالت]
 ارزش حالت \  
  $s$
   تحت خط‌مشی\  
 $\pi$،
 $v_\pi(s)$،
  به شکل امیدریاضی عایدی، با شروع از $s$ و دنبال کردن خط‌مشی $\pi$ تعریف می‌شود
$$v_\pi(s) \doteq \mathbb{E}_\pi\left[G_t| S_t=s\right] = \mathbb{E}_\pi\left[\sum_{k=t0}^{\infty} \gamma^{k} R_{k+t+1}|S_t =s \right]$$
که $\mathbb{E}_\pi[.]$
به معنی امیدریاضی  یک متغیرتصادفی با فرض دنبال شدن خط‌مشی $\pi$ است.

تابع $v_\pi$ را 
\textit{تابع ارزش حالت}\LTRfootnote{State Value Function} مربوط به خط‌مشی 
$\pi$
می‌نامیم. اگر $s$ یک حالت نهایی باشد آنگاه 
$v_\pi(s) = 0$.


\label{statevaluedef}
\پایان{تعریف}

\شروع{تعریف}[تابع ارزش عمل]
ارزش عمل  $a$ در حالت
\ $s$
 تحت خط‌مشی 
$\pi$،
$q_\pi(s,a)$،
  به شکل امیدریاضی عایدی، با شروع از $s$ و انتخاب عمل $a$  و سپس دنبال‌کردن خط‌مشی $\pi$ تعریف می‌شود.
$$q_\pi(s,a) = \mathbb{E}_\pi\left[G_t| S_t=s, A_t=a\right] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t =s, A_t=a \right].$$
تابع $q_\pi$ را 
\textit{تابع ارزش عمل}
\LTRfootnote{Action Value Function} مربوط به خط‌مشی 
$\pi$
می‌نامیم.
\پایان{تعریف}
\section{برنامه‌ریزی پویا}
در بخش قبل، با مفاهیم کلیدی یادگیری تقویتی آشنا شدیم. در این بخش، به یکی از مهم‌ترین روش‌های کلاسیک در حل مسئله یادگیری تقویتی، یعنی 
\textit{برنامه ریزی پویا}
\LTRfootnote{ (DP)Dynamic Programming}خواهیم پرداخت. برنامه‌ریزی پویا
 مجموعه‌ای از الگوریتم‌هاست  که در صورت وجود مدل کاملی از محیط، در قالب یک 
\lr{MDP}،
 می‌توانند برای محاسبه بهترین خط‌مشی (خط‌مشی‌ای که بیش‌ترین عایدی را به دست می‌دهد) استفاده شوند.
الگوریتم‌های کلاسیک برنامه‌ریزی پویا به دلیل فرض
مدل کاملی از محیط و همچنین هزینه محاسباتی زیادشان، به لحاظ عملی چندان قابل استفاده نیستند اما به لحاظ نظری مهم هستند. قبل از آن که به روش‌های برنامه ریزی پویا بپردازیم، لازم است با مفهوم خط‌مشی بهینه و تابع ارزش بهینه آشنا شویم.
\subsection{خط‌مشی و تابع ارزش بهینه}
حل کردن مسئله یادگیری تقویتی، به معنی پیدا کردن خط‌مشی‌ای است که بیشترین پاداش را در طول زمان موجب می‌شود.	به چنین خط‌مشی‌ای، 
\textit{خط‌مشی بهینه} 
\LTRfootnote{Optimal Policy}
گفته می‌شود. برای
 MDP‌ 
 های متناهی، می‌توانیم خط‌مشی بهینه را به صورت زیر تعریف کنیم:
\شروع{تعریف}
می‌گوییم خط‌مشی $\pi$ \مهم{بهتر یا مساوی} خط‌مشی 
$\pi'$
است،
$\pi \ge \pi'$
هرگاه برای هر 
$s \in \EuScript{S}$
$$v_\pi(s) \ge v_{\pi'}(s)$$
که $v_\pi$ تابع ارزش حالت مربوط به خط‌مشی $\pi$ است.
\پایان{تعریف} می‌توان نشان داد که برای هر MDP متناهی، حداقل یک خط‌مشی وجود دارد که بهتر یا مساوی هر خط‌مشی دیگری باشد
\cite{suttonbook}.
 به چنین خط‌مشی‌ای 
\textit{\مهم{خط‌مشی بهینه}}
\LTRfootnote{Optimal Policy}
 گفته می‌ شود. ممکن است بیش از یک خط‌مشی بهینه وجود داشته باشد. تمام خط‌مشی‌های بهینه را با نماد $\pi_*$  نمایش می‌دهیم. تابع ارزش متناظر با همه خط‌مشی\nf
های بهینه یکسان است و برابر با 
\textit{تابع ارزش بهینه}
\LTRfootnote{Optimal Value Function}
 است که با نماد $v_*$ نمایش داده شده و به شکل زیر تعریف می‌شود:
$$v_*(s) \doteq \max_{\pi} v_\pi(s).$$
همچنین تمام خط‌مشی‌‌های بهینه، تابع ارزشِ عمل مشترکی دارند که آن را با نماد $q_*$ نمایش می‌دهیم و به شکل زیر تعریف می‌کنیم:
$$q_*(s,a) \doteq max_{\pi} q_\pi(s,a).$$
می‌توانیم $q_*$ را برحسب $v_*$ به شکل زیر بنویسیم \cite{suttonbook}
\begin{equation}
q_* (s,a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1})| S_t=s, A_t=a].
%\label{•}
\end{equation}

\subsection{معادله بلمن}
یک ویژگی اساسی توابع ارزش، که در بخش قبل تعریف کردیم، این است که آن‌ها در یک رابطه بازگشتی موسوم به 
\textit{معادله بلمن} 
\LTRfootnote{Bellman Equation}
صدق می‌کنند. معادله بلمن رابطه‌ای بین ارزش یک حالت و ارزش‌ حالت‌های بعدی آن را بیان می‌کند.
فرض کنید $\pi$ یک خط‌مشی دلخواه باشد و
 $s \in \EuScript{S}$.
  با استفاده مستقیم از تعریف
 \ref{statevaluedef}
 داریم
\begin{flalign}
v_\pi (s) \doteq& \mathbb{E}_\pi [G_t | S_t = s] & \nonumber \\
=& \mathbb{E}_\pi [R_{t+1}+ \gamma G_{t+1}|S_t=s] & \nonumber \\
=&\sum_{a} \pi(a|s) \sum_{s'}\sum_{r} p(s',r|s,a)[r+\gamma \mathbb{E}_\pi[G_{t+1}|S_{t+1}=s']] & \nonumber\\
=&\sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a)[r+\gamma v_\pi(s')] %\quad \forall s \in \mathbb(S) \numerhis 
\label{bellman}.
\end{flalign}
به رابطه 
 \ref{bellman}
\مهم{معادله بلمن} گفته می‌شود\cite{suttonbook}. این معادله، رابطه ای بین ارزش یک حالت و ارزش حالت‌های بعدی آن را مشخص می‌کند. مشابه این رابطه را می‌توان برای تابع ارزش عمل نیز تحقیق کرد. فرض کنید 
$s \in \EuScript{S}$
 و 
 $a \in \EuScript{A}$.
 داریم
\begin{flalign}
q_\pi (s,a) \doteq& \mathbb{E}_\pi [G_t | S_t = s, A_t = a] & \nonumber \\
=& \mathbb{E}_\pi [R_{t+1}+ \gamma G_{t+1}|S_t=s, A_t = a] & \nonumber \\
=&\sum_{s',r} p(s',r|s,a) \sum_{a'} \pi(a'|s')[r+\gamma \mathbb{E}_\pi[G_{t+1}|S_{t+1}=s', A_{t+1} = a']] & \nonumber\\
=&\sum_{s',r}  p(s',r|s,a) \sum_{a'} \pi(a'|s')[r+\gamma q_\pi(s',a')] \quad \forall s \in \EuScript{S}. \numerhis 
\label{bellmanaction}
\end{flalign}
رابطه \ref{bellmanaction} به معادله بلمن مربوط به تابع ارزش عمل معروف است.
\subsection{بهینگی و معادله بهینگی بلمن}
%Because v∗ is the value function for a policy, it must satisfy the self-consistency condition given by
%the Bellman equation for state values (3.14). Because it is the optimal value function, however, v∗’s
%consistency condition can be written in a special form without reference to any specific policy. This is
%the Bellman equation for v∗, or the Bellman optimality equation. Intuitively, the Bellman optimality
%equation expresses the fact that the value of a state under an optimal policy must equal the expected
%return for the best action from that state

از آنجا که $v_*$ تابع ارزش یک خط‌مشی است، بنابراین باید در شرایط معادله \ref{bellman} صدق کند.
در این حالت خاص، معادله بلمن را می‌توان به فرم ویژه ای نوشت که با نام
 \textit{معادله بهینگی بلمن}
 \LTRfootnote{Bellman Optimality Equation}
شناخته می‌شود.
\begin{flalign}
  v_{*}(s)= &\max_{a \in \EuScript{A}(s)} q_{\pi_*}(s,a) & \nonumber \\
      =& \max_{a} \mathbb{E}_{\pi_*} [G_t|S_t=s, A_t=a] & \nonumber \\ 
      =& \max_{a} \mathbb{E}_{\pi_*} [R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a] & \nonumber \\ 
      =& \max_{a} \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t= s, A_t = a] & \nonumber \\
      =& \max_{a} \sum_{s',r} p(s',r|s,a) [r + \gamma v_*(s')]. \numberthis
\label{eq:bellman-opti}
\end{flalign}
معادله 
\ref{eq:bellman-opti}
 بیانگر این واقعیت است که ارزش یک حالت تحت یک خط‌مشی بهینه، برابر با امیدریاضی عایدی، برای بهترین عمل از آن حالت است\cite{suttonbook}. به طور مشابه برای تابع ارزش عمل بهینه، معادله بهینگی بلمن به شکل زیر خواهد بود 
\begin{flalign}
q_*(s,a) =& \mathbb{E}\left[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1},a')| S_t=s, A_t=a \right] & \nonumber\\
=& \sum_{s',r} p(s',r|s,a) \left[r + \gamma \max_{a'} q_*(s',a')\right] 
\label{eq:2}.
\end{flalign}

\subsection{مثال: معادله بهینگی بلمن برای ربات قوطی یاب }
با استفاده از 
\ref{eq:bellman-opti}
می‌توانیم معادله بهینگی بلمن را برای مثال ربات قوطی‌یاب، ارائه دهیم. همانطور که دیدیم، فرآیند تصمیم گیری مارکوف ارائه شده برای این مثال، شامل حالت‌های 
$\EuScript{S} = \{high, low\}$
و اعمال 
$\EuScript{A} = \{search, wait, recharge\}$
است.
برای سادگی، حالت‌های $high$ و $low$ را به ترتیب با $h$ و $l$ و اعمال $search$،
$wait$
و
$recharge$
را به ترتیب با $s$ و $w$ و $re$ نشان می‌دهیم. از آنجایی که تنها دو حالت وجود دارد، معادله بهینگی بلمن شامل دو معادله است. ابتدا معادله مربوط به حالت $h$ را بررسی می‌کنیم. داریم
\begin{align}
v_*(h) =& \max \left\{ 
\begin{array}{lr}
   p(h|h,s) \left[ r(h,s,h)+ \gamma v_*(h) \right] + p(l|h,s) \left[ r(h,s,l)+\gamma v_*(l) \right], \\ 
   p(h|h,w)[r(h,w,h)+ \gamma v_*(h)]+p(l|h,w)[r(h,w,l)+\gamma v_*(l)]
\end{array}
\right\} \nonumber \\
 =& \max \left\{ 
\begin{array}{lr}
   \alpha \left[ r_s+ \gamma v_*(h) \right] + (1 - \alpha) \left[ r_s+\gamma v_*(l) \right], \\ 
   1 \left[ r_w + \gamma v_*(h)]+ 0[r_w + \gamma v_*(l)]
\end{array}
\right\} \\
 =& \max \left\{ 
\begin{array}{lr}
   r_s + \gamma \left[  \alpha \gamma v_*(h) + (1 - \alpha) \gamma v_*(l) \right], \\ 
   r_w + \gamma v_* (h)
\end{array}
\right\}.
\label{eq:recycle_h}
\end{align}
به طریق مشابه برای 
$v_*(l)$
معادله زیر به‌دست می‌آید
\begin{align}
	v_*(l) = \max \left \{ 
	\begin{array}{lr}
		\beta r_s - 3(1-\beta) + \gamma \left[ (1-\beta) v_*(h)+\beta v_*(l)\right] \\
		r_w + \gamma v_*(l), \\
		\gamma v_*(h)
	\end{array}
	\right \}.
	\label{eq:recycle_l}
\end{align}
برای هر انتخاب از 
$r_s$،
$r_w$،
$\alpha$،
$\beta$
و $\lambda$ که 
$0 \le \lambda < 1$
و
$0 \le \alpha,\beta \le 1$
دقیقا یک جفت از اعداد حقیقی 
$v_*(h)$
و 
$v_*(l)$
وجود دارد که در معالات
\ref{eq:recycle_h}
و
\ref{eq:recycle_l}
 صدق می‌کنند\cite{suttonbook}. در بخش بعدی، این موضوع را در حالت کلی بیان خواهیم‌کرد.
\subsection{برنامه‌ریزی پویا}
ایده اصلی DP و بیشتر الگوریتم یادگیری تقویتی‌، استفاده از تابع ارزش حالت یا عمل برای سازماندهی یک الگوریتم جستجو برای پیدا کردن خط‌مشی بهینه
$v_*$
یا
$q_*$
است
که در معادلات بهینگی بلمن صدق می‌کند.
\begin{equation}
v_{*}(s) = \max_{a} \sum_{s',r} p(s',r | s,a)[r + \gamma v_*(s')],
\label{bellman_opt_state}
\end{equation}
\begin{equation}
q_{*}(s,a) = \sum_{s',r} p(s',r | s,a)[r + \gamma \max_{a'} q_* (s',a')].
\label{bellman_opt_action}
\end{equation}
%\section{ارزیابی خط‌مشی}
اگر دینامیک محیط کاملاً معلوم باشد معادلات
\ref{bellman_opt_state}
و
\ref{bellman_opt_action}
به ترتیب یک دستگاه معادلات خطی با اندازه
 $|\EuScript{S}|$
 و
 $|\EuScript{S} \times \EuScript{A}|$
  هستند \cite{suttonbook}.
در ساده ترین حالت، باحل این دستگاه معادلات  خطی می‌توان  مقادیر 
$v_*(s)$
و
$q_*(s,a)$
را به ازای هر حالت $s$ و عمل $a$ به دست آورد. اما اگر تعداد حالت‌ها زیاد باشد، ممکن است حل چنین دستگاهی به لحاظ محاسباتی عملی نباشد. برای اهداف ما، روش‌های تکراری مناسب‌ترین روش‌ها هستند. یک دنباله $ v_0 , v_1 , v_2 , ...$ 
از توابع ارزش تقریبی
را در نظر بگیرید که هرکدام نگاشتی از
$\EuScript{S}$ 
به 
$\IR$
 هستند.
تقریب اولیه $v_0$
در همه حالت‌ها
به طور دلخواه انتخاب می‌شود (به جز در حالت‌های پایانی که باید صفر باشد). هر تقریب به عنوان یک قانون به‌روزرسانی به وسیله معادله بلمن از روی تقریب قبلی به‌دست می‌آید. برای هر 
$s \in \EuScript{S}$
%If the environment’s dynamics are completely known, then (4.4) is a system of |S| simultaneous linear
%equations in |S| unknowns (the vπ(s), s ∈ S). In principle, its solution is a straightforward, if tedious,
%computation. For our purposes, iterative solution methods are most suitable. Consider a sequence
%of approximate value functions v0, v1, v2, . . ., each mapping S
%+ to R (the real numbers). The initial
%approximation, v0, is chosen arbitrarily (except that the terminal state, if any, must be given value 0),
%and each successive approximation is obtained by using the Bellman equation for vπ (4.4) as an update
%rule:
\begin{flalign}
v_{k+1}(s) \doteq & \mathbb{E}_{\pi} [R_{t+1} + \gamma v_k(S_{t+1}) | S_t=s] & \nonumber \\
=& \sum_{a} \pi(a|s) \sum_{s',r} p(s',r | s,a)[r + \gamma v_k(s')].
\end{flalign}
بر اساس معادله بلمن می‌توان درستی این تساوی را برای 
$v_{\pi}$
نوشت. روشن است که 
 $v_k = v_{\pi}$
 نقطه ثابتی برای این قانون به‌روز‌رسانی است.
 در واقع، می‌توان نشان داد که در حالت کلی دنباله 
 $\{ v_k \}$
وقتی که 
$ k \rightarrow \infty $
و تحت همان شرایط که وجود 
$v_{\pi}$
را تضمین می‌کند، به مقدار 
$v_{\pi}$
همگرا می‌شود \cite{suttonbook}. با دانستن این نکته می توان الگوریتمی تکراری برای  تقریب زدن تابع ارزش حالت، تحت خط مشی $\pi$، طراحی کرد. این الگوریتم را 
\textit{ارزیابی خط‌مشی گام به گام}
\LTRfootnote{Iterative policy evaluation} یا به طور خلاصه 
ارزیابی خط‌مشی
\LTRfootnote{Policy Evaluation}
 می‌نامند.


\شروع{الگوریتم}
{الگوریتم ارزیابی خط مشی}
\ورودی{خط مشی $\pi$}
\به‌ازای{تا رسیدن به همگرایی تکرار کن}
\به‌ازای{برای هر $s \in \EuScript{S}$}
\دستور{
	$V(s) \longleftarrow \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a) \left[r + \gamma V(s')\right]$
}
\پایان‌به‌ازای
\پایان‌به‌ازای
\خروجی{$V \approx v_\pi$}
\پایان{الگوریتم}
روشن است که همگرایی تنها در حد اتفاق می‌افتد، اما می توان تقریبی به اندازه کافی نزدیک از تابع ارزش به دست آورد، کافیست عملیات به‌روزرسانی را تا جایی ادامه دهیم که تغییرات تابع ارزش از یک
$\epsilon > 0$
کوچکتر شود. در این صورت می توانیم الگوریتم را متوقف کرده و آخرین تابع ارزش دنباله را به عنوان تقریبی از تابع ارزش واقعی، برگردانیم \cite{suttonbook}.
% 
%for all s ∈ S. Clearly, vk = vπ is a fixed point for this update rule because the Bellman equation for vπ
%assures us of equality in this case. Indeed, the sequence {vk} can be shown in general to converge to
%vπ as k → ∞ under the same conditions that guarantee the existence of vπ. This algorithm is called
%iterative policy evaluation.
%
\subsection{بهبود خط‌مشی}
هدف ما از محاسبه تابع ارزش یک خط‌مشی کمک به یافتن خط‌مشی‌های بهتر است.
فرض کنید تابع ارزش 
$v_{\pi}$
 را برای یک خط‌مشی معین \LTRfootnote{Deterministic} دلخواه 
 $\pi$
  محاسبه کرده ایم. برای یک حالت 
  $s$
 می‌خواهیم بدانیم که آیا باید خط‌مشی را به انتخاب قطعی یک عمل 
 $a \neq \pi(s)$
  تغییر دهیم یا خیر.
ما می‌دانیم که با پیروی از خط‌مشی فعلی از حالت
 $s$
  چقدر عایدی خواهیم داشت، اما آیا تغییر به یک خط‌مشی جدید بهتر است یا بدتر؟
  یکی از راه‌های پاسخ به این سوال در نظر گرفتن انتخاب عمل  
  $a$
در حالت 
$s$
و پس از آن پیروی از خط‌مشی
$\pi$
 است. 
 ارزش این شیوه عملکرد برابر است با
 \begin{align}
q_\pi(s,a) = \sum_{s',r} p(s',r|s,a)\left[r + \gamma v_\pi(s')\right]	
 	\label{eq:policybetter}
 \end{align}
 اگر عبارت 
 \ref{eq:policybetter}
 از $v_\pi(s)$ بزرگتر باشد، بدین معناست که انتخاب قطعی عمل $a$ در حالت $s$ و سپس دنبال کردن خط‌مشی $\pi$ بهتر از این است که همواره خط‌مشی $\pi$ را دنبال کنیم \cite{suttonbook}. این گزاره را می‌توان به فرم کلی‌تر در قالب یک قضیه بیان کرد.

%Our reason for computing the value function for a policy is to help find better policies. Suppose we
%have determined the value function vπ for an arbitrary deterministic policy π. For some state s we
%would like to know whether or not we should change the policy to deterministically choose an action
%a 6= π(s). We know how good it is to follow the current policy from s—that is vπ(s)—but would it be better or worse to change to the new policy? One way to answer this question is to consider selecting a in s and thereafter following the existing policy, π. The value of this way of behaving is
\شروع{قضیه}[قضیه‌ی بهبود خط‌مشی]
فرض کنید $\pi$ و 
$\pi'$
دو خط‌مشی قطعی باشند که برای هر 
$s \in \EuScript{S}$
\begin{align}
q_\pi(s, \pi'(s)) \ge \v_{\pi}(s), \numberthis
\label{action-state}
\end{align}
در این صورت 
\begin{align}
v_{\pi'}(s) \ge v_\pi(s).
\label{state-state}
\end{align}
همچنین اگر نامساوی
\ref{action-state}
برای یکی از حالت های $s \in \EuScript{S}$، به صورت اکید برقرار باشد آنگاه
\ref{state-state}
نیز به صورت اکید برقرار است.
\label{policyimptheorem}
\پایان{قضیه}
درستی قضیه بالا را می‌توان با استفاده از تعاریف، به روشنی بررسی کرد \cite{suttonbook}
\begin{flalign*}
v_\pi (s) \le & \ q_\pi (s,\pi'(s)) & \\
 =& \  \mathbb{E}\left[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t = \pi'(a)\right] & \\
= & \  \mathbb{E}_{\pi'}\left[R_{t+1}+\gamma v_\pi(s_{t+1}) | S_t=s\right] & \\
 \le & \  \mathbb{E}_{\pi'}[R_{t+1}+ \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t=s] & \\
= & \ \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2})] | S_t=s]& \\
\le & \ \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_\pi(s_{t+3}) | S_t=s] & \\
\vdots &\\
\le & \  \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots | S_t=s] &\\
\le & \  v_{\pi'}(s).
\end{flalign*}
قضیه \ref{policyimptheorem} یک شرط کافی برای اینکه خط‌مشی $\pi'$ بهتر یا مساوی خط‌مشی $\pi$ باشد را بیان می‌کند. این قضیه می‌تواند ما را به روشی برای بهبود یک خط‌مشی، سوق دهد. منظور ما از بهبود خط‌مشی، روشی برای دستیابی به یک خط‌مشی بهتر، از روی یک خط‌مشی ضعیف‌تر است.
فرض کنید $\pi'$ یک خط‌مشی قطعی باشد که نسبت به تابع ارزش عمل 
$q_\pi$
حریصانه عمل می کند. به عبارت دیگر;
\begin{flalign*}
\pi'(s) \doteq& \ arg\max_{a} q_\pi(s,a) & \\ 
= & \ arg\max_{a} \mathbb{E}\left[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a\right]& \\
= & \ arg \max_{a} \sum_{s',r} p(s',r|s,a) \left[r + \gamma v_\pi(s')\right]. 
\end{flalign*}
با استفاده از قضیه \ref{policyimptheorem} می‌توان نشان داد که برای هر
$s \in \EuScript{S}$
$$v_{\pi'}(s) \ge v_{\pi}(s).$$
بنابراین با حریصانه عمل کردن نسبت به تابع ارزش خط‌مشی $\pi$ می‌توان به یک خط‌مشی بهتر رسید \cite{suttonbook}.
\شروع{الگوریتم}
{الگوریتم بهبود خط‌مشی \cite{suttonbook}}
\ورودی{تقریبی از تابع ارزش حالت مربوط به خط‌مشی $\pi$،
	$V \approx v_\pi$
}
\به‌ازای{برای هر $s \in \EuScript{S}$}
\دستور{
	$\pi'(s) \longleftarrow arg \max_{a} \sum_{s',r} p(s',r|s,a) \left[r + \gamma V(s')\right]$
}
\پایان‌به‌ازای
\خروجی{$\pi'$}
\پایان{الگوریتم}
\subsection{الگوریتم تکرار خط‌مشی}
تا اینجا روشی برای محاسبه تقریبی تابع ارزش حالت و همینطور روشی دستیابی به یک خط‌مشی بهتر با استفاده از تابع ارزش، ارائه دادیم. اما هدف ما دستیابی به خط‌مشی بهینه است. 
برای این کار، با استفاده از ارزیابی خط‌مشی، روی یک خط‌مشی دلخواه $\pi_0$، به تخمینی از 
$v_{\pi_0}$
 برسیم؛ سپس با استفاده از الگوریتم بهبود خط‌مشی روی 
$v_{\pi_0}$،
خط‌مشی جدید 
$\pi_1$
را می سازیم. با تکرار این عملیات روی خط‌مشی 
$\pi_1$
به خط‌مشی 
$\pi_2$
خواهیم رسید. با ادامه دادن این روند، می‌توانیم دنباله‌ای از خط‌مشی‌ها و تابع ارزش‌ها به دست آوریم که به صورت یکنوا در حال بهبود هستند. می‌توان نشان داد که این دنباله از خط‌مشی‌ها و توابع ارزش، به خط‌مشی و تابع ارزش بهینه همگرا می‌شود
%Once a policy, π, has been improved using vπ to yield a better policy, π0, we can then compute vπ0 and
%improve it again to yield an even better π00. We can thus obtain a sequence of monotonically improving
%policies and value functions:
$$\pi_0 \longrightarrow v_{\pi_0} \longrightarrow \pi_1 \longrightarrow v_{\pi_1} \longrightarrow \pi_2 \longrightarrow \dots \longrightarrow \pi_* \longrightarrow v_*$$
به این الگوریتم، الگوریتم \textit{تکرار خط‌مشی}  \LTRfootnote{Policy iteration}
گفته می‌شود.
\شروع{الگوریتم}
{الگوریتم تکرار خط‌مشی}
\دستور{تابع ارزش 
	$V(s)$
	 و خط‌مشی 
	 $\pi$
	 را به دلخواه مقداردهی اولیه کن.
}

\به‌ازای{تا رسیدن به خط‌مشی و تابع ارزش بهینه تکرار کن}

(ارزیابی خط‌مشی)
\به‌ازای{تا رسیدن به همگرایی تکرار کن}
\به‌ازای{برای هر $s \in \EuScript{S}$}
\دستور{
	$V(s) \longleftarrow \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a) \left[r + \gamma V(s')\right]$
}

\پایان‌به‌ازای
\پایان‌به‌ازای
(بهبود خط‌مشی)
\به‌ازای{برای هر $s \in \EuScript{S}$}
\دستور{
	$\pi(s) \longleftarrow arg \max_{a} \sum_{s',r} p(s',r|s,a) \left[r + \gamma V(s')\right]$
}
\پایان‌به‌ازای
\پایان‌به‌ازای
\خروجی{
	$V_*, \pi_*$
}
\پایان{الگوریتم}
\subsection{الگوریتم تکرار ارزش}
یک اشکال در روش تکرار خط‌مشی این است که هر یک از تکرارهای آن، شامل ارزیابی خط‌مشی است که
به خودی خود یک محاسبه زمان‌بر است و نیاز به رفت و برگشت‌های متعدد روی مجموعه حالت دارد. اگر
ارزیابی خط‌مشی به صورت گام به گام تکراری انجام شود، همگرایی فقط در حد اتفاق می‌افتد. بنابراین یا باید منتظر همگرایی دقیق بمانیم و یا می‌توانیم قبل تر از آن متوقف شویم. 
در واقع، عملیات ارزیابی خط‌مشی را می‌توان پس از یک بار رفت و برگشت روی فضای حالت (یک به‌روزرسانی در هر حالت)،  بدون از دست دادن تضمین همگرایی به خط‌مشی بهینه، متوقف کرد. 
این الگوریتم،
\textit{تکرار ارزش}
\LTRfootnote{Value Iteration}
 نامیده می‌شود. الگوریتم تکرار ارزش نسبت به تکرار خط‌مشی، گام های بیشتری تا رسیدن به همگرایی می پیماید اما به دلیل آنکه در هر گام تنها یک بار رفت و برگشت روی فضای حالت های اتفاق می افتد، در عمل سریع تر از تکرار خط‌مشی به همگرایی می‌رسد. این الگوریتم
را می‌توان به عنوان یک عملیات به‌روزرسانی ساده نوشت که ترکیبی از بهبود خط‌مشی و ارزیابی خط‌مشی کوتاه شده است.
\شروع{الگوریتم}
{الگوریتم تکرار ارزش}
\دستور{تابع ارزش 
	$V(s)$
	را به دلخواه مقداردهی اولیه کن.
}

\به‌ازای{تا رسیدن به تابع ارزش بهینه تکرار کن}

\به‌ازای{برای هر $s \in \EuScript{S}$}
\دستور{
	$V(s) \longleftarrow \max_{a} \sum_{s',r} p(s',r|s,a) \left[r + \gamma V(s')\right]$
}
\پایان‌به‌ازای
\پایان‌به‌ازای
\دستور{خط مشی قطعی $\pi$  را تولید کن به طوری که: 
	$$\pi(s) = arg \max_{a} \sum_{s',r} p(s',r|s,a)\left[r + \gamma V(s')\right]$$
}
\خروجی{
	$V \approx V_*, \pi \approx \pi_*$
}
\پایان{الگوریتم}
\subsection{الگوریتم های تکرار خط‌مشی تعمیم یافته}
%A major drawback to the DP methods that we have discussed so far is that they involve operations
%over the entire state set of the MDP, that is, they require sweeps of the state set. If the state set is very
%large, then even a single sweep can be prohibitively expensive
%
%
%Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms
%of systematic sweeps of the state set. These algorithms update the values of states in any order
%whatsoever, using whatever values of other states happen to be available. The values of some states
%may be updated several times before the values of others are updated once. To converge correctly,
%however, an asynchronous algorithm must continue to update the values of all the states: it can’t ignore
%any state after some point in the computation. Asynchronous DP algorithms allow great flexibility in
%selecting states to update.
%
یک اشکال عمده در روش‌های برنامه‌ریزی پویا که تاکنون بحث شده‌است، این است که آن‌ها شامل عملیات‌هایی روی تمام فضای حالت‌ها هستند.
اگر فضای حالت بسیار بزرگ باشد،
حتی یک بار رفت و برگشت می‌تواند بسیار پرهزینه باشد.
الگوریتم‌های ناهمزمان برنامه‌ریزی پویا، الگوریتم‌های تکرار شونده درجا هستند که مقید به رفت و برگشت سیستماتیک روی تمام مجموعه حالت‌ها نیستند. این الگوریتم‌ها ارزش حالت‌ها را بدون هیچ ترتیب منظمی ‌و با استفاده از هر تخمینی از ارزش حالت‌های دیگر که در دسترس باشد، به‌روز می‌کنند.
 ارزش برخی حالت‌ها ممکن است قبل از اینکه ارزش دیگران یکبار به‌روز شود، چندین بار به‌روز شود. با این حال، برای همگرایی صحیح، یک الگوریتم ناهمزمان باید به‌روزرسانی مقادیر همه حالت‌ها را ادامه دهد و نمی‌تواند حالتی را نادیده بگیرد.
الگوریتم‌های ناهم‌زمان برنامه‌ریزی پویا، امکان انعطاف‌پذیری زیادی را در سیستم فراهم می‌کنند و در بسیاری از مسائل، تنها گزینه قابل اجرا به‌حساب می‌آیند 
\cite{suttonbook}.
\section{روش های یادگیری تفاوت زمانی}
در بخش قبل، با روش‌های برنامه‌ریزی پویا آشنا شدیم. این روش‌ها علی‌رغم اهمیت فراون‌شان،
برخلاف روش های برنامه‌ریزی پویا، روش‌های 
{یادگیری تفاوت زمانی} (TD) می‌توانند مستقیماً از تجربه خام و بدون در اختیار داشتن مدلی از دینامیک محیط، یاد بگیرند. مانند برنامه‌ریزی پویا، روش‌های تفاوت زمانی، ارزش‌ هر حالت (حالت-عمل) را بر اساس تخمین‌ حالت‌‌های دیگر به‌روزرسانی می‌کنند.
\\همان‌طور که دیدیم، ارزش یک حالت، امید ریاضی عایدی، با شروع از آن حالت است
$$v_\pi(s) \doteq \mathbb{E}_\pi\left[G_t| S_t=s\right] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty}. \gamma^k R_{t+k+1}|S_t =s \right]$$
بنابراین، یک روش آشکار برای تخمین ارزش حالت $s$ از طریق تجربه، میانگین گرفتن تمام عایدی‌های تجربه شده در هر بار عبور از $s$ است.
با تجربه بیشتر و مشاهده عایدی‌های بیشتر، میانگین آن‌ها باید به $v_\pi(s)$ همگرا شود.
روش‌های تفاوت زمانی در ساده‌ترین حالت، بعد از عبور از هر حالت، ارزش آن حالت را با استفاده از ارزش حالت بعدی به عنوان تخمینی از عایدی، بلافاصله به‌روزرسانی می‌کنند
$$V(S_t) \longleftarrow V(S_t) + \alpha\left[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right].$$

\شروع{الگوریتم}{الگوریتم TD(0) برای تخمین $v_\pi$ \cite{suttonbook}}
\ورودی{خط‌مشی $\pi$}
\دستور{تابع $V(s)$ را با مقادیر دلخواه مقداردهی اولیه کن.}
\‌به‌ازای{برای هر گام در اپیزود}
\دستور{عمل $a$ را با دنبال کردن خط‌مشی $\pi$ انتخاب کن}
\دستور{پاداش $r$ و حالت جدید $s'$ را مشاهده کن}
\دستور{
$V(s) \longleftarrow V(s) + \alpha\left[r + \gamma V(s') - V(s)\right]$
}
\دستور{
	$s \longleftarrow s'$
}
\خروجی{
	$V \approx v_\pi$
}
\پایان{الگوریتم}
\subsection{روش های Q-learning}
ایده اصلی در روش 
Q-learning،
 تخمین تابع ارزش عمل  
$q^*(s,a)$ 
با استفاده از معادله بلمن به عنوان یک به‌روزرسانی تکراری به شکل زیر است
\cite{mnih2013playing}
$$q_{i+1}(s,a) = \mathbb{E}[r+ \gamma \max_{a'} q_i(s',a')|s,a].$$
الگوریتم 
\lr{Q-learning}،
یک الگوریتم تکرار ارزش است که به تابع ارزشِ عمل بهینه همگرا می‌شود، 
$q_i \longrightarrow q^*$
وقتی
$i \longrightarrow \infty$.
این رویکرد کلی، در مسائل بزرگ غیر عملی است؛
زیرا تابع ارزشِ عمل برای هر دنباله از تجربیات،  به طور جداگانه و بدون تعمیم روی سایر موقعیت‌ها برآورد می‌شود. نتیجتا فرآیند یادگیری بسیار طولانی خواهد شد. در عوض، معمولاً از یک تقریب‌گر توابع (مثل شبکه عصبی) برای تخمین تابع ارزشِ عمل استفاده می‌شود.  یکی از فواید این کار، در این است که با تجربه یک حالت-عمل، نه تنها درمورد ارزش حالت-عمل تجربه شده، اطلاعات کسب می‌کنیم، بلکه می‌توانیم این اطلاعات را به حالت-عمل‌های مشابه نیز تعمیم دهیم.
الگوریتم‌هایی که از شبکه‌های عصبی، به عنوان تقریب‌گر تابع، برای یادگیری تابع ارزش و یا خط‌مشی استفاده می کنند، با عنوان الگوریتم‌های 
\textit{\مهم{یادگیری تقویتی عمیق}}
\LTRfootnote{Deep Reinforcement Learning}
شناخته می‌شوند. در فصل سوم با این الگوریتم‌ها بیشتر آشنا خواهیم شد.