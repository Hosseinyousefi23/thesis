
\ÙØµÙ„{Ù†ØªØ§ÛŒØ¬ Ø§Ø®ÛŒØ±}
Ø¯Ø± Ø§ÛŒÙ† ÙØµÙ„ Ø¨Ù‡ Ù…Ø¹Ø±ÙÛŒ Ø¨Ø±Ø®ÛŒ Ø§Ø² Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ù…Ø¯Ø±Ù† Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ Ù…ÛŒ Ù¾Ø±Ø¯Ø§Ø²ÛŒÙ…. Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ù…Ø¹Ø±ÙÛŒ Ø´Ø¯Ù‡ Ø¯Ø± Ø§ÛŒÙ† ÙØµÙ„ Ø¨Ù‡ Ø¯Ùˆ Ø¯Ø³ØªÙ‡ Ú©Ù„ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø¯Ù„ Ùˆ Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒ Ø´ÙˆÙ†Ø¯. ØªÙØ§ÙˆØª Ø§ØµÙ„ÛŒ Ø§ÛŒÙ† Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø¢ÛŒØ§ Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² ØªØ§Ø¨Ø¹ Ø§Ù†ØªÙ‚Ø§Ù„ MDP ÛŒØ§ ØªØ®Ù…ÛŒÙ†ÛŒ Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ø´ÙˆØ¯ ÛŒØ§ Ø®ÛŒØ±. Ø¯Ø± Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„ØŒ Ø¹Ø§Ù…Ù„ Ù‡ÛŒÚ† Ø§Ø·Ù„Ø§Ø¹ÛŒ Ø§Ø² Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ù…Ø­ÛŒØ· Ù†Ø¯Ø§Ø±Ø¯ Ùˆ ØªÙ†Ù‡Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ ØªØ¬Ø±Ø¨Ù‡ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø¨ÛŒØ§Ù…ÙˆØ²Ø¯. Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø¯Ù„ Ø¯Ø³ØªØ±Ø³ÛŒ Ú©Ø§Ù…Ù„ ÛŒØ§ ØªÙ‚Ø±ÛŒØ¨ÛŒ Ø¨Ù‡ ØªØ§Ø¨Ø¹ Ø§Ù†ØªÙ‚Ø§Ù„ Ø±Ø§ Ù…ÙØ±ÙˆØ¶ Ù…ÛŒ Ú¯ÛŒØ±Ù†Ø¯. Ø§Ù„Ø¨ØªÙ‡ Ø¨Ø±Ø®ÛŒ Ø§Ø² Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø¯Ù„ Ù…ÛŒ ØªÙˆØ§Ù†Ù†Ø¯ Ø§Ø² Ø·Ø±ÛŒÙ‚ ØªØ¬Ø±Ø¨Ù‡ ØªØ®Ù…ÛŒÙ†ÛŒ Ø§Ø² Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ù…Ø­ÛŒØ· Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ù†Ø¯.


\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´\nf Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø¯Ù„ Ùˆ Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„}


ÛŒÚ©ÛŒ Ø§Ø² Ù…Ù‡Ù…ØªØ±ÛŒÙ† Ù†Ù‚Ø§Ø· Ø§Ù†Ø´Ø¹Ø§Ø¨ Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…\nf Ù‡Ø§ÛŒ RL Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø¢ÛŒØ§ Ø¹Ø§Ù…Ù„ Ø¨Ù‡ ÛŒÚ© Ù…Ø¯Ù„ Ø§Ø² Ù…Ø­ÛŒØ· Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ø±Ø¯ ÛŒØ§  ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ø¢Ù…ÙˆØ®ØªÙ† Ù…Ø¯Ù„ÛŒ Ø§Ø² Ù…Ø­ÛŒØ· Ø±Ø§ Ø¯Ø§Ø±Ø¯ØŸ Ù…Ù†Ø¸ÙˆØ± Ø§Ø² Ù…Ø¯Ù„ Ù…Ø­ÛŒØ· ØŒ ØªØ§Ø¨Ø¹ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø§Ù†ØªÙ‚Ø§Ù„ Ùˆ Ù¾Ø§Ø¯Ø§Ø´ Ù‡Ø± Ø­Ø§Ù„Øª-Ø¹Ù…Ù„ Ø±Ø§ Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ù…ÛŒ Ú©Ù†Ø¯.

Ù†Ú©ØªÙ‡ Ù…Ø«Ø¨Øª Ø§ØµÙ„ÛŒ Ø¯Ø± Ø¯Ø§Ø´ØªÙ† Ù…Ø¯Ù„ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ø¹Ø§Ù…Ù„ Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒ Ø¯Ù‡Ø¯ Ø¨Ø§ ØªÙÚ©Ø± Ø§Ø² Ù‚Ø¨Ù„ ØŒ Ø¨Ø¨ÛŒÙ†Ø¯ Ú†Ù‡ Ø§ØªÙØ§Ù‚ÛŒ Ø¨Ø±Ø§ÛŒ Ø·ÛŒÙ ÙˆØ³ÛŒØ¹ÛŒ Ø§Ø² Ú¯Ø²ÛŒÙ†Ù‡ Ù‡Ø§ÛŒ Ù…Ù…Ú©Ù† Ø±Ø® Ù…ÛŒ Ø¯Ù‡Ø¯ Ùˆ Ø¨Ù‡ ØµØ±Ø§Ø­Øª Ø¯Ø± Ù…ÙˆØ±Ø¯ Ú¯Ø²ÛŒÙ†Ù‡ Ù‡Ø§ÛŒ Ø®ÙˆØ¯ ØªØµÙ…ÛŒÙ… Ø¨Ú¯ÛŒØ±Ø¯. Ø³Ù¾Ø³ Ø¹Ø§Ù…Ù„ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ù†ØªØ§ÛŒØ¬ Ø­Ø§ØµÙ„ Ø§Ø² Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø±ÛŒØ²ÛŒ Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø¯Ø± Ù‚Ø§Ù„Ø¨ ÛŒÚ© Ø®Ø· Ù…Ø´ÛŒ Ø¨ÛŒØ§Ù…ÙˆØ²Ø¯ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ù…Ø´Ù‡ÙˆØ± Ø§Ø² Ø§ÛŒÙ† Ø±ÙˆØ´ AlphaZero Ø§Ø³Øª. Ø¯Ø± Ø¹Ù…Ù„ØŒ Ø§Ú¯Ø± Ø¯Ø³ØªÛŒØ§Ø¨ÛŒ Ø¨Ù‡ Ù…Ø¯Ù„ÛŒ Ø§Ø² Ù…Ø­ÛŒØ· Ø§Ù…Ú©Ø§Ù† Ù¾Ø°ÛŒØ± Ùˆ Ø¹Ù…Ù„ÛŒ Ø¨Ø§Ø´Ø¯ØŒ Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø§Ø² Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ø´ÙˆØ¯. Ø²ÛŒØ±Ø§ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø¨Ø§Ø¹Ø« Ø¨Ù‡Ø¨ÙˆØ¯ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ Ú©Ø§Ø±Ø§ÛŒÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„ Ø´ÙˆÙ†Ø¯.

Ø§ØµÙ„ÛŒ ØªØ±ÛŒÙ† Ù†Ù‚Ø·Ù‡ Ø¶Ø¹Ù Ø§ÛŒÙ† Ø±ÙˆØ´ Ù‡Ø§ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù…Ø¹Ù…ÙˆÙ„Ø§ ÛŒÚ© Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„ Ø§Ø² Ù…Ø­ÛŒØ·  Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¹Ø§Ù…Ù„ Ù†ÛŒØ³Øª Ùˆ Ø¹Ø§Ù…Ù„ØŒ Ø¨Ø§ÛŒØ¯ Ù…Ø¯Ù„ Ø±Ø§ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø§Ø² Ø·Ø±ÛŒÙ‚ ØªØ¬Ø±Ø¨Ù‡ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯.

Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø² ÛŒÚ© Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ú©Ù†Ù†Ø¯ ØŒ Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø¯Ù„ Ùˆ Ø¢Ù†Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø² Ú†Ù†ÛŒÙ† Ù…Ø¯Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒ Ú©Ù†Ù†Ø¯ØŒ Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„ Ù†Ø§Ù…ÛŒØ¯Ù‡ Ù…ÛŒ Ø´ÙˆÙ†Ø¯ ØŒ Ù†Ø§Ù…ÛŒØ¯Ù‡ Ù…ÛŒ Ø´ÙˆÙ†Ø¯. Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„ Ø§Ø² Ø¯Ø³ØªØ§ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø§Ù„Ù‚ÙˆÙ‡ Ø¯Ø± Ø¨Ù‡Ø±Ù‡ ÙˆØ±ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ú†Ø´Ù… Ù¾ÙˆØ´ÛŒ Ù…ÛŒ Ú©Ù†Ù†Ø¯ ØŒ Ø§Ù…Ø§ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ùˆ ØªÙ†Ø¸ÛŒÙ… Ø¢Ù†Ù‡Ø§ Ø¢Ø³Ø§Ù† ØªØ± Ø§Ø³Øª. Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø®Ø§Ø·Ø±ØŒ Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„ Ø§Ø² Ù…Ø­Ø¨ÙˆØ¨ÛŒØª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø¨ÙˆØ¯Ù‡ Ùˆ Ø¨Ù‡ Ø·ÙˆØ± Ú¯Ø³ØªØ±Ø¯Ù‡ ØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø¯Ù„ ØªÙˆØ³Ø¹Ù‡ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´ Ø´Ø¯Ù‡ Ø§Ù†Ø¯.

\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„}

\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø§Ø±Ø²Ø´}


Ø¯Ø± Ø±ÙˆØ´Ù‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø§Ø±Ø²Ø´
ØªØ§Ø¨Ø¹ Ø§Ø±Ø²Ø´ Ø¹Ù…Ù„ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø²  ÛŒÚ© function approximatorØŒ Ù…Ø§Ù†Ù†Ø¯ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ ØŒ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ù…ÛŒ Ø´ÙˆØ¯. ÙØ±Ø¶ Ú©Ù†ÛŒØ¯
$Q(s,a;\theta)$
ÛŒÚ© ØªØ§Ø¨Ø¹ Ø§Ø±Ø²Ø´ Ø¹Ù…Ù„ ØªÙ‚Ø±ÛŒØ¨ÛŒ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ± 
$\theta$
Ø¨Ø§Ø´Ø¯
Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ $\theta$ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Q-learning ÛŒÚ©ÛŒ Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡ Ù‡Ø§ÛŒ Ú†Ù†ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…ÛŒ Ø§Ø³Øª
Ú©Ù‡ Ù‡Ø¯Ù Ø¢Ù† ØªÙ‚Ø±ÛŒØ¨ Ù…Ø³ØªÙ‚ÛŒÙ… ØªØ§Ø¨Ø¹ Ø¹Ù…Ù„-Ø§Ø±Ø²Ø´ Ø¨Ù‡ÛŒÙ†Ù‡ 
$Q^*(s,a) \approx Q(s,a: \theta)$
 Ø§Ø³Øª

Ø¯Ø± Q-learning ÛŒÚ© Ù…Ø±Ø­Ù„Ù‡ Ø§ÛŒØŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ $\theta$ Ø§Ø² ØªØ§Ø¨Ø¹ Ø¹Ù…Ù„-Ø§Ø±Ø²Ø´ Ø¨Ø§ Ø¨Ù‡ Ø­Ø¯Ø§Ù‚Ù„ Ø±Ø³Ø§Ù†Ø¯Ù† ØªØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ù‡ Ø´Ú©Ù„ Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ù…Ø±Ø­Ù„Ù‡ Ø¢Ù…ÙˆØ®ØªÙ‡ Ù…ÛŒ Ø´ÙˆÙ†Ø¯ØŒ Ø¨Ù‡ Ø´Ú©Ù„ÛŒ Ú©Ù‡ ØªØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡ i-Ø§Ù… Ø¨Ù‡ Ø´Ú©Ù„ 
$$L_i(\theta_i) = \mathbb{E} {\left( r+\gamma \max_{a'} Q(s',a'; \theta_{i-1})- Q(s,a:\theta_i) \right)}^2$$
 ØªØ¹Ø±ÛŒÙ Ù…ÛŒ Ø´ÙˆØ¯ Ú©Ù‡ 
 $s'$
 Ø­Ø§Ù„ØªÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ø­Ø§Ù„Øª $s$ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒ Ø´ÙˆØ¯.

\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø®Ø· Ù…Ø´ÛŒ}

Ø¯Ø± Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ø±ÙˆØ´Ù‡Ø§ÛŒÛŒ Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒ Ú¯ÛŒØ±ÛŒÙ… Ú©Ù‡ Ø¨Ù‡ Ø¬Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ø¹Ù…Ù„-Ø§Ø±Ø²Ø´ ÛŒØ§ Ø­Ø§Ù„Øª-Ø§Ø±Ø²Ø´ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÛŒØ§Ø¨ÛŒ Ø¨Ù‡ Ø®Ø· Ù…Ø´ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ØŒ ÛŒÚ© Ø®Ø· Ù…Ø´ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±ÛŒØ²Ù‡ \Ù¾Ø§ÙˆØ±Ù‚ÛŒ{Parameterized} Ø´Ø¯Ù‡ Ø±Ø§ Ù…ÛŒ Ø¢Ù…ÙˆØ²Ø¯ Ú©Ù‡ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ø±Ø§ Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÛŒÚ© ØªØ§Ø¨Ø¹ Ø§Ø±Ø²Ø´ØŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†Ø¯. ÛŒÚ© ØªØ§Ø¨Ø¹ Ø§Ø±Ø²Ø´
Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù‡Ù…Ú†Ù†Ø§Ù† Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø®Ø· Ù…Ø´ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯ ØŒ Ø§Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ù‚Ø¯Ø§Ù… Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ù†ÛŒØ³Øª. Ù…Ø§ Ø§Ø² Ù†Ù…Ø§Ø¯
$\theta \in \mathbb{R}^{d'}$
 


Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø¯Ø§Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø®Ø· Ù…Ø´ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ú©Ù†ÛŒÙ….
Ø¨Ø± Ø®Ù„Ø§Ù Ø±ÙˆØ´Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø§Ø±Ø²Ø´ ØŒ Ø±ÙˆØ´Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø®Ø· Ù…Ø´ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ ØªØ§Ø¨Ø¹ Ø®Ø· Ù…Ø´ÛŒ 
$\pi_\theta(a|s)$
 Ø±Ø§ ØªØ®Ù…ÛŒÙ† Ù…ÛŒ Ø²Ù†Ù†Ø¯ Ùˆ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ $\theta$ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² \Ù…Ù‡Ù…{ØµØ¹ÙˆØ¯ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†} Ø±ÙˆÛŒ ÛŒÚ© Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ \Ù¾Ø§ÙˆØ±Ù‚ÛŒ{Performance Measure} 
$J(\pi_\theta)$

ÛŒØ§ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ùˆ ÛŒØ§ Ø¨Ø§ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ ØªØ®Ù…ÛŒÙ† Ù‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ Ø§Ø² 
$J(\pi_\theta)$
Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒ Ú©Ù†Ø¯.  Ø§ÛŒÙ† Ø±ÙˆØ´ ØªÙ‚Ø±ÛŒØ¨Ø§ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª on-policy  Ø¹Ù…Ù„ Ù…ÛŒ Ú©Ù†Ù†Ø¯. 

Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯ÛŒØ¯ØŒ Ù…ÛŒ ØªÙˆØ§Ù† Ø§Ø² ØªÙˆØ§Ø¨Ø¹ Ù…Ø®ØªÙ„ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ $J$ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÙˆØ¯. ÛŒÚ© Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ø¯ÛŒÙ‡ÛŒ 
$J(\pi_\theta) = \mathbb{E}[R_t]$
Ø§Ø³Øª. Ø§ÛŒÙ† Ø±ÙˆØ´ Ù‡Ø§ÛŒ ØªÙ„Ø§Ø´ Ù…ÛŒ Ú©Ù†Ù†Ø¯ ØªØ§Ø¨Ø¹ $J$ Ø±Ø§ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ú©Ù†Ù†Ø¯

$$\theta_{t+1}=\theta_t + \alpha \hat{\nabla J(\theta_t)}$$

Ú©Ù‡ 
$\hat{\nabla J(\theta_t)}$

ØªØ®Ù…ÛŒÙ†ÛŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§ØªÛŒ Ø§Ø³Øª Ú©Ù‡ Ú©Ù‡ Ø§Ù…ÛŒØ¯Ø±ÛŒØ§Ø¶ÛŒ Ø¢Ù† Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ $J$ Ø±Ø§ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø®Ø· Ù…Ø´ÛŒ $\theta_t$ ØªØ®Ù…ÛŒÙ† Ù…ÛŒ Ø²Ù†Ø¯.

Ø¨Ù‡ Ø±ÙˆØ´ Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ú†Ù†ÛŒÙ† Ø§Ù„Ú¯ÙˆÛŒÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø· Ù…Ø´ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ù…ÛŒ Ú©Ù†Ù†Ø¯ØŒ Ø±ÙˆØ´ Ù‡Ø§ÛŒ \Ù…Ù‡Ù…{Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø®Ø· Ù…Ø´ÛŒ} \Ù¾Ø§ÙˆØ±Ù‚ÛŒ{Policy Gradient} Ù…ÛŒ Ú¯ÙˆÛŒÛŒÙ…. Ø¯Ø³ØªÙ‡ Ø§ÛŒ Ø§Ø² Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø®Ø· Ù…Ø´ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ Ú©Ù‡ ØªÙ„Ø§Ø´ Ù…ÛŒ Ú©Ù†Ù†Ø¯ ØªØ®Ù…ÛŒÙ†ÛŒ Ø§Ø² ØªØ§Ø¨Ø¹ Ø§Ø±Ø²Ø´ Ø±Ø§ Ù†ÛŒØ² Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†Ù†Ø¯. Ø¨Ù‡ Ú†Ù†ÛŒÙ† Ø±ÙˆØ´ Ù‡Ø§ÛŒÛŒ Ø±ÙˆØ´ Ù‡Ø§ÛŒØŒ \Ù…Ù‡Ù…{Ø¨Ø§Ø²ÛŒÚ¯Ø±-Ù…Ù†ØªÙ‚Ø¯} (Actor-Critic) Ú¯ÙØªÙ‡ Ù…ÛŒ Ø´ÙˆØ¯ Ú©Ù‡ \Ù…Ù‡Ù…{Ø¨Ø§Ø²ÛŒÚ¯Ø±} (Actor) Ø§Ø´Ø§Ø±Ù‡ Ø¨Ù‡ Ø®Ø· Ù…Ø´ÛŒ Ø¢Ù…ÙˆØ®ØªÙ‡ Ø´Ø¯Ù‡ Ùˆ \Ù…Ù‡Ù…{Ù…Ù†ØªÙ‚Ø¯} (Critic) Ø§Ø´Ø§Ø±Ù‡ Ø¨Ù‡ ØªØ§Ø¨Ø¹ Ø§Ø±Ø²Ø´ Ø¢Ù…ÙˆØ®ØªÙ‡ Ø´Ø¯Ù‡ (Ù…Ø¹Ù…ÙˆÙ„Ø§ ÛŒÚ© ØªØ§Ø¨Ø¹ Ø­Ø§Ù„Øª-Ø§Ø±Ø²Ø´) Ø¯Ø§Ø±Ø¯.

\Ø´Ø±ÙˆØ¹{Ù‚Ø¶ÛŒÙ‡}[Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø®Ø· Ù…Ø´ÛŒ]
$$\nabla J(\pi_\theta) \propto \sum_{s} \mu(s) \sum_{a} q_\pi(s,a) \nabla_\theta \pi_\theta(a|s)$$

Ú©Ù‡ $\mu$ ÛŒÚ© ØªÙˆØ²ÛŒØ¹ Ø§Ø­ØªÙ…Ø§Ù„ Ø±ÙˆÛŒ $S$ Ø§Ø³Øª Ú©Ù‡ Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙØ¹Ø§ØªÛŒ Ø§Ø³Øª Ú©Ù‡ Ø­Ø§Ù„Øª $s$ Ø¨Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ø±Ø¯Ù† Ø®Ø· Ù…Ø´ÛŒ 
$\pi_\theta$
 ØªÚ©Ø±Ø§Ø± Ù…ÛŒ Ø´ÙˆØ¯.
\Ù¾Ø§ÛŒØ§Ù†{Ù‚Ø¶ÛŒÙ‡}


Ù…ÛŒ ØªÙˆØ§Ù† Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯ 
\cite{(suttonbook)}

$$\nabla J(\pi_\theta) = \mathbb{E}_\pi \left[ R_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} \right]$$
Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø¯Ø± Ù‡Ø± Ú¯Ø§Ù…
$\left[ R_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} \right]$
ÛŒÚ© ØªØ®Ù…ÛŒÙ† Ú¯Ø± Ù†Ø§Ø§Ø±ÛŒØ¨ Ø§Ø² $\nabla J(\pi_\theta)$ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯. Ù¾Ø³ Ù…ÛŒ ØªÙˆØ§Ù† Ø¯Ø± Ù‡Ø± Ú¯Ø§Ù… $\theta$ Ø±Ø§ Ø¨Ù‡ Ø´Ú©Ù„ Ø²ÛŒØ± Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ø±Ø¯

$$\theta_{t+1} = \theta_t + \alpha R_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} = \theta_t + \alpha R_t \nabla_\theta log \pi_\theta (a|S_t)$$

Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ Ø®Ø· Ù…Ø´ÛŒ Ø¨Ù‡ Ø´Ø±Ø­ Ø²ÛŒØ± Ø§Ø³Øª. \\
Ø±ÙˆØ´ Ù‡Ø§ÛŒ Actor-Critic Ú©Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Gradient ascent Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§ Ø¨Ø±Ø§ÛŒ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ 
$J(\pi\theta)$
Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒ Ø¨Ø±Ù†Ø¯.

Ø±ÙˆØ´  Proximal Policy Optimization Ú©Ù‡


\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ Ù‡Ø§ÛŒ Actor-Critic}
ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ Ø®Ø· Ù…Ø´ÛŒØŒ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ REINFORCE Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ Ø§Ø³Øª.
\cite{williams1992simple}

Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ REINFORCE Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ $\theta$ Ø±Ø§ Ø¯Ø± Ø¬Ù‡Øª 

$\nabla_\theta \  log  \ \pi (a_t|s_t:\theta)R_t$
Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒ Ú©Ù†Ø¯ Ú©Ù‡ ÛŒÚ© ØªØ®Ù…ÛŒÙ† Ù†Ø§Ø§Ø±ÛŒØ¨ Ø§Ø² 
$\nabla_\theta \mathbb{E}[R_t]$
Ø§Ø³Øª. Ù…ÛŒ ØªÙˆØ§Ù†  Ø¨Ø§ Ú©Ù… Ú©Ø±Ø¯Ù† ÛŒÚ© ØªØ§Ø¨Ø¹ Ø¢Ù…ÙˆØ®ØªÙ‡ Ø´Ø¯Ù‡ Ø±ÙˆÛŒ Ø­Ø§Ù„Øª Ù‡Ø§ 
$b_t(s_t)$
Ø§Ø² 
$R_t$
ØŒ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ø§ÛŒÙ† ØªØ®Ù…ÛŒÙ† Ø±Ø§ Ú©Ø§Ù‡Ø´ Ø¯Ø§Ø¯ Ø¨Ø·ÙˆØ±ÛŒÚ©Ù‡ Ù†Ø§Ø§Ø±ÛŒØ¨ Ø¨Ø§Ù‚ÛŒ Ø¨Ù…Ø§Ù†Ø¯. Ø¨Ù‡ Ú†Ù†ÛŒÙ† ØªØ§Ø¨Ø¹ÛŒ \Ù…Ù‡Ù…{Ù¾Ø§ÛŒÙ‡} Ú¯ÙØªÙ‡ Ù…ÛŒ Ø´ÙˆØ¯. Ù†ØªÛŒØ¬ØªØ§ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø¨Ù‡ Ø´Ú©Ù„
$\nabla_\theta \ log \ \pi(a_t|s_t;\theta) (R_t - b_t(s_t))$
Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯. \\

Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø§Ø² ÛŒÚ© ØªØ®Ù…ÛŒÙ† Ø¢Ù…ÙˆØ®ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ø§Ø±Ø²Ø´ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù¾Ø§ÛŒÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ø´ÙˆØ¯
$b_t(s_t) \approx V^\pi (s_t)$

Ú©Ù‡ Ù…Ù†Ø¬Ø± Ø¨Ù‡ ØªØ®Ù…ÛŒÙ†ÛŒ Ø¨Ø§ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆÚ†Ú© ØªØ± Ø§Ø² Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø®Ø· Ù…Ø´ÛŒ Ù…ÛŒ \nf Ø´ÙˆØ¯ Ø¯Ø± Ø­Ø§Ù„ÛŒÚ©Ù‡ ØªØ®Ù…ÛŒÙ†ØŒ Ù†Ø§Ø§Ø±ÛŒØ¨ Ø¨Ø§Ù‚ÛŒ Ù…ÛŒ Ù…Ø§Ù†Ø¯ Ùˆ Ù†ØªÛŒØ¬ØªØ§ Ø¹Ù…Ù„ÛŒØ§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø§ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒ Ø´ÙˆØ¯.
Ø§ÛŒÙ† Ø±ÙˆØ´ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ø´Ú©Ù„ Ù…Ø¹Ù…Ø§Ø±ÛŒ \Ù…Ù‡Ù…{Ø¨Ø§Ø²ÛŒÚ¯Ø±-Ù…Ù†ØªÙ‚Ø¯} \Ù¾Ø§ÙˆØ±Ù‚ÛŒ{Actor-Critic} ØªØ¹Ø¨ÛŒØ± Ø´ÙˆØ¯ Ú©Ù‡ Ø®Ø· Ù…Ø´ÛŒ $\pi$ \Ù…Ù‡Ù…{Ø¨Ø§Ø²ÛŒÚ¯Ø±} Ùˆ Ù¾Ø§ÛŒÙ‡ $b_t$ \Ù…Ù‡Ù…{Ù…Ù†ØªÙ‚Ø¯} Ø§Ø³Øª. 


 Ø§Ú¯Ø± Ø§Ø² ØªØ®Ù…ÛŒÙ† ÛŒÚ© ØªØ§Ø¨Ø¹ Ø­Ø§Ù„Øª-Ø§Ø±Ø²Ø´ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù¾Ø§ÛŒÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ… 
$b_t(s) = V_{\pi_{\theta_t}}(s)$
Ø¹Ø¨Ø§Ø±Øª 
$R_t - b_t(s)$

Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ø´Ú©Ù„ ØªØ®Ù…ÛŒÙ†ÛŒ Ø§Ø² \Ù…Ù‡Ù…{Ù…Ø²ÛŒØª} \Ù¾Ø§ÙˆØ±Ù‚ÛŒ{Advantage}Ù Ø¹Ù…Ù„ $a_t$ Ø¯Ø± Ø­Ø§Ù„Øª $s_t$ ÛŒØ§ 
$A(a_t,s_t)=Q(a_t,s_t)-V(s_t)$
ØªØ¹Ø¨ÛŒØ± Ø´ÙˆØ¯. Ú†Ø±Ø§Ú©Ù‡ $R_t$ ØªØ®Ù…ÛŒÙ†ÛŒ Ø§Ø² 
$Q^\pi (a_t, s_t)$

Ùˆ $b_t$ ØªØ®Ù…ÛŒÙ†ÛŒ Ø§Ø² 
$V^\pi (s_t)$

Ø§Ø³Øª. Ø¯Ø± Ø§ÛŒÙ† ØµÙˆØ±Øª Ø¨Ù‡ Ø§ÛŒÙ† Ø±ÙˆØ´ $Advantage Actor-Critic$ ÛŒØ§ $A2C$ Ú¯ÙØªÙ‡ Ù…ÛŒ Ø´ÙˆØ¯.

\Ø´Ø±ÙˆØ¹{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…}{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… $Advantage \  Actor-Critic$}
\ÙˆØ±ÙˆØ¯ÛŒ{ÛŒÚ© Ù¾Ø§Ø±Ø§Ù…ØªØ±ÛŒØ²Ù‡ Ø³Ø§Ø²ÛŒ Ù…Ø´ØªÙ‚ Ù¾Ø°ÛŒØ± Ø§Ø² Ø®Ø· Ù…Ø´ÛŒ 
	$\pi_\theta(a|s)$}
\ÙˆØ±ÙˆØ¯ÛŒ{ÛŒÚ© Ù¾Ø§Ø±Ø§Ù…ØªØ±ÛŒØ²Ù‡ Ø³Ø§Ø²ÛŒ Ù…Ø´ØªÙ‚ Ù¾Ø°ÛŒØ± Ø§Ø² ØªØ§Ø¨Ø¹ Ø­Ø§Ù„Øª-Ø§Ø±Ø²Ø´ 
	$v_\omega(s)$}
\Ø¯Ø³ØªÙˆØ±{Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø®Ø· Ù…Ø´ÛŒ 
	$\theta \in \mathbb{R}^{d'}$
	Ùˆ ØªØ§Ø¨Ø¹ Ø­Ø§Ù„Øª-Ø§Ø±Ø²Ø´
	$\omega \in \mathbb{R}^d$
	Ø±Ø§ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù†
}
\Ø¨Ù‡â€ŒØ§Ø²Ø§ÛŒ{ØªÚ©Ø±Ø§Ø± Ú©Ù†}
%\Ø§Ú¯Ø±{$|E| > 0$}
%	\Ø¯Ø³ØªÙˆØ±{ÛŒÚ© Ú©Ø§Ø±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡}
%\Ù¾Ø§ÛŒØ§Ù†â€ŒØ§Ú¯Ø±
\Ø¯Ø³ØªÙˆØ±{Ø­Ø§Ù„Øª Ø§ÙˆÙ„ÛŒÙ‡ $S$ Ø±Ø§ Ø¨Ø³Ø§Ø²}
\Ø¯Ø³ØªÙˆØ±{$1 \longrightarrow I$}
\ØªØ§ÙˆÙ‚ØªÛŒ{$S$ Ø­Ø§Ù„Øª Ù†Ù‡Ø§ÛŒÛŒ Ù†ÛŒØ³Øª:}
\Ø¯Ø³ØªÙˆØ±{
	$A \sim \pi_\theta(.|S)$
}
\Ø¯Ø³ØªÙˆØ±{Ø¹Ù…Ù„ $A$ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡ Ùˆ Ø­Ø§Ù„Øª $S'$ Ùˆ Ù¾Ø§Ø¯Ø§Ø´ $R$ Ø±Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†}

\Ø¯Ø³ØªÙˆØ±{
	$ \longrightarrow \delta$
	$R + \gamma v_\omega(S') - v_\omega(S)$
}
\Ø¯Ø³ØªÙˆØ±{
	$ \longrightarrow \omega$
	$\omega + \alpha^\omega I \delta \nabla_\omega v_\omega(S)$
}
\Ø¯Ø³ØªÙˆØ±{
	$ \longrightarrow \theta$
	$\theta +  \alpha^\theta   I  \delta \nabla_\theta \ ln \ \pi_\theta(A| S)$
}

\Ø¯Ø³ØªÙˆØ±{
	$\gamma I \longrightarrow I$
}

\Ø¯Ø³ØªÙˆØ±{
	$S' \longrightarrow S$
}
\Ù¾Ø§ÛŒØ§Ù†â€ŒØªØ§ÙˆÙ‚ØªÛŒ
\Ù¾Ø§ÛŒØ§Ù†â€ŒØ¨Ù‡â€ŒØ§Ø²Ø§ÛŒ
\Ù¾Ø§ÛŒØ§Ù†{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…}
\cite{suttonbook}
 




\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ TRPO}
\Ø´Ø±ÙˆØ¹{ØªØ¹Ø±ÛŒÙ}
$\rho_\pi : S \longrightarrow \mathbb{R}$
ØªØ§Ø¨Ø¹ ÙØ±Ú©Ø§Ù†Ø³ ØªØ®ÙÛŒÙ Ø¯Ø§Ø± Ø¯ÛŒØ¯Ù‡ Ø´Ø¯Ù† Ø­Ø§Ù„Øª Ù‡Ø§ Ø¨Ø§Ø´Ø¯. ÛŒØ¹Ù†ÛŒ
$\rho_\pi(s) = P(S_0=s) + \gamma P(S_1=s) + \gamma^2 P(S_2=s) + ...$

Ú©Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡
$S_0, S_1, S_2$
Ø®Ø· Ù…Ø´ÛŒ $\pi$ Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ù…ÛŒ Ú©Ù†Ø¯.

\Ù¾Ø§ÛŒØ§Ù†{ØªØ¹Ø±ÛŒÙ}

Ø§Ú¯Ø± $\pi$ Ùˆ $\pi'$ Ø¯Ùˆ Ø®Ø· Ù…Ø´ÛŒ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ 
$J(\pi) = \mathbb{E}_\pi[R_0]$
 Ù…ÛŒ ØªÙˆØ§Ù† Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯
\cite{schulman2015trust}

$$J(\pi')= J(\pi) + \sum_{s} \rho_{\pi'}(s) \sum_{a} \pi'(a|s) A_\pi(s,a)$$

Ú©Ù‡ 
$A_\pi(s,a) = Q_\pi (s,a) - V_\pi(s)$
ØªØ§Ø¨Ø¹ Ù…Ø²ÛŒØª Ø¹Ù…Ù„ $a$ Ø¯Ø± Ø­Ø§Ù„Øª $s$ Ø¨Ø§Ø´Ø¯.
ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ 
$\rho_\pi'(s)$
Ø¯Ø± Ø·Ø±Ù Ø±Ø§Ø³Øª ØªØ³Ø§ÙˆÛŒ Ø¨Ù‡ $\pi'$ Ø¨Ù‡ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø±Ø§ Ù…Ø´Ú©Ù„ Ù…ÛŒ Ú©Ù†Ø¯.

Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ø§ÛŒÙ† Ù…Ø´Ú©Ù„ 
\cite{schulman2015trust}
Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯ÛŒÚ¯Ø±ÛŒ $L_\pi(\pi')$ Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ù…ÛŒ Ú©Ù†Ø¯ Ùˆ Ù†Ø´Ø§Ù† Ù…ÛŒ Ø¯Ù‡Ø¯ Ú©Ù‡ Ø§Ú¯Ø± $\pi$ Ùˆ $\pi'$ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù†Ø²Ø¯ÛŒÚ© Ø¨Ø§Ø´Ù†Ø¯ØŒ Ø§ÙØ²Ø§ÛŒØ´ 
$L_\pi(\pi')$
 Ù‡Ù…ÙˆØ§Ø±Ù‡ Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ $J$ Ù‡Ù…Ø±Ø§Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯.

$$L_\pi(\pi')= J(\pi) + \sum_{s} \rho_{\pi}(s) \sum_{a} \pi'(a|s) A_\pi(s,a)$$

ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ú©Ù‡ $L_\pi$ Ø§Ø² ØªØ§Ø¨Ø¹ ÙØ±Ú©Ø§Ù†Ø³ $\rho_\pi$ Ø¨Ù‡ Ø¬Ø§ÛŒ $
\rho_{\pi'}$
 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ú©Ù†Ø¯.
 
\Ø´Ø±ÙˆØ¹{Ù‚Ø¶ÛŒÙ‡}
ÙØ±Ø¶ Ú©Ù†ÛŒØ¯ 
$\alpha = D_{TV}^max(\pi_{old}, \pi_{new})$
Ø¨Ø§Ø´Ø¯ Ú©Ù‡ 
$$D_{TV}^max (\pi, \pi') = \max_{s} D_{TV}(\pi(.|s) || \pi'(.|s)$$
Ùˆ 
$D_{TV}(p || q)$
Ø¯ÛŒÙˆØ±Ú˜Ø§Ù†Ø³ $total \ variation$ Ø¨ÛŒÙ† Ø¯Ùˆ Ø¨Ø±Ø¯Ø§Ø± $p$ Ùˆ $q$ Ø¨Ø§Ø´Ø¯

$$D_{TV}(p || q) = \dfrac{1}{2} \sum_{i} |p_i - q_i|$$

Ø¯Ø± Ø§ÛŒÙ† ØµÙˆØ±Øª

$$J(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \dfrac{4 \epsilon \gamma}{(1- \gamma)^2}$$
Ú©Ù‡ 
$\epsilon = \max_{s,a} |A_\pi(s,a)|$
\Ù¾Ø§ÛŒØ§Ù†{Ù‚Ø¶ÛŒÙ‡}

Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù‚Ø¶ÛŒÙ‡ ÙÙˆÙ‚ Ùˆ Ù†Ø§Ù…Ø¹Ø§Ø¯Ù„Ù‡ 
$D_{TV}(p || q)^2 \le D_{KL}(p || q)$
\cite{schulman2015trust}
Ú©Ù‡ 
$D_{KL} (p || q)$
Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ Ø¯ÛŒÙˆØ±Ú˜Ø§Ù†Ø³ KL Ø¯Ùˆ Ø¨Ø±Ø¯Ø§Ø± $p$ Ùˆ $q$ Ø§Ø³Øª. Ù…ÛŒ ØªÙˆØ§Ù†  Ù†ØªÛŒØ¬Ù‡ Ú¯Ø±ÙØª

$$J(\pi') \ge L_{\pi}(\pi') - C \ D_{KL}^max(\pi, \pi')$$

Ú©Ù‡

$$C = \dfrac{4 \epsilon \gamma}{(1-\gamma)^2}$$

Ø±Ø§Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒ Ø¯Ù‡Ø¯ Ú©Ù‡ Ù…ÛŒ ØªÙˆØ§Ù† ÛŒÚ© Ø¯Ù†Ø¨Ø§Ù„Ù‡ ØµØ¹ÙˆØ¯ÛŒ Ø§Ø² Ø®Ø· Ù…Ø´ÛŒ Ù‡Ø§ Ø¯Ø§Ø´Øª Ø¨Ù‡ Ø·ÙˆØ±ÛŒ Ú©Ù‡
$J(\pi_0) \le J(\pi_1) \le J(\pi_2) \le ...$.
Ú†Ø±Ø§ Ú©Ù‡ ÙØ±Ø¶ Ú©Ù†ÛŒØ¯ 
$M_i(\pi) = L_{\pi_i}(\pi) - C \ D_{KL}^max(\pi_i, \pi)$
Ø¯Ø± Ø§ÛŒÙ† ØµÙˆØ±Øª
$J(\pi_{i+1}) \ge M_i(\pi{i+1}) \\
J(\pi_i) = M_i(\pi_i)
$
Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†
$J(\pi_{i+1}) - J(\pi_i) \ge M_i(\pi_{i+1}) - M_i(\pi_i)$
Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø¨Ø§ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ú©Ø±Ø¯Ù† $M_i$ Ø¯Ø± Ù‡Ø± Ú¯Ø§Ù… Ù…ÛŒ ØªÙˆØ§Ù† Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ø±Ø¯ Ú©Ù‡ Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ $J$ ØºÛŒØ±Ù†Ø²ÙˆÙ„ÛŒ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯.

\Ø´Ø±ÙˆØ¹{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…}{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… $Policy Iteration$ Ø¨Ø§ Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ $L_\pi$}

\Ø¯Ø³ØªÙˆØ±{Ø®Ø· Ù…Ø´ÛŒ $\pi_0$
	Ø±Ø§ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù†
}
\Ø¨Ù‡â€ŒØ§Ø²Ø§ÛŒ{Ø¨Ø±Ø§ÛŒ 
	$i=0,1,...$
	 ØªÚ©Ø±Ø§Ø± Ú©Ù†}
%\Ø§Ú¯Ø±{$|E| > 0$}
%	\Ø¯Ø³ØªÙˆØ±{ÛŒÚ© Ú©Ø§Ø±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡}
%\Ù¾Ø§ÛŒØ§Ù†â€ŒØ§Ú¯Ø±
\Ø¯Ø³ØªÙˆØ±{Ù‡Ù…Ù‡ ÛŒ Ù…Ø²ÛŒØª Ù‡Ø§ÛŒ 
	$A_{\pi_i}(s,a)$
	Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†
}
\Ø¯Ø³ØªÙˆØ±{
$arg \max_{\pi} [L_{\pi_i}(\pi) - C \ D_{KL}^max(\pi_i, \pi)] \longrightarrow \pi_{i+1} = $
Ú©Ù‡ 
$C = (4 \epsilon \gamma) / (1-\gamma)^2$
Ùˆ 
$L_{\pi_i}(\pi) = J(\pi_i) + \sum_{s} \rho_{\pi_i}(s) \sum_{a} \pi(a|s) A_{\pi_i} (s,a)$
}
\Ù¾Ø§ÛŒØ§Ù†â€ŒØ¨Ù‡â€ŒØ§Ø²Ø§ÛŒ
\Ù¾Ø§ÛŒØ§Ù†{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…}

Ø§Ú¯Ø± $\hat{A}_t$ ØªØ®Ù…ÛŒÙ† Ù…Ø²ÛŒØª 
$A_{\pi_t}(S_t, A_t)$
Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ø¯Ø± Ú¯Ø§Ù… $t$ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒ Ø´ÙˆØ¯ØŒ Ù…ÛŒ ØªÙˆØ§Ù† Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯ Ú©Ù‡ Ø¯Ø± Ø±ÙˆØ´ TRPO Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ $L_\pi$ Ø¯Ø± Ù‡Ø± Ú¯Ø§Ù… Ø¨Ù‡ Ø´Ú©Ù„
$$\mathbb{E}_t\left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} \hat{A}_t \right]$$
Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯.

\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ PPO}
Ø¯Ø± Ø±ÙˆØ´ TRPO Ø¯ÛŒØ¯ÛŒÙ… Ú©Ù‡ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ú©Ù„Ø±Ø¯ $L_\pi$ Ø³Ø§Ø¯Ù‡ ØªØ± Ø§Ø² Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ $J$ Ø§Ø³Øª ÙˆÙ„ÛŒ Ø¯Ø± Ø¹ÙˆØ¶ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Gradient ascent ØªÙ†Ù‡Ø§ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø¹Ù…Ø§Ù„ ØªØºÛŒÛŒØ±Ø§Øª Ú©ÙˆÚ†Ú© Ø¯Ø± Ø®Ø· Ù…Ø´ÛŒ Ø§Ø³Øª. ÛŒÚ© Ø±Ø§Ù‡ Ø¯ÛŒÚ¯Ø± Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ ØªØºÛŒÛŒØ±Ø§Øª Ø®Ø· Ù…Ø´ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ clip Ø§Ø³Øª. Ø¯Ø± Ø±ÙˆØ´ TRPO Ø¯ÛŒØ¯ÛŒÙ… Ú©Ù‡  ØªØ§Ø¨Ø¹ Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ØŒ Ø¯Ø± Ú¯Ø§Ù… $t$ Ø¨Ù‡ Ø´Ú©Ù„ Ø²ÛŒØ± Ø§Ø³Øª

$$L_{\pi_{old}}(\pi_{new}) = \mathbb{E_t}\left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} A_{\pi}(S_t, A_t)\right]$$

ÙØ±Ø¶ Ú©Ù†ÛŒØ¯ 
$r_t(\theta_{new})$
 Ù†Ø³Ø¨Øª Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª 
$r_t(\theta_{new}) = \dfrac{\pi_{\theta_{new}}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$

Ø¨Ø§Ø´Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†
$$L_{\pi_{old}}(\pi_{new}) = \mathbb{E}_t\left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} A_{\pi}(S_t, A_t)\right] = \mathbb{E}_t\left[r_t(\theta_{new}) A_{\pi_{old}}(S_t,A_t)\right]$$

Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ 
$L^{CLIP}(\theta)$
Ø±Ø§ Ø¨Ù‡ Ø´Ú©Ù„ Ø²ÛŒØ± ØªØ¹Ø±ÛŒÙ Ù…ÛŒ Ú©Ù†ÛŒÙ…

$$L^{CLIP}(\theta) = \mathbb{E}_t\left[min(r_t(\theta) \hat{A}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right]$$

Ú©Ù‡ $\epsilon$ ÛŒÚ© Ø§Ø¨Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ± \Ù¾Ø§ÙˆØ±Ù‚ÛŒ{Hyperparameter} Ù…Ø«Ù„Ø§ 
$\epsilon=0.2$
Ø§Ø³Øª. Ø§ÙˆÙ„ÛŒÙ† Ø¹Ø¨Ø§Ø±Øª Ø¯Ø§Ø®Ù„ min Ù‡Ù…Ø§Ù† Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±ÙˆØ´ TRPO Ø§Ø³Øª. Ø¯Ø± Ø¹Ø¨Ø§Ø±Øª Ø¯ÙˆÙ… Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø²Ø±Ú¯ØªØ± Ø§Ø² $1+\epsilon$ ÛŒØ§ Ú©ÙˆÚ†Ú©ØªØ± Ø§Ø² 
$1-\epsilon$
Ø¯Ø± $r_t(\theta)$ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø¨Ù‡ $1+\epsilon$ Ùˆ $1-\epsilon$ ØªØºÛŒÛŒØ± Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù‡ Ø§Ù†Ø¯ ØªØ§ ØªØºÛŒÛŒØ±Ø§Øª Ø¨Ø²Ø±Ú¯ Ø®Ø· Ù…Ø´ÛŒ Ø±Ø§ Ú©Ù†ØªØ±Ù„ Ú©Ù†Ù†Ø¯. Ù†Ù‡Ø§ÛŒØªØ§ Ø¹Ø¨Ø§Ø±Øª Ú©ÙˆÚ†Ú©ØªØ± Ø§Ø² Ù…ÛŒØ§Ù† Ø§ÛŒÙ† Ø¯Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø§Ú¯Ø± Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ clip Ù†Ø´Ø¯Ù‡ (Ø¹Ø¨Ø§Ø±Øª Ø§ÙˆÙ„) Ú©ÙˆÚ†Ú©ØªØ± ÛŒØ§ Ù…Ø³Ø§ÙˆÛŒ Ø¨Ø§ Ø­Ø§Ù„Øª clip Ø´Ø¯Ù‡ (Ø¹Ø¨Ø§Ø±Øª Ø¯ÙˆÙ…) Ø¨Ø§Ø´Ø¯ØŒ $L^{CLIP}$ Ø¯Ù‚ÛŒÙ‚Ø§ Ù‡Ù…Ø§Ù† Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ $L_\pi$ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯. Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ clip Ø´Ø¯Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒ Ø´ÙˆØ¯ ØªØ§ Ø§Ø² ØªØºÛŒÛŒØ±Ø§Øª Ø¨Ø²Ø±Ú¯ Ø®Ø· Ù…Ø´ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø´ÙˆØ¯.


\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ Ù‡Ø§ÛŒ Q-learning}

Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ Ø±ÙˆØ´ Ù‡Ø§ÛŒ  Q-learning ØªÙ„Ø§Ø´ Ù…ÛŒ Ú©Ù†Ù†Ø¯ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§ ØªØ§Ø¨Ø¹ Ø§Ø±Ø²Ø´ Ø¹Ù…Ù„-Ø­Ø§Ù„Øª Ø¨Ù‡ÛŒÙ†Ù‡ $Q^*(s,a)$ Ø±Ø§ ØªØ®Ù…ÛŒÙ† Ø¨Ø²Ù†Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø¹Ù…ÙˆÙ„ Ø§Ø² ÛŒÚ© ØªØ§Ø¨Ø¹ Ù‡Ø¯Ù Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø¹Ø§Ø¯Ù„Ù‡ Ø¨Ù„Ù…Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ú©Ù†Ù†Ø¯. Ø§ÛŒÙ† Ø¨Ù‡ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª off-policy Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒ Ø´ÙˆØ¯ØŒ Ø¨Ù‡ Ø§ÛŒÙ† Ù…Ø¹Ù†ÛŒ Ú©Ù‡ Ù‡Ø± Ø¨Ù‡ Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ Ø¬Ù…Ø¹ Ø¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡ Ø¯Ø± Ù‡Ø± Ù†Ù‚Ø·Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ø¯ ØŒ Ø¨Ø¯ÙˆÙ† Ø¯Ø±Ù†Ø¸Ø±Ú¯Ø±ÙØªÙ† Ù†Ø­ÙˆÙ‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø¹Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ú©Ø´Ù Ù…Ø­ÛŒØ· Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø¨Ø¯Ø³Øª Ø¢ÙˆØ±Ø¯Ù† Ø¯Ø§Ø¯Ù‡ Ù‡Ø§. Ø®Ø· Ù…Ø´ÛŒ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÛŒÙ† 
$Q^*$
Ùˆ
$\pi^*$
 Ø¨Ø¯Ø³Øª Ù…ÛŒ Ø¢ÛŒØ¯: Ø¹Ø§Ù…Ù„ Ø¨Ø¹Ø¯ Ø§Ø² ÛŒØ§Ø¯Ú¯Ø±ÙØªÙ† ØªØ§Ø¨Ø¹ $Q_\theta(s,a)$ Ø¨Ù‡ Ø·ÙˆØ±ÛŒÚ©Ù‡ 
 $Q_\theta(s,a) \approx Q^*(s,a)$
Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø¹Ù…Ù„ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¯Ø± Ø­Ø§Ù„Øª $s$ Ø±Ø§ Ø¨Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†Ø¯

$$a(s) = arg \max_a Q_{\theta}(s,a)$$.

Ø§Ø² Ø¬Ù…Ù„Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù‡Ø§ÛŒ Q-learning Ù…ÛŒ ØªÙˆØ§Ù† Ø¨Ù‡ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯


Ø±ÙˆØ´ Ú©Ù„Ø§Ø³ÛŒÚ© DQN Ú©Ù‡ Ø­ÙˆØ²Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ Ú˜Ø±Ù Ø±Ø§ Ø¹Ù…ÛŒÙ‚Ø§ Ø§Ø±ØªÙ‚Ø§ Ø¨Ø®Ø´ÛŒØ¯

Ø±ÙˆØ´ C51 Ú©Ù‡ ØªÙˆØ²ÛŒØ¹ÛŒ Ø±ÙˆÛŒ Ø¹Ø§ÛŒØ¯ÛŒ Ø±Ø§ Ù…ÛŒ Ø¢Ù…ÙˆØ²Ø¯ Ú©Ù‡ Ø§Ù…ÛŒØ¯Ø±ÛŒØ§Ø¶ÛŒ Ø¢Ù† $Q^*$ Ø§Ø³Øª

\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ DQN}

Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø¨Ø±Ø§ÛŒ ØªÙ‚Ø±ÛŒØ¨ Ø²Ø¯Ù† ØªÙˆØ§Ø¨Ø¹ Ø§Ø±Ø²Ø´ Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒØŒ Ø§Ø² ÛŒÚ© ØªØ§Ø¨Ø¹ Ø®Ø·ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ø´ÙˆØ¯.
Ø§Ù…Ø§ Ú¯Ø§Ù‡ÛŒ Ø§ÙˆÙ‚Ø§Øª Ø§Ø² ÛŒÚ© ØªÙ‚Ø±ÛŒØ¨ Ø¹Ù…Ù„Ú©Ø±Ø¯ ØºÛŒØ± Ø®Ø·ÛŒ Ø¨Ù‡ Ø¬Ø§ÛŒ Ø¢Ù† ØŒ Ù…Ø§Ù†Ù†Ø¯ ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ø´ÙˆØ¯. Ø´Ø¨Ú©Ù‡ Ù‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ø´Ø¨Ú©Ù‡ Q\Ù¾Ø§ÙˆØ±Ù‚ÛŒ{Q-Network} Ø´Ù†Ø§Ø®ØªÙ‡ Ù…ÛŒ Ø´ÙˆÙ†Ø¯.
Ø´Ø¨Ú©Ù‡ Q Ø±Ø§ Ù…ÛŒ ØªÙˆØ§Ù† Ø¨Ø§ Ú©Ù…ÛŒÙ†Ù‡ Ø³Ø§Ø®ØªÙ† Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø§ÛŒ Ø§Ø² ØªÙˆØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ù‡ Ø´Ú©Ù„ 
$L_1(\theta_1), L_2(\theta_2), L_3(\theta_3), ... $

 Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ø› Ø¨Ù‡ Ø·ÙˆØ±ÛŒ Ú©Ù‡

$$L_i(\theta_i)=\mathbb{E}\left[(y_i - Q(s,a;\theta_i))^2\right]$$

Ú©Ù‡ 
$$y_i = \mathbb{E}[r + \gamma \max_{a'} Q(s',a'; \theta_{i-1})| s,a]$$.

Ø¨Ø§ Ù…Ø´ØªÙ‚ Ú¯Ø±ÙØªÙ† Ø§Ø² ØªØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ
$\theta_i$
 Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø´Øª

$$\nabla_{\theta_i} L_i{\theta_i} = \mathbb{E}\left[ \left(r + \gamma \max{a'} Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i)\right) \nabla_{\theta_i} Q(s,a;\theta_i)\right]$$

%Rather than computing the full expectations in the above gradient, it is often computationally expedient to optimise the loss function by stochastic gradient descent. If the weights are updated after
%every time-step, and the expectations are replaced by single samples from the behaviour distribution
%Ï and the emulator E respectively, then we arrive at the familiar Q-learning algorithm [26].


Ø¨Ù‡ Ø¬Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ÛŒØ¯Ø±ÛŒØ§Ø¶ÛŒ Ú©Ø§Ù…Ù„ Ø¯Ø± Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† ÙÙˆÙ‚ ØŒ ØºØ§Ù„Ø¨Ø§Ù‹ Ø§Ø² Ù†Ø¸Ø± Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒØŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ ØªØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø§ Ù†Ø²ÙˆÙ„ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† ØªØµØ§Ø¯ÙÛŒ  \Ù¾Ø§ÙˆØ±Ù‚ÛŒ{stochastic gradient descend} Ø±Ø§Ù‡ Ø­Ù„ Ø¨Ù‡ØªØ±ÛŒ Ø§Ø³Øª. Ø§Ú¯Ø± Ø¯Ø± Ù‡Ø± Ù…Ù‚Ø·Ø¹ Ø²Ù…Ø§Ù†ÛŒØŒ ÙˆØ²Ù† Ù‡Ø§ Ø¨Ù‡ Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´ÙˆØ¯ Ùˆ Ø§Ù…ÛŒØ¯Ø±ÛŒØ§Ø¶ÛŒ Ø¨Ø§ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ø®Ø· Ù…Ø´ÛŒ Ø±ÙØªØ§Ø± \Ù¾Ø§ÙˆØ±Ù‚ÛŒ{behavior policy} Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´ÙˆØ¯ØŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Q-learning Ø¨Ù‡ Ø´Ú©Ù„ Ø²ÛŒØ± Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯.

\Ø´Ø±ÙˆØ¹{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…}{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Q-learning Ø¨Ø§ Experience replay}

\Ø¯Ø³ØªÙˆØ±{Ø­Ø§ÙØ¸Ù‡ 
	replay 
	$D$
Ø±Ø§ Ù…Ù‚Ø¯Ø§Ø± Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù†}
\Ø¯Ø³ØªÙˆØ±{ØªØ§Ø¨Ø¹ Ø¹Ù…Ù„-Ø§Ø±Ø²Ø´ Q Ø±Ø§ Ø¨Ø§ ÙˆØ²Ù† Ù‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù†}
\Ø¨Ù‡â€ŒØ§Ø²Ø§ÛŒ{Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ù¾ÛŒØ²ÙˆØ¯ 
$1...M$}
\Ø¯Ø³ØªÙˆØ±{Ø¯Ù†Ø¨Ø§Ù„Ù‡ 
	$d_1 = \{S_1\}$
	 Ùˆ Ú©Ø¯ÛŒÙ†Ú¯ 
	 $\phi_1 = \phi(d_1)$
	  Ø±Ø§ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù†}
\Ø¨Ù‡â€ŒØ§Ø²Ø§ÛŒ{Ø¨Ø±Ø§ÛŒ $t=1...T$}
\Ø¯Ø³ØªÙˆØ±{Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ $\epsilon$ ÛŒÚ© Ø¹Ù…Ù„ ØªØµØ§Ø¯ÙÛŒ $a_t$ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ØŒ Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª 
	$a_t = \max_{a} Q^*(\phi(d_t),a;\theta)$
	 Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†}
 \Ø¯Ø³ØªÙˆØ±{Ø¹Ù…Ù„ $a_t$ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡ Ùˆ Ø­Ø§Ù„Øª $S_{t+1}$ Ùˆ Ù¾Ø§Ø¯Ø§Ø´ $R_t$ Ø±Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†}
 \Ø¯Ø³ØªÙˆØ±{Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡ 
 	$d_{t+1} = d_t,a_t,S_{t+1}$
 	 Ùˆ 
 	 $\phi_{t+1} = \phi(d_{t+1})$
  }
\Ø¯Ø³ØªÙˆØ±{ØªØ¬Ø±Ø¨Ù‡ 
	$(\phi_t, A_t, R_t, \phi_{t+1})$
	 Ø±Ø§ Ø¯Ø± $D$ Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†}
 \Ø¯Ø³ØªÙˆØ±{ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² ØªØ¬Ø±ÛŒÙ‡ Ù‡Ø§ÛŒ  
 	$(\phi(j), A_j, R_j, \phi_{j+1})$
 	Ø§Ø² Ø§Ù†Ø¨Ø§Ø± ØªØ¬Ø±Ø¨ÛŒØ§Øª $D$ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†}
 \Ø¯Ø³ØªÙˆØ±{Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡ 
 	\lr{
 	$y_j =$ 
 	\begin{cases}
 		$r_j$
 		 &
 		  $\phi_{j+1} \  terminal$ \\
 		$r_j$ & $otherwise$
 	\end{cases}}
}
\Ø¯Ø³ØªÙˆØ±{ÛŒÚ© Ú¯Ø§Ù… Ø§Ø² Ù†Ø²ÙˆÙ„ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡ 
	$(y_j - Q(\phi_j, a_j; \theta))^2$
	 Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡}
  \Ù¾Ø§ÛŒØ§Ù†â€ŒØ¨Ù‡â€ŒØ§Ø²Ø§ÛŒ
\Ù¾Ø§ÛŒØ§Ù†â€ŒØ¨Ù‡â€ŒØ§Ø²Ø§ÛŒ
\Ù¾Ø§ÛŒØ§Ù†{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…}

%Note that this algorithm is model-free: it solves the reinforcement learning task directly using samples from the emulator E, without explicitly constructing an estimate of E. It is also off-policy: it
%learns about the greedy strategy a = maxa Q(s, a; Î¸), while following a behaviour distribution that
%ensures adequate exploration of the state space. In practice, the behaviour distribution is often selected by an -greedy strategy that follows the greedy strategy with probability 1 âˆ’  and selects a
%random action with probability \epsilon

ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ Ø§ÛŒÙ† ÛŒÚ© Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„ Ø§Ø³Øª: Ø§ÛŒÙ† Ú©Ø§Ø± ÙˆØ¸ÛŒÙÙ‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡ Ù‡Ø§ÛŒ Ø´Ø¨ÛŒÙ‡ Ø³Ø§Ø² E Ø¨Ø¯ÙˆÙ† Ø³Ø§Ø®ØªÙ† ØµØ±ÛŒØ­ ØªØ®Ù…ÛŒÙ† E. Ø­Ù„ Ù…ÛŒ Ú©Ù†Ø¯.
Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø­Ø±ÛŒØµ a = maxa Q (s ØŒ aØ› Î¸) ÛŒØ§Ø¯ Ù…ÛŒ Ú¯ÛŒØ±Ø¯ ØŒ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ ØªÙˆØ²ÛŒØ¹ Ø±ÙØªØ§Ø±ÛŒ Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ù…ÛŒ Ú©Ù†Ø¯ Ú©Ù‡
Ú©Ø§ÙˆØ´ Ú©Ø§ÙÛŒ Ø¯Ø± ÙØ¶Ø§ÛŒ Ø¯ÙˆÙ„Øª Ø±Ø§ ØªØ¶Ù…ÛŒÙ† Ù…ÛŒ Ú©Ù†Ø¯. Ø¯Ø± Ø¹Ù…Ù„ ØŒ ØªÙˆØ²ÛŒØ¹ Ø±ÙØªØ§Ø± Ø§ØºÙ„Ø¨ ØªÙˆØ³Ø· ÛŒÚ© Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Greed Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒ Ø´ÙˆØ¯ Ú©Ù‡ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø­Ø±ÛŒØµØ§Ù†Ù‡ Ø±Ø§ Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ 1 Ø¯Ù†Ø¨Ø§Ù„ Ù…ÛŒ Ú©Ù†Ø¯ - Ùˆ ÛŒÚ©
Ø§Ù‚Ø¯Ø§Ù… ØªØµØ§Ø¯ÙÛŒ Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ $\epsilon$

\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ C51}



\Ù‚Ø³Ù…Øª{Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø±ÙˆØ´ Ø¨Ù‡ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ Ø®Ø· Ù…Ø´ÛŒ Ùˆ Q-learning}
Ù†Ù‚Ø·Ù‡ Ù‚ÙˆØª Ø§ØµÙ„ÛŒ Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ Ø®Ø· Ù…Ø´ÛŒØŒ Ø§ØµÙˆÙ„ÛŒ Ø¨ÙˆØ¯Ù† Ø¢Ù†Ù‡Ø§Ø³Øª ØŒ Ø¨Ù‡ Ø§ÛŒÙ† Ù…Ø¹Ù†Ø§ Ú©Ù‡ Ø´Ù…Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ú†ÛŒØ²ÛŒ Ú©Ù‡ Ù…ÛŒ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ø±Ø§ Ø¨Ù‡ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ Ù…ÛŒ Ú©Ù†ÛŒØ¯. Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ø§ÛŒÙ† Ø±ÙˆØ´ Ù‡Ø§ Ù‚Ø§Ø¨Ù„ Ø§ØªÚ©Ø§ Ùˆ Ø¨Ø§Ø«Ø¨Ø§Øª Ù‡Ø³ØªÙ†Ø¯. Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ ØŒ Ø±ÙˆØ´Ù‡Ø§ÛŒ 
\rl{Q-learning}
Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªØ§Ø¨Ø¹ QØŒ Ù…Ù‚ÛŒØ§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± ØºÛŒØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ÛŒÙ†Ù‡ Ù…ÛŒ Ú©Ù†Ø¯. Ø­Ø§Ù„Øª Ù‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†ÙˆØ¹ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø¨Ù‡ Ø´Ú©Ø³Øª Ù…Ù†ØªÙ‡ÛŒ Ù…ÛŒ Ø´ÙˆØ¯ØŒ Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø§ÛŒÙ† Ø±ÙˆØ´ Ù‡Ø§  Ø«Ø¨Ø§Øª Ú©Ù…ØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯. 
\cite{suttonbook}
Ø§Ù…Ø§ ØŒ Ø±ÙˆØ´ Ù‡Ø§ÛŒ 
\rl{Q-learning}
Ø§ÛŒÙ† Ù…Ø²ÛŒØª Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯ Ú©Ù‡ Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ú©Ø§Ø± ØŒ Ø¨Ù‡ Ø·ÙˆØ± Ù‚Ø§Ø¨Ù„ Ù…Ù„Ø§Ø­Ø¸Ù‡ Ø§ÛŒ Ú©Ø§Ø±Ø¢Ù…Ø¯ Ù‡Ø³ØªÙ†Ø¯ ØŒ Ø²ÛŒØ±Ø§ Ø¢Ù†Ù‡Ø§ Ù…ÛŒ ØªÙˆØ§Ù†Ù†Ø¯ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ù…ÙˆØ«Ø±ØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ ØªÚ©Ù†ÛŒÚ© Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ Ø®Ø· Ù…Ø´ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù†Ø¯.


ØªØ¹Ø§Ù…Ù„ Ø¨ÛŒÙ† Ø¨Ù‡ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ Ø®Ø· Ù…Ø´ÛŒ Ùˆ Q-learning. Ø¨Ù‡ÛŒÙ†Ù‡ Ø³Ø§Ø²ÛŒ Ø®Ø· Ù…Ø´ÛŒ Ùˆ Q-learning Ù†Ø§Ø³Ø§Ø²Ú¯Ø§Ø± Ù†ÛŒØ³ØªÙ†Ø¯ (Ùˆ Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒ Ø±Ø³Ø¯ ØªØ­Øª Ø¨Ø±Ø®ÛŒ Ø´Ø±Ø§ÛŒØ· ØŒ Ù…Ø¹Ø§Ø¯Ù„ Ø¢Ù† Ø¨Ø§Ø´Ø¯) ØŒ Ùˆ Ø·ÛŒÙ ÙˆØ³ÛŒØ¹ÛŒ Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù‡Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø¨ÛŒÙ† Ø¯Ùˆ Ø­Ø¯ Ø§ÛŒÙ† Ø·ÛŒÙ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ù†Ù†Ø¯. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ø·ÛŒÙ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ù†Ù†Ø¯ Ù‚Ø§Ø¯Ø±Ù†Ø¯ Ø¨Ø§ Ø¯Ù‚Øª Ø¨ÛŒÙ† Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ùˆ Ø¶Ø¹Ù Ø·Ø±ÙÛŒÙ† Ù…Ø¹Ø§Ù…Ù„Ù‡ Ú©Ù†Ù†Ø¯. Ù…Ø«Ø§Ù„Ù‡Ø§ Ø´Ø§Ù…Ù„

DDPG
 ØŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…ÛŒ Ø§Ø³Øª Ú©Ù‡ Ù‡Ù…Ø²Ù…Ø§Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ø± ÛŒÚ© Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯ÛŒÚ¯Ø±ÛŒ ØŒ ÛŒÚ© Ø®Ø· Ù…Ø´ÛŒ Ù‚Ø·Ø¹ÛŒ Ùˆ ÛŒÚ© ØªØ§Ø¨Ø¹ Q Ø±Ø§ ÛŒØ§Ø¯ Ù…ÛŒ Ú¯ÛŒØ±Ø¯ ØŒ
Ùˆ SAC ØŒ Ù†ÙˆØ¹ÛŒ Ú©Ù‡ Ø§Ø² Ø®Ø· Ù…Ø´ÛŒ Ù‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ ØŒ ØªÙ†Ø¸ÛŒÙ… Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ  \Ù¾Ø§ÙˆØ±Ù‚ÛŒ{entropy regularization}Ùˆ Ú†Ù†Ø¯ ØªØ±ÙÙ†Ø¯ Ø¯ÛŒÚ¯Ø± Ø¨Ø±Ø§ÛŒ ØªØ«Ø¨ÛŒØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ùˆ Ú©Ø³Ø¨ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² DDPG Ø¯Ø± Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ú©Ù†Ø¯.


\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ DDPG}
Here we combine the actor-critic approach with insights from the recent success of Deep Q Network
(DQN) (Mnih et al., 2013; 2015). Prior to DQN, it was generally believed that learning value
functions using large, non-linear function approximators was difficult and unstable. DQN is able
to learn value functions using such function approximators in a stable and robust way due to two
innovations: 1. the network is trained off-policy with samples from a replay buffer to minimize
correlations between samples; 2. the network is trained with a target Q network to give consistent
targets during temporal difference backups. In this work we make use of the same ideas, along with
batch normalization (Ioffe & Szegedy, 2015), a recent advance in deep learning.

\Ø´Ø±ÙˆØ¹{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…}{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… DDPG}
\Ø¯Ø³ØªÙˆØ±{Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ 
	$\theta_\mu$
	 Ùˆ 
	 $\theta_Q$
	 Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¨Ø§Ø²ÛŒÚ¯Ø± 
	 $\mu(s;\theta_\mu)$
	 Ùˆ Ù…Ù†ØªÙ‚Ø¯
	 $Q(s,a;\theta_Q)$
	 Ø±Ø§ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù†.
}
\Ø¯Ø³ØªÙˆØ±{Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØªÙˆØ§Ø¨Ø¹ Ù‡Ø¯Ù $\mu'$ Ùˆ $Q'$ Ø±Ø§ Ø¨Ø§ ÙˆØ²Ù† Ù‡Ø§ÛŒ 
$\theta_{\mu'} \longleftrightarrow \theta_\mu$
Ùˆ
$\theta_{Q'} \longleftarrow \theta_Q$

Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù†
}

\Ø¯Ø³ØªÙˆØ±{Ø­Ø§ÙØ¸Ù‡ ØªÚ©Ø±Ø§Ø±Ù‡Ø§ $R$ Ø±Ø§ Ø¨Ø³Ø§Ø²}
\â€ŒØ¨Ù‡â€ŒØ§Ø²Ø§ÛŒ{Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ù¾ÛŒØ²ÙˆØ¯ $1...M$}
\Ø¯Ø³ØªÙˆØ±{ÛŒÚ© ØªØ§Ø¨Ø¹ Ù†ÙˆÛŒØ² ØªØµØ§Ø¯ÙÛŒ $\mathbb{N}$} Ø¨Ø³Ø§Ø²
\Ø¯Ø³ØªÙˆØ±{Ø­Ø§Ù„Øª Ø§ÙˆÙ„ÛŒÙ‡ $S_1$ Ø±Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†}
\â€ŒØ¨Ù‡â€ŒØ§Ø²Ø§ÛŒ{Ø¨Ø±Ø§ÛŒ $t=1...T$}
\Ø¯Ø³ØªÙˆØ±{Ø¹Ù…Ù„ 
	$a_t = \mu(s_t; \theta_\mu) + \mathbb{N}_t$
	 Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø®Ø· Ù…Ø´ÛŒ ÙØ¹Ù„ÛŒ Ùˆ Ù†ÙˆÛŒØ² Ø§Ú©ØªØ´Ø§ÙØŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù† Ùˆ Ø­Ø§Ù„Øª Ø¨Ø¹Ø¯ÛŒ $S_{t+1}$} Ùˆ Ù¾Ø§Ø¯Ø§Ø´ $R_t$ Ø±Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†.
 \Ø¯Ø³ØªÙˆØ±{ØªØ¬Ø±Ø¨Ù‡ 
 	$(s_t, a_t, r_t, s_{t+1})$
 	 Ø±Ø§ Ø¯Ø± Ø§Ù†Ø¨Ø§Ø± ØªØ¬Ø±Ø¨Ù‡ $R$ Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†}
  \Ø¯Ø³ØªÙˆØ±{ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ $N$ Ø§Ø² ØªØ¬Ø±Ø¨Ù‡ Ù‡Ø§ÛŒ $(s_i, a_i, r_i, s_{i+1})$ Ø§Ø² Ø§Ù†Ø¨Ø§Ø± ØªØ¬Ø±Ø¨Ù‡ $R$ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù† }
  \Ø¯Ø³ØªÙˆØ±{ÙˆØ²Ù† Ù‡Ø§ÛŒ Ù…Ù†ØªÙ‚Ø¯ $\theta_Q$ Ø±Ø§ Ø¨Ø§ Ø¯Ø±Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† ØªØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡ 
  	$L = \dfrac{1}{N} \sum_i(y_i - Q(S_i, A_i; \theta_Q))^2$
  	 Ø¨Ù‡ Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†}
   \Ø¯Ø³ØªÙˆØ±{ÙˆØ²Ù† Ù‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒÚ¯Ø± $\theta_\mi$ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø®Ø· Ù…Ø´ÛŒ Ù†Ù…ÙˆÙ†Ù‡ 
   	$$\nabla_{\theta_\mu} J \approx \dfrac{1}{N} \sum_{i} \nabla_a Q(s,a;\theta_Q) |_{s=s_i, a=\mu(s_i)} \nabla_{\thata_\mu} \mu(s;\theta_\mu) |_{S_i}$$
   	 Ø¨Ù‡ Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†}
    
   \Ø¯Ø³ØªÙˆØ±{ÙˆØ²Ù† Ù‡Ø§ÛŒ ØªÙˆØ§Ø¨Ø¹ Ù‡Ø¯Ù Ø±Ø§ Ø¨Ù‡ Ø´Ú©Ù„ 
   	$$\theta_{Q'} = \tau \theta_Q + (1-\tau) \theta_{Q'} \\
   	\theta_{\mu'} = \tau \theta_\mu + (1-\tau) \theta_{\mu'}
   	$$
   	 Ø¨Ù‡ Ø±ÙˆØ² Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†}
\Ù¾Ø§ÛŒØ§Ù†â€ŒØ¨Ù‡â€ŒØ§Ø²Ø§ÛŒ
\Ù¾Ø§ÛŒØ§Ù†â€ŒØ¨Ù‡â€ŒØ§Ø²Ø§ÛŒ
\Ù¾Ø§ÛŒØ§Ù†{Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…}

\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ SAC}

\Ù‚Ø³Ù…Øª{Ø±ÙˆØ´ \nf Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø¯Ù„}

Unlike model-free RL, there arenâ€™t a small number of easy-to-define clusters of methods for model-based RL: there are many orthogonal ways of using models. Weâ€™ll give a few examples, but the list is far from exhaustive. In each case, the model may either be given or learned.

Background: Pure Planning. The most basic approach never explicitly represents the policy, and instead, uses pure planning techniques like model-predictive control (MPC) to select actions. In MPC, each time the agent observes the environment, it computes a plan which is optimal with respect to the model, where the plan describes all actions to take over some fixed window of time after the present. (Future rewards beyond the horizon may be considered by the planning algorithm through the use of a learned value function.) The agent then executes the first action of the plan, and immediately discards the rest of it. It computes a new plan each time it prepares to interact with the environment, to avoid using an action from a plan with a shorter-than-desired planning horizon.

The MBMF work explores MPC with learned environment models on some standard benchmark tasks for deep RL.
Expert Iteration. A straightforward follow-on to pure planning involves using and learning an explicit representation of the policy, \pi_{\theta}(a|s). The agent uses a planning algorithm (like Monte Carlo Tree Search) in the model, generating candidate actions for the plan by sampling from its current policy. The planning algorithm produces an action which is better than what the policy alone would have produced, hence it is an â€œexpertâ€ relative to the policy. The policy is afterwards updated to produce an action more like the planning algorithmâ€™s output.

The ExIt algorithm uses this approach to train deep neural networks to play Hex.
AlphaZero is another example of this approach.
Data Augmentation for Model-Free Methods. Use a model-free RL algorithm to train a policy or Q-function, but either 1) augment real experiences with fictitious ones in updating the agent, or 2) use only fictitous experience for updating the agent.

See MBVE for an example of augmenting real experiences with fictitious ones.
See World Models for an example of using purely fictitious experience to train the agent, which they call â€œtraining in the dream.â€
Embedding Planning Loops into Policies. Another approach embeds the planning procedure directly into a policy as a subroutineâ€”so that complete plans become side information for the policyâ€”while training the output of the policy with any standard model-free algorithm. The key concept is that in this framework, the policy can learn to choose how and when to use the plans. This makes model bias less of a problem, because if the model is bad for planning in some states, the policy can simply learn to ignore it.

See I2A for an example of agents being endowed with this style of imagination.