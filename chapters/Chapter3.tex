
\فصل{نتایج اخیر}
در این فصل به معرفی برخی از روش های مدرن در یادگیری تقویتی می پردازیم. روش های معرفی شده در این فصل به دو دسته کلی مبتنی بر مدل و بدون مدل تقسیم می شوند. تفاوت اصلی این روش های در این است که آیا در الگوریتم یادگیری از تابع انتقال MDP یا تخمینی از آن استفاده می شود یا خیر. در روش های بدون مدل، عامل هیچ اطلاعی از دینامیک محیط ندارد و تنها از طریق تجربه می تواند بیاموزد. در مقابل روش های مبتنی بر مدل دسترسی کامل یا تقریبی به تابع انتقال را مفروض می گیرند. البته برخی از روش های مبتنی بر مدل می توانند از طریق تجربه تخمینی از دینامیک محیط را یاد بگیرند.


\قسمت{روش\nf های مبتنی بر مدل و بدون مدل}


یکی از مهمترین نقاط انشعاب در الگوریتم\nf های RL این است که آیا عامل به یک مدل از محیط دسترسی دارد یا  توانایی آموختن مدلی از محیط را دارد؟ منظور از مدل محیط ، تابعی است که انتقال و پاداش هر حالت-عمل را پیش بینی می کند.

نکته مثبت اصلی در داشتن مدل این است که به عامل اجازه می دهد با تفکر از قبل ، ببیند چه اتفاقی برای طیف وسیعی از گزینه های ممکن رخ می دهد و به صراحت در مورد گزینه های خود تصمیم بگیرد. سپس عامل می تواند نتایج حاصل از برنامه ریزی قبلی را در قالب یک خط مشی بیاموزد یک نمونه مشهور از این روش AlphaZero است. در عمل، اگر دستیابی به مدلی از محیط امکان پذیر و عملی باشد، معمولا از روش های مبتنی بر مدل استفاده می شود. زیرا می تواند باعث بهبود قابل توجه کارایی نمونه نسبت به روش های بدون مدل شوند.

اصلی ترین نقطه ضعف این روش ها این است که معمولا یک مدل کامل از محیط  در دسترس عامل نیست و عامل، باید مدل را کاملاً از طریق تجربه یاد بگیرد.

الگوریتم هایی که از یک مدل استفاده می کنند ، روش های مبتنی بر مدل و آنهایی که از چنین مدلی استفاده نمی کنند، بدون مدل نامیده می شوند ، نامیده می شوند. در حالی که روش های بدون مدل از دستاوردهای بالقوه در بهره وری نمونه با استفاده از مدل چشم پوشی می کنند ، اما پیاده سازی و تنظیم آنها آسان تر است. به همین خاطر، روش های بدون مدل از محبوبیت بیشتری برخوردار بوده و به طور گسترده تری نسبت به روش های مبتنی بر مدل توسعه و آزمایش شده اند.

\قسمت{روش های بدون مدل}

\قسمت{روش های مبتنی بر ارزش}


در روشهای یادگیری تقویتی بدون مدل مبتنی بر ارزش
تابع ارزش عمل با استفاده از  یک function approximator، مانند شبکه عصبی ، نشان داده می شود. فرض کنید
$Q(s,a;\theta)$
یک تابع ارزش عمل تقریبی با پارامتر 
$\theta$
باشد
الگوریتم های مختلفی برای بروزرسانی $\theta$ وجود دارد
الگوریتم Q-learning یکی از نمونه های چنین الگوریتمی است
که هدف آن تقریب مستقیم تابع عمل-ارزش بهینه 
$Q^*(s,a) \approx Q(s,a: \theta)$
 است

در Q-learning یک مرحله ای، پارامترهای $\theta$ از تابع عمل-ارزش با به حداقل رساندن تابع هزینه به شکل مرحله به مرحله آموخته می شوند، به شکلی که تابع هزینه i-ام به شکل 
$$L_i(\theta_i) = \mathbb{E} {\left( r+\gamma \max_{a'} Q(s',a'; \theta_{i-1})- Q(s,a:\theta_i) \right)}^2$$
 تعریف می شود که 
 $s'$
 حالتی است که بعد از حالت $s$ دیده می شود.

\قسمت{روش های مبتنی بر خط مشی}

در این قسمت روشهایی را در نظر می گیریم که به جای استفاده از تابع عمل-ارزش یا حالت-ارزش برای دستیابی به خط مشی بهینه، یک خط مشی پارامتریزه \پاورقی{Parameterized} شده را می آموزد که می تواند اقدامات را بدون استفاده از یک تابع ارزش، انتخاب کند. یک تابع ارزش
ممکن است همچنان برای یادگیری پارامترهای خط مشی استفاده شود ، اما برای انتخاب اقدام مورد نیاز نیست. ما از نماد
$\theta \in \mathbb{R}^{d'}$
 


برای بردار پارامتر خط مشی استفاده می کنیم.
بر خلاف روشهای مبتنی بر ارزش ، روشهای مبتنی بر خط مشی مستقیماً تابع خط مشی 
$\pi_\theta(a|s)$
 را تخمین می زنند و پارامترهای $\theta$ را با استفاده از \مهم{صعود گرادیان} روی یک مقیاس عملکرد \پاورقی{Performance Measure} 
$J(\pi_\theta)$

یا به طور مستقیم و یا با بیشینه سازی تخمین های محلی از 
$J(\pi_\theta)$
بروزرسانی می کند.  این روش تقریبا همیشه به صورت on-policy  عمل می کنند. 

همانطور که در ادامه خواهیم دید، می توان از توابع مختلفی برای مقیاس عملکرد $J$ استفاده نمود. یک انتخاب بدیهی 
$J(\pi_\theta) = \mathbb{E}[R_t]$
است. این روش های تلاش می کنند تابع $J$ را بیشینه کنند

$$\theta_{t+1}=\theta_t + \alpha \hat{\nabla J(\theta_t)}$$

که 
$\hat{\nabla J(\theta_t)}$

تخمینی احتمالاتی است که که امیدریاضی آن گرادیان مقیاس عملکرد $J$ را نسبت به پارامترهای خط مشی $\theta_t$ تخمین می زند.

به روش هایی که چنین الگویی را برای محاسبه خط مشی بهینه دنبال می کنند، روش های \مهم{گرادیان خط مشی} \پاورقی{Policy Gradient} می گوییم. دسته ای از روش های گرادیان خط مشی وجود دارند که تلاش می کنند تخمینی از تابع ارزش را نیز محاسبه کنند. به چنین روش هایی روش های، \مهم{بازیگر-منتقد} (Actor-Critic) گفته می شود که \مهم{بازیگر} (Actor) اشاره به خط مشی آموخته شده و \مهم{منتقد} (Critic) اشاره به تابع ارزش آموخته شده (معمولا یک تابع حالت-ارزش) دارد.

\شروع{قضیه}[گرادیان خط مشی]
$$\nabla J(\pi_\theta) \propto \sum_{s} \mu(s) \sum_{a} q_\pi(s,a) \nabla_\theta \pi_\theta(a|s)$$

که $\mu$ یک توزیع احتمال روی $S$ است که متناسب با تعداد دفعاتی است که حالت $s$ با دنبال کردن خط مشی 
$\pi_\theta$
 تکرار می شود.
\پایان{قضیه}


می توان نشان داد 
\cite{(suttonbook)}

$$\nabla J(\pi_\theta) = \mathbb{E}_\pi \left[ R_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} \right]$$
بنابراین در هر گام
$\left[ R_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} \right]$
یک تخمین گر نااریب از $\nabla J(\pi_\theta)$ خواهد بود. پس می توان در هر گام $\theta$ را به شکل زیر بروزرسانی کرد

$$\theta_{t+1} = \theta_t + \alpha R_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} = \theta_t + \alpha R_t \nabla_\theta log \pi_\theta (a|S_t)$$

چند نمونه از روش های بهینه سازی خط مشی به شرح زیر است. \\
روش های Actor-Critic که الگوریتم Gradient ascent را مستقیما برای بیشینه سازی 
$J(\pi\theta)$
به کار می برند.

روش  Proximal Policy Optimization که


\قسمت{روش های Actor-Critic}
یک نمونه از روش های بهینه سازی خط مشی، خانواده REINFORCE از الگوریتم های یادگیری تقویتی است.
\cite{williams1992simple}

الگوریتم استاندارد REINFORCE پارامترهای $\theta$ را در جهت 

$\nabla_\theta \  log  \ \pi (a_t|s_t:\theta)R_t$
بروزرسانی می کند که یک تخمین نااریب از 
$\nabla_\theta \mathbb{E}[R_t]$
است. می توان  با کم کردن یک تابع آموخته شده روی حالت ها 
$b_t(s_t)$
از 
$R_t$
، واریانس این تخمین را کاهش داد بطوریکه نااریب باقی بماند. به چنین تابعی \مهم{پایه} گفته می شود. نتیجتا گرادیان به شکل
$\nabla_\theta \ log \ \pi(a_t|s_t;\theta) (R_t - b_t(s_t))$
خواهد بود. \\

معمولا از یک تخمین آموخته شده از تابع ارزش به عنوان پایه استفاده می شود
$b_t(s_t) \approx V^\pi (s_t)$

که منجر به تخمینی با واریانس بسیار کوچک تر از گرادیان خط مشی می \nf شود در حالیکه تخمین، نااریب باقی می ماند و نتیجتا عملیات یادگیری با سرعت بیشتری انجام می شود.
این روش می تواند به شکل معماری \مهم{بازیگر-منتقد} \پاورقی{Actor-Critic} تعبیر شود که خط مشی $\pi$ \مهم{بازیگر} و پایه $b_t$ \مهم{منتقد} است. 


 اگر از تخمین یک تابع حالت-ارزش به عنوان پایه استفاده کنیم 
$b_t(s) = V_{\pi_{\theta_t}}(s)$
عبارت 
$R_t - b_t(s)$

می تواند به شکل تخمینی از \مهم{مزیت} \پاورقی{Advantage}َ عمل $a_t$ در حالت $s_t$ یا 
$A(a_t,s_t)=Q(a_t,s_t)-V(s_t)$
تعبیر شود. چراکه $R_t$ تخمینی از 
$Q^\pi (a_t, s_t)$

و $b_t$ تخمینی از 
$V^\pi (s_t)$

است. در این صورت به این روش $Advantage Actor-Critic$ یا $A2C$ گفته می شود.

\شروع{الگوریتم}{الگوریتم $Advantage \  Actor-Critic$}
\ورودی{یک پارامتریزه سازی مشتق پذیر از خط مشی 
	$\pi_\theta(a|s)$}
\ورودی{یک پارامتریزه سازی مشتق پذیر از تابع حالت-ارزش 
	$v_\omega(s)$}
\دستور{پارامترهای خط مشی 
	$\theta \in \mathbb{R}^{d'}$
	و تابع حالت-ارزش
	$\omega \in \mathbb{R}^d$
	را مقداردهی اولیه کن
}
\به‌ازای{تکرار کن}
%\اگر{$|E| > 0$}
%	\دستور{یک کاری انجام بده}
%\پایان‌اگر
\دستور{حالت اولیه $S$ را بساز}
\دستور{$1 \longrightarrow I$}
\تاوقتی{$S$ حالت نهایی نیست:}
\دستور{
	$A \sim \pi_\theta(.|S)$
}
\دستور{عمل $A$ را انجام بده و حالت $S'$ و پاداش $R$ را مشاهده کن}

\دستور{
	$ \longrightarrow \delta$
	$R + \gamma v_\omega(S') - v_\omega(S)$
}
\دستور{
	$ \longrightarrow \omega$
	$\omega + \alpha^\omega I \delta \nabla_\omega v_\omega(S)$
}
\دستور{
	$ \longrightarrow \theta$
	$\theta +  \alpha^\theta   I  \delta \nabla_\theta \ ln \ \pi_\theta(A| S)$
}

\دستور{
	$\gamma I \longrightarrow I$
}

\دستور{
	$S' \longrightarrow S$
}
\پایان‌تاوقتی
\پایان‌به‌ازای
\پایان{الگوریتم}
\cite{suttonbook}
 




\قسمت{روش TRPO}
\شروع{تعریف}
$\rho_\pi : S \longrightarrow \mathbb{R}$
تابع فرکانس تخفیف دار دیده شدن حالت ها باشد. یعنی
$\rho_\pi(s) = P(S_0=s) + \gamma P(S_1=s) + \gamma^2 P(S_2=s) + ...$

که دنباله
$S_0, S_1, S_2$
خط مشی $\pi$ را دنبال می کند.

\پایان{تعریف}

اگر $\pi$ و $\pi'$ دو خط مشی باشند و 
$J(\pi) = \mathbb{E}_\pi[R_0]$
 می توان نشان داد
\cite{schulman2015trust}

$$J(\pi')= J(\pi) + \sum_{s} \rho_{\pi'}(s) \sum_{a} \pi'(a|s) A_\pi(s,a)$$

که 
$A_\pi(s,a) = Q_\pi (s,a) - V_\pi(s)$
تابع مزیت عمل $a$ در حالت $s$ باشد.
وابستگی پیچیده 
$\rho_\pi'(s)$
در طرف راست تساوی به $\pi'$ بهینه سازی مستقیم را مشکل می کند.

برای حل این مشکل 
\cite{schulman2015trust}
مقیاس عملکرد دیگری $L_\pi(\pi')$ را معرفی می کند و نشان می دهد که اگر $\pi$ و $\pi'$ به اندازه کافی به یکدیگر نزدیک باشند، افزایش 
$L_\pi(\pi')$
 همواره با افزایش $J$ همراه خواهد بود.

$$L_\pi(\pi')= J(\pi) + \sum_{s} \rho_{\pi}(s) \sum_{a} \pi'(a|s) A_\pi(s,a)$$

توجه کنید که $L_\pi$ از تابع فرکانس $\rho_\pi$ به جای $
\rho_{\pi'}$
 استفاده می کند.
 
\شروع{قضیه}
فرض کنید 
$\alpha = D_{TV}^max(\pi_{old}, \pi_{new})$
باشد که 
$$D_{TV}^max (\pi, \pi') = \max_{s} D_{TV}(\pi(.|s) || \pi'(.|s)$$
و 
$D_{TV}(p || q)$
دیورژانس $total \ variation$ بین دو بردار $p$ و $q$ باشد

$$D_{TV}(p || q) = \dfrac{1}{2} \sum_{i} |p_i - q_i|$$

در این صورت

$$J(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \dfrac{4 \epsilon \gamma}{(1- \gamma)^2}$$
که 
$\epsilon = \max_{s,a} |A_\pi(s,a)|$
\پایان{قضیه}

با توجه به قضیه فوق و نامعادله 
$D_{TV}(p || q)^2 \le D_{KL}(p || q)$
\cite{schulman2015trust}
که 
$D_{KL} (p || q)$
برابر با دیورژانس KL دو بردار $p$ و $q$ است. می توان  نتیجه گرفت

$$J(\pi') \ge L_{\pi}(\pi') - C \ D_{KL}^max(\pi, \pi')$$

که

$$C = \dfrac{4 \epsilon \gamma}{(1-\gamma)^2}$$

رابطه بالا نشان می دهد که می توان یک دنباله صعودی از خط مشی ها داشت به طوری که
$J(\pi_0) \le J(\pi_1) \le J(\pi_2) \le ...$.
چرا که فرض کنید 
$M_i(\pi) = L_{\pi_i}(\pi) - C \ D_{KL}^max(\pi_i, \pi)$
در این صورت
$J(\pi_{i+1}) \ge M_i(\pi{i+1}) \\
J(\pi_i) = M_i(\pi_i)
$
بنابراین
$J(\pi_{i+1}) - J(\pi_i) \ge M_i(\pi_{i+1}) - M_i(\pi_i)$
بنابراین با بیشینه کردن $M_i$ در هر گام می توان اطمینان حاصل کرد که مقیاس عملکرد واقعی $J$ غیرنزولی خواهد بود.

\شروع{الگوریتم}{الگوریتم $Policy Iteration$ با مقیاس عملکرد $L_\pi$}

\دستور{خط مشی $\pi_0$
	را مقداردهی اولیه کن
}
\به‌ازای{برای 
	$i=0,1,...$
	 تکرار کن}
%\اگر{$|E| > 0$}
%	\دستور{یک کاری انجام بده}
%\پایان‌اگر
\دستور{همه ی مزیت های 
	$A_{\pi_i}(s,a)$
	را محاسبه کن
}
\دستور{
$arg \max_{\pi} [L_{\pi_i}(\pi) - C \ D_{KL}^max(\pi_i, \pi)] \longrightarrow \pi_{i+1} = $
که 
$C = (4 \epsilon \gamma) / (1-\gamma)^2$
و 
$L_{\pi_i}(\pi) = J(\pi_i) + \sum_{s} \rho_{\pi_i}(s) \sum_{a} \pi(a|s) A_{\pi_i} (s,a)$
}
\پایان‌به‌ازای
\پایان{الگوریتم}

اگر $\hat{A}_t$ تخمین مزیت 
$A_{\pi_t}(S_t, A_t)$
باشد که در گام $t$ محاسبه می شود، می توان نشان داد که در روش TRPO مقیاس عملکرد $L_\pi$ در هر گام به شکل
$$\mathbb{E}_t\left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} \hat{A}_t \right]$$
خواهد بود.

\قسمت{روش PPO}
در روش TRPO دیدیم که بیشینه سازی مقیاس عمکلرد $L_\pi$ ساده تر از مقیاس عملکرد $J$ است ولی در عوض الگوریتم Gradient ascent تنها مجاز به اعمال تغییرات کوچک در خط مشی است. یک راه دیگر برای کنترل تغییرات خط مشی استفاده از تابع clip است. در روش TRPO دیدیم که  تابع مقیاس عملکرد، در گام $t$ به شکل زیر است

$$L_{\pi_{old}}(\pi_{new}) = \mathbb{E_t}\left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} A_{\pi}(S_t, A_t)\right]$$

فرض کنید 
$r_t(\theta_{new})$
 نسبت احتمالات 
$r_t(\theta_{new}) = \dfrac{\pi_{\theta_{new}}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$

باشد. بنابراین
$$L_{\pi_{old}}(\pi_{new}) = \mathbb{E}_t\left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} A_{\pi}(S_t, A_t)\right] = \mathbb{E}_t\left[r_t(\theta_{new}) A_{\pi_{old}}(S_t,A_t)\right]$$

مقیاس عملکرد 
$L^{CLIP}(\theta)$
را به شکل زیر تعریف می کنیم

$$L^{CLIP}(\theta) = \mathbb{E}_t\left[min(r_t(\theta) \hat{A}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right]$$

که $\epsilon$ یک ابرپارامتر \پاورقی{Hyperparameter} مثلا 
$\epsilon=0.2$
است. اولین عبارت داخل min همان مقیاس عملکرد روش TRPO است. در عبارت دوم مقادیر بزرگتر از $1+\epsilon$ یا کوچکتر از 
$1-\epsilon$
در $r_t(\theta)$ به ترتیب به $1+\epsilon$ و $1-\epsilon$ تغییر پیدا کرده اند تا تغییرات بزرگ خط مشی را کنترل کنند. نهایتا عبارت کوچکتر از میان این دو انتخاب خواهد شد. بنابراین اگر مقیاس عملکرد clip نشده (عبارت اول) کوچکتر یا مساوی با حالت clip شده (عبارت دوم) باشد، $L^{CLIP}$ دقیقا همان مقیاس عملکرد $L_\pi$ خواهد بود. در غیر این صورت مقیاس عملکرد clip شده انتخاب می شود تا از تغییرات بزرگ خط مشی جلوگیری شود.


\قسمت{روش های Q-learning}

خانواده روش های  Q-learning تلاش می کنند مستقیما تابع ارزش عمل-حالت بهینه $Q^*(s,a)$ را تخمین بزنند. آنها به طور معمول از یک تابع هدف مبتنی بر معادله بلمن استفاده می کنند. این بهینه سازی تقریباً همیشه به صورت off-policy انجام می شود، به این معنی که هر به روزرسانی می تواند از داده های جمع آوری شده در هر نقطه استفاده کند ، بدون درنظرگرفتن نحوه انتخاب عامل برای کشف محیط در هنگام بدست آوردن داده ها. خط مشی مربوطه از طریق ارتباط بین 
$Q^*$
و
$\pi^*$
 بدست می آید: عامل بعد از یادگرفتن تابع $Q_\theta(s,a)$ به طوریکه 
 $Q_\theta(s,a) \approx Q^*(s,a)$
می تواند عمل بهینه در حالت $s$ را به به صورت زیر محاسبه کند

$$a(s) = arg \max_a Q_{\theta}(s,a)$$.

از جمله الگوریتم های Q-learning می توان به موارد زیر اشاره کرد


روش کلاسیک DQN که حوزه یادگیری تقویتی ژرف را عمیقا ارتقا بخشید

روش C51 که توزیعی روی عایدی را می آموزد که امیدریاضی آن $Q^*$ است

\قسمت{روش DQN}

معمولا برای تقریب زدن توابع ارزش در یادگیری تقویتی، از یک تابع خطی استفاده می شود.
اما گاهی اوقات از یک تقریب عملکرد غیر خطی به جای آن ، مانند یک شبکه عصبی استفاده می شود. شبکه های عصبی با عنوان شبکه Q\پاورقی{Q-Network} شناخته می شوند.
شبکه Q را می توان با کمینه ساختن دنباله ای از توابع هزینه به شکل 
$L_1(\theta_1), L_2(\theta_2), L_3(\theta_3), ... $

 آموزش داد؛ به طوری که

$$L_i(\theta_i)=\mathbb{E}\left[(y_i - Q(s,a;\theta_i))^2\right]$$

که 
$$y_i = \mathbb{E}[r + \gamma \max_{a'} Q(s',a'; \theta_{i-1})| s,a]$$.

با مشتق گرفتن از تابع هزینه نسبت به پارامترهای
$\theta_i$
 خواهیم داشت

$$\nabla_{\theta_i} L_i{\theta_i} = \mathbb{E}\left[ \left(r + \gamma \max{a'} Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i)\right) \nabla_{\theta_i} Q(s,a;\theta_i)\right]$$

%Rather than computing the full expectations in the above gradient, it is often computationally expedient to optimise the loss function by stochastic gradient descent. If the weights are updated after
%every time-step, and the expectations are replaced by single samples from the behaviour distribution
%ρ and the emulator E respectively, then we arrive at the familiar Q-learning algorithm [26].


به جای محاسبه امیدریاضی کامل در گرادیان فوق ، غالباً از نظر محاسباتی، بهینه سازی تابع هزینه با نزول گرادیان تصادفی  \پاورقی{stochastic gradient descend} راه حل بهتری است. اگر در هر مقطع زمانی، وزن ها به روزرسانی شود و امیدریاضی با یک نمونه از توزیع خط مشی رفتار \پاورقی{behavior policy} جایگزین شود، الگوریتم Q-learning به شکل زیر خواهد بود.

\شروع{الگوریتم}{الگوریتم Q-learning با Experience replay}

\دستور{حافظه 
	replay 
	$D$
را مقدار دهی اولیه کن}
\دستور{تابع عمل-ارزش Q را با وزن های تصادفی مقداردهی اولیه کن}
\به‌ازای{برای هر اپیزود 
$1...M$}
\دستور{دنباله 
	$d_1 = \{S_1\}$
	 و کدینگ 
	 $\phi_1 = \phi(d_1)$
	  را مقداردهی اولیه کن}
\به‌ازای{برای $t=1...T$}
\دستور{با احتمال $\epsilon$ یک عمل تصادفی $a_t$ را انتخاب کن، در غیر این صورت 
	$a_t = \max_{a} Q^*(\phi(d_t),a;\theta)$
	 را انتخاب کن}
 \دستور{عمل $a_t$ را انجام بده و حالت $S_{t+1}$ و پاداش $R_t$ را مشاهده کن}
 \دستور{قرار بده 
 	$d_{t+1} = d_t,a_t,S_{t+1}$
 	 و 
 	 $\phi_{t+1} = \phi(d_{t+1})$
  }
\دستور{تجربه 
	$(\phi_t, A_t, R_t, \phi_{t+1})$
	 را در $D$ ذخیره کن}
 \دستور{یک نمونه تصادفی از تجریه های  
 	$(\phi(j), A_j, R_j, \phi_{j+1})$
 	از انبار تجربیات $D$ انتخاب کن}
 \دستور{قرار بده 
 	\lr{
 	$y_j =$ 
 	\begin{cases}
 		$r_j$
 		 &
 		  $\phi_{j+1} \  terminal$ \\
 		$r_j$ & $otherwise$
 	\end{cases}}
}
\دستور{یک گام از نزول گرادیان را برای تابع هزینه 
	$(y_j - Q(\phi_j, a_j; \theta))^2$
	 انجام بده}
  \پایان‌به‌ازای
\پایان‌به‌ازای
\پایان{الگوریتم}

%Note that this algorithm is model-free: it solves the reinforcement learning task directly using samples from the emulator E, without explicitly constructing an estimate of E. It is also off-policy: it
%learns about the greedy strategy a = maxa Q(s, a; θ), while following a behaviour distribution that
%ensures adequate exploration of the state space. In practice, the behaviour distribution is often selected by an -greedy strategy that follows the greedy strategy with probability 1 −  and selects a
%random action with probability \epsilon

توجه داشته باشید که این یک الگوریتم بدون مدل است: این کار وظیفه یادگیری تقویتی را مستقیماً با استفاده از نمونه های شبیه ساز E بدون ساختن صریح تخمین E. حل می کند.
در مورد استراتژی حریص a = maxa Q (s ، a؛ θ) یاد می گیرد ، در حالی که توزیع رفتاری را دنبال می کند که
کاوش کافی در فضای دولت را تضمین می کند. در عمل ، توزیع رفتار اغلب توسط یک استراتژی Greed انتخاب می شود که استراتژی حریصانه را با احتمال 1 دنبال می کند - و یک
اقدام تصادفی با احتمال $\epsilon$

\قسمت{روش C51}



\قسمت{مقایسه روش بهینه سازی خط مشی و Q-learning}
نقطه قوت اصلی روش های بهینه سازی خط مشی، اصولی بودن آنهاست ، به این معنا که شما مستقیماً چیزی که می خواهید را بهینه سازی می کنید. در نتیجه این روش ها قابل اتکا و باثبات هستند. در مقابل ، روشهای 
\rl{Q-learning}
با یادگیری تابع Q، مقیاس عملکرد را به طور غیر مستقیم بهینه می کند. حالت های زیادی برای این نوع یادگیری وجود دارد که به شکست منتهی می شود، بنابراین این روش ها  ثبات کمتری دارند. 
\cite{suttonbook}
اما ، روش های 
\rl{Q-learning}
این مزیت را دارند که در هنگام کار ، به طور قابل ملاحظه ای کارآمد هستند ، زیرا آنها می توانند از داده ها به طور موثرتری نسبت به تکنیک های بهینه سازی خط مشی استفاده کنند.


تعامل بین بهینه سازی خط مشی و Q-learning. بهینه سازی خط مشی و Q-learning ناسازگار نیستند (و به نظر می رسد تحت برخی شرایط ، معادل آن باشد) ، و طیف وسیعی از الگوریتم ها وجود دارد که بین دو حد این طیف زندگی می کنند. الگوریتم هایی که در این طیف زندگی می کنند قادرند با دقت بین نقاط قوت و ضعف طرفین معامله کنند. مثالها شامل

DDPG
 ، الگوریتمی است که همزمان با استفاده از هر یک برای بهبود دیگری ، یک خط مشی قطعی و یک تابع Q را یاد می گیرد ،
و SAC ، نوعی که از خط مشی های تصادفی ، تنظیم آنتروپی  \پاورقی{entropy regularization}و چند ترفند دیگر برای تثبیت یادگیری و کسب امتیاز بالاتر از DDPG در معیارهای استاندارد استفاده می کند.


\قسمت{روش DDPG}
Here we combine the actor-critic approach with insights from the recent success of Deep Q Network
(DQN) (Mnih et al., 2013; 2015). Prior to DQN, it was generally believed that learning value
functions using large, non-linear function approximators was difficult and unstable. DQN is able
to learn value functions using such function approximators in a stable and robust way due to two
innovations: 1. the network is trained off-policy with samples from a replay buffer to minimize
correlations between samples; 2. the network is trained with a target Q network to give consistent
targets during temporal difference backups. In this work we make use of the same ideas, along with
batch normalization (Ioffe & Szegedy, 2015), a recent advance in deep learning.

\شروع{الگوریتم}{الگوریتم DDPG}
\دستور{پارامترهای 
	$\theta_\mu$
	 و 
	 $\theta_Q$
	 به ترتیب مربوط به بازیگر 
	 $\mu(s;\theta_\mu)$
	 و منتقد
	 $Q(s,a;\theta_Q)$
	 را مقداردهی اولیه کن.
}
\دستور{پارامترهای توابع هدف $\mu'$ و $Q'$ را با وزن های 
$\theta_{\mu'} \longleftrightarrow \theta_\mu$
و
$\theta_{Q'} \longleftarrow \theta_Q$

مقداردهی اولیه کن
}

\دستور{حافظه تکرارها $R$ را بساز}
\‌به‌ازای{برای هر اپیزود $1...M$}
\دستور{یک تابع نویز تصادفی $\mathbb{N}$} بساز
\دستور{حالت اولیه $S_1$ را مشاهده کن}
\‌به‌ازای{برای $t=1...T$}
\دستور{عمل 
	$a_t = \mu(s_t; \theta_\mu) + \mathbb{N}_t$
	 را بر اساس خط مشی فعلی و نویز اکتشاف، انتخاب کن و حالت بعدی $S_{t+1}$} و پاداش $R_t$ را مشاهده کن.
 \دستور{تجربه 
 	$(s_t, a_t, r_t, s_{t+1})$
 	 را در انبار تجربه $R$ ذخیره کن}
  \دستور{یک نمونه به اندازه $N$ از تجربه های $(s_i, a_i, r_i, s_{i+1})$ از انبار تجربه $R$ انتخاب کن }
  \دستور{وزن های منتقد $\theta_Q$ را با درنظر گرفتن تابع هزینه 
  	$L = \dfrac{1}{N} \sum_i(y_i - Q(S_i, A_i; \theta_Q))^2$
  	 به روزرسانی کن}
   \دستور{وزن های بازیگر $\theta_\mi$ را با استفاده از گرادیان خط مشی نمونه 
   	$$\nabla_{\theta_\mu} J \approx \dfrac{1}{N} \sum_{i} \nabla_a Q(s,a;\theta_Q) |_{s=s_i, a=\mu(s_i)} \nabla_{\thata_\mu} \mu(s;\theta_\mu) |_{S_i}$$
   	 به روزرسانی کن}
    
   \دستور{وزن های توابع هدف را به شکل 
   	$$\theta_{Q'} = \tau \theta_Q + (1-\tau) \theta_{Q'} \\
   	\theta_{\mu'} = \tau \theta_\mu + (1-\tau) \theta_{\mu'}
   	$$
   	 به روز رسانی کن}
\پایان‌به‌ازای
\پایان‌به‌ازای
\پایان{الگوریتم}

\قسمت{روش SAC}

\قسمت{روش \nf های مبتنی بر مدل}

Unlike model-free RL, there aren’t a small number of easy-to-define clusters of methods for model-based RL: there are many orthogonal ways of using models. We’ll give a few examples, but the list is far from exhaustive. In each case, the model may either be given or learned.

Background: Pure Planning. The most basic approach never explicitly represents the policy, and instead, uses pure planning techniques like model-predictive control (MPC) to select actions. In MPC, each time the agent observes the environment, it computes a plan which is optimal with respect to the model, where the plan describes all actions to take over some fixed window of time after the present. (Future rewards beyond the horizon may be considered by the planning algorithm through the use of a learned value function.) The agent then executes the first action of the plan, and immediately discards the rest of it. It computes a new plan each time it prepares to interact with the environment, to avoid using an action from a plan with a shorter-than-desired planning horizon.

The MBMF work explores MPC with learned environment models on some standard benchmark tasks for deep RL.
Expert Iteration. A straightforward follow-on to pure planning involves using and learning an explicit representation of the policy, \pi_{\theta}(a|s). The agent uses a planning algorithm (like Monte Carlo Tree Search) in the model, generating candidate actions for the plan by sampling from its current policy. The planning algorithm produces an action which is better than what the policy alone would have produced, hence it is an “expert” relative to the policy. The policy is afterwards updated to produce an action more like the planning algorithm’s output.

The ExIt algorithm uses this approach to train deep neural networks to play Hex.
AlphaZero is another example of this approach.
Data Augmentation for Model-Free Methods. Use a model-free RL algorithm to train a policy or Q-function, but either 1) augment real experiences with fictitious ones in updating the agent, or 2) use only fictitous experience for updating the agent.

See MBVE for an example of augmenting real experiences with fictitious ones.
See World Models for an example of using purely fictitious experience to train the agent, which they call “training in the dream.”
Embedding Planning Loops into Policies. Another approach embeds the planning procedure directly into a policy as a subroutine—so that complete plans become side information for the policy—while training the output of the policy with any standard model-free algorithm. The key concept is that in this framework, the policy can learn to choose how and when to use the plans. This makes model bias less of a problem, because if the model is bad for planning in some states, the policy can simply learn to ignore it.

See I2A for an example of agents being endowed with this style of imagination.