
\فصل{نتایج اخیر}
\قسمت{روش‌های مبتنی بر مدل و بدون مدل}
یکی از مهم‌ترین نقاط انشعاب در الگوریتم‌‌های RL این است که آیا عامل به یک مدل از محیط دسترسی دارد یا  توانایی آموختن مدلی از محیط را دارد؟ منظور از مدل محیط، تابعی است که انتقال و پاداش هر حالت-عمل را پیش‌بینی می‌کند.
نقطه قوت داشتن مدل این است که به عامل اجازه می‌دهد با برآورد قبلی، طیف وسیعی از گزینه‌های ممکن را پیش بینی کند و به صراحت در مورد گزینه‌های خود تصمیم بگیرد. سپس عامل می‌تواند نتایج حاصل از برنامه‌ریزی قبلی را در قالب یک خط‌مشی بیاموزد یک نمونه مشهور از این روش 
AlphaZero است. در عمل، چنانچه دستیابی به مدلی از محیط امکان‌پذیر و عملی باشد، معمولا از روش‌های مبتنی بر مدل استفاده می‌شود. زیرا می‌تواند باعث بهبود قابل توجه‌ای در کارایی نمونه نسبت به روش‌های بدون مدل شود. اصلی ترین نقطه ضعف این روش‌ها این است که معمولا یک مدل کامل از محیط  در دسترس عامل نیست و مدل کاملاً از طریق تجربه یاد گرفته می‌شود. الگوریتم‌هایی که از یک مدل استفاده می‌کنند، روش‌های مبتنی بر مدل و آن‌هایی که از چنین مدلی استفاده نمی‌کنند، بدون مدل نامیده می‌شوند. روش‌های بدون مدل از دستاوردهای بالقوه روش‌های مبتنی بر مدل چشم‌پوشی می‌کنند‌، اما پیاده‌سازی و تنظیم آنها آسان‌تر است. به همین خاطر، روش‌های بدون مدل از محبوبیت بیشتری برخوردار بوده و به طور گسترده‌تری توسعه و آزمایش شده‌اند.

\قسمت{روش‌های بدون مدل}
\subsection{روش‌های مبتنی بر ارزش}
%\قسمت{روش‌های مبتنی بر ارزش}


در روش‌های یادگیری تقویتی بدون مدل مبتنی بر ارزش
تابع ارزش عمل با استفاده از  یک تخمین‌گیر تابع
\LTRfootnote{Function approximator}
 ، مانند شبکه عصبی ، نشان داده می‌شود. فرض کنید
$Q(s,a;\theta)$
یک تابع ارزش عمل تقریبی با پارامتر 
$\theta$
باشد
الگوریتم‌های مختلفی برای بروزرسانی $\theta$ وجود دارد
الگوریتم $ Q-learning$ یکی از نمونه‌های چنین الگوریتمی‌است
که هدف آن تقریب مستقیم تابع ارزشِ عمل بهینه 
$Q^*(s,a) \approx Q(s,a: \theta)$
 است. در $ Q-learning$ یک مرحله‌ای، پارامترهای $\theta$ از تابع ارزشِ عمل با به حداقل رساندن تابع هزینه به شکل مرحله به مرحله آموخته می‌شوند، به شکلی که تابع هزینه $i$ام به شکل 
$$L_i(\theta_i) = \mathbb{E} {\left( r+\gamma \max_{a'} Q(s',a'; \theta_{i-1})- Q(s,a:\theta_i) \right)}^2$$
 تعریف می‌شود که 
 $s'$
 حالتی است که بعد از حالت $s$ دیده می‌شود.
 \subsubsection{روش‌های Q-learning}
%\قسمت{روش‌های Q-learning}

خانواده روش‌های  Q-learning تلاش می‌کنند مستقیما تابع ارزش عمل-حالت بهینه $Q^*(s,a)$ را تخمین بزنند. آن‌ها به طور معمول از یک تابع هدف مبتنی بر معادله بلمن استفاده می‌کنند. این بهینه‌سازی تقریباً همیشه به صورت off-policy انجام می‌شود، به این معنی که هر بروزرسانی می‌تواند از داده‌های جمع‌آوری شده در هر نقطه استفاده کند، بدون اینکه در نظر بگیرد نحوه انتخاب عامل برای کشف محیط در هنگام بدست آوردن داده‌ها چگونه بوده است. خط‌مشی مربوطه از طریق ارتباط بین 
$Q^*$
و
$\pi^*$
 بدست می‌آید: 
 
 عامل بعد از یادگرفتن تابع $Q_\theta(s,a)$ به طوری‌که  $Q_\theta(s,a) \approx Q^*(s,a)$می‌تواند عمل بهینه در حالت $s$ را به به صورت زیر محاسبه کند $$a(s) = arg \max_a Q_{\theta}(s,a).$$
از جمله الگوریتم‌های Q-learning می‌توان به موارد زیر اشاره کرد:
\begin{itemize}
\item روش کلاسیک DQN که حوزه یادگیری تقویتی ژرف\LTRfootnote{Deep reinforcement learning} را عمیقا ارتقا بخشید.
\item روش C51 که توزیعی روی عایدی را می‌آموزد که امیدریاضی آن $Q^*$ است.
\end{itemize}
\subsubsection{روش DQN}
%\قسمت*{روش DQN}
معمولا برای تقریب زدن توابع ارزش در یادگیری تقویتی، از یک تابع خطی استفاده می‌شود.
اما گاهی اوقات از یک تقریب عملکرد غیرخطی به جای آن، مانند یک شبکه عصبی استفاده می‌شود. شبکه‌های عصبی با عنوان شبکه Q\LTRfootnote{Q-Network} شناخته می‌شوند.
شبکه Q را می‌توان با کمینه ساختن دنباله‌ای از توابع هزینه به شکل 
$L_1(\theta_1), L_2(\theta_2), L_3(\theta_3), ... $ آموزش داد؛ به طوری‌که
$$L_i(\theta_i)=\mathbb{E}\left[(y_i - Q(s,a;\theta_i))^2\right]$$  و  $$y_i = \mathbb{E}[r + \gamma \max_{a'} Q(s',a'; \theta_{i-1})| s,a].$$ با مشتق گرفتن از تابع هزینه نسبت به پارامترهای $\theta_i$  خواهیم داشت: $$\nabla_{\theta_i} L_i{\theta_i} = \mathbb{E}\left[ \left(r + \gamma \max{a'} Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i)\right) \nabla_{\theta_i} Q(s,a;\theta_i)\right].$$
%Rather than computing the full expectations in the above gradient, it is often computationally expedient to optimise the loss function by stochastic gradient descent. If the weights are updated after
%every time-step, and the expectations are replaced by single samples from the behaviour distribution
%ρ and the emulator E respectively, then we arrive at the familiar Q-learning algorithm [26].
به جای محاسبه امیدریاضی کامل در گرادیان فوق، غالباً از نظر محاسباتی، بهینه‌سازی تابع هزینه با نزول گرادیان تصادفی  \LTRfootnote{stochastic gradient descend} راه‌حل بهتری است. اگر در هر مقطع زمانی، وزن‌ها بروزرسانی شود و امیدریاضی با یک نمونه از توزیع خط‌مشی رفتار \LTRfootnote{behavior policy} جایگزین شود، الگوریتم
\ref{alg:qlearn}
 Q-learning را نشان می‌دهد.
\شروع{الگوریتم}
{الگوریتم Q-learning با Experience replay}

\دستور{حافظه 
	replay 
	$D$
را مقدار دهی اولیه کن}
\دستور{تابع ارزشِ عمل Q را با وزن‌های تصادفی مقداردهی اولیه کن}
\به‌ازای{برای هر اپیزود 
$1...M$}
\دستور{دنباله 
	$d_1 = \{S_1\}$
	 و کدینگ 
	 $\phi_1 = \phi(d_1)$
	  را مقداردهی اولیه کن}
\به‌ازای{برای $t=1...T$}
\دستور{با احتمال $\epsilon$ یک عمل تصادفی $a_t$ را انتخاب کن، در غیر این صورت 
	$a_t = \max_{a} Q^*(\phi(d_t),a;\theta)$
	 را انتخاب کن}
 \دستور{عمل $a_t$ را انجام بده و حالت $S_{t+1}$ و پاداش $R_t$ را مشاهده کن}
 \دستور{قرار بده 
 	$d_{t+1} = d_t,a_t,S_{t+1}$
 	 و 
 	 $\phi_{t+1} = \phi(d_{t+1})$
  }
\دستور{تجربه 
	$(\phi_t, A_t, R_t, \phi_{t+1})$
	 را در $D$ ذخیره کن}
 \دستور{یک نمونه تصادفی از تجریه‌های  
 	$(\phi(j), A_j, R_j, \phi_{j+1})$
 	از انبار تجربیات $D$ انتخاب کن}
 \دستور{قرار بده 
 	\lr{
 	$y_j =$ 
 	\begin{cases}
 		$r_j$
 		 &
 		  $\phi_{j+1} \  terminal$ \\
 		$r_j$ & $otherwise$
 	\end{cases}}
}
\دستور{یک گام از نزول گرادیان را برای تابع هزینه 
	$(y_j - Q(\phi_j, a_j; \theta))^2$
	 انجام بده}
  \پایان‌به‌ازای
\پایان‌به‌ازای
\label{alg:qlearn}
\پایان{الگوریتم}
توجه داشته باشید که \ref{alg:qlearn} یک الگوریتم بدون مدل است: این کار وظیفه یادگیری تقویتی را مستقیماً با استفاده از نمونه‌های شبیه ساز E بدون ساختن صریح تخمین E حل می‌کند.
 استراتژی حریصانه،
 $a = \max_{a} Q(s, a; \theta)$
 ، در حالی که یاد می‌گیرد که
کاوش کافی در فضای حالت را تضمین کند.!!!!!!!!!!!!!! . توزیع رفتار اغلب توسط یک استراتژی Greed انتخاب می‌شود که استراتژی حریصانه را با احتمال 1 دنبال می‌کند و یک
اقدام تصادفی با احتمال $\epsilon$
!!!!!!!!!!!!!!
\subsubsection{روش C51 }
\begin{align}
Q^*(x,a)= \mathbb{E} R(x,a)+\gamma \mathbb{E}_P \max_{a' \in \EuScript{A}} Q^* (x',a').
\label{eq:Q^*}
\end{align}
در معادله \ref{eq:Q^*} $Q^*$ نقطه ثابت منحصر به فرد است، تابع ارزش بهینه، که مربوط است به مجموعه خط‌مشی‌های بهینه 
$\Pi^*$
($\pi*$ بهینه است اگر
$\mathbb{E}_{a\sim \pi^*} Q^*(x,a) = \max_{a}Q^*(x,a).$)

تابع ارزش و تابع پاداش چشمداشتی را بردارهایی در فضای 
$\mathbb{R}^{\EuScript{X \times A}}$ 
در نظر میگیریم. در این حالت عملگر بلمن، $\EuScript{T}^\pi$، و عملگر بهینه $\EuScript{T}$ به شکل زیر است:
\begin{align}
\EuScript{T}^\pi Q(x,a) := \mathbb{E} R(x,a) + \gamma \mathbb{E}_{P,\pi} Q(x',a') \\
\EuScript{T} Q(x,a) := \mathbb{E} R(x,a) + \gamma \mathbb{E}_P \max_{a' \in \EuScript{A}} Q(x',a')
\end{align}
عملگر بلمن و عملگر بهینه، به طور خاص، هردو نگاشتی انقباضی هستند و اعمال مکرر آنها روی یک $Q_0$ اولیه، به صورت نمایی به ترتیب به $Q^\pi$ و $Q^*$ همگرا می‌شود
\cite{bertsekas1996neuro}.
 امید ریاضی از معادله بلمن بیرون کشیده می‌شود و در عوض یک توزیع کامل از متغیر تصادفی 
$Z^\pi$ 
قرار داده می‌شود که نگاشتی از دوتایی حالت عمل به توزیعی از عایدی‌ها است. تابع 
$Z^\pi$ 
 توزیع ارزش می‌‌باشد.
\شروع{تعریف}
برای 
$F$
 و 
 $G$
که دو تابع توزیع تجمعی بر روی اعداد حقیقی هستند، تعریف می‌شود\\
\begin{align}
d_p (F,G):= \inf_{U,V} \parallel U-V \parallel_{p},
\label{eq:inf}
\end{align}
 
  که اینفیمم روی تمام جفت متغیرهای تصادفی 
 $(U,V)$
  که تابع توزیع تجمعی مربوط به آن‌ها به ترتیب 
 $F$
 و 
 $G$
 باشد، گرفته می‌شود.
 اینفیمم توسط تبدیل معکوس تابع توزیع تجمعی روی یک متغیر تصادفی 
 $\EuScript{u}$
 با توزیع یکنواخت در بازه 
 $[0,1]$
 حاصل می‌شود:
 $$d_p (F,G) = \parallel F^{-1}(\EuScript{U})-G^{-1} (\EuScript{U})\parallel_{p}$$
 برای 
$p < \infty$
می‌توان نوشت:
$$d_p (F,G) = \left( \int_{0}^{1} \mid F^{-1}(u)-G^{-1} (u)\mid^{p} du \right)^{1/p}$$
با داشتن دو متغیر تصادفی
$U,V$
با توابع توزیع تجمعی 
$F_U,F_V$
می‌توان نوشت
$$d_p(U,V) := d_p(F_U,F_V)$$
 می‌توان با قرار دادن خود متغیر‌های
   $U$ و $V$
     به جای توابع توزیع تجمعی‌شان در فرمول 
     \ref{eq:inf}
      به طور ساده تر نوشت:
$$d_p(U,V)= \inf_{U,V} \parallel U-V\parallel_p$$ 
\پایان{تعریف}

\شروع{تعریف}
\EuScript{Z} 
فضای توزیع ارزش با ممان‌های محدود تعریف می‌شود.
برای دو توزیع ارزش 
$Z_1$
و
$Z_2$
عضو 
\EuScript{Z} 
از فرم ماکسیمال متریک واسرشتاین
\LTRfootnote{Wasserstein}
استفاده میکنیم:
$$\bar{d_p}(Z_1,Z_2) := \sup_{x,a} d_p(Z_1(x,a),Z_2(x,a))$$
از 
$\bar{d_p}$
برای همگرایی عملگرهای توزیعی بلمن
\LTRfootnote{Distributional Bellman Operators}
 استفاده می‌شود.
\پایان{تعریف}
\paragraph{ارزیابی خط‌مشی}
در روند ارزیابی خط‌مشی، مطلوب است تابع ارزش،
$V^\pi$،
مرتبط با خط‌مشی، $\pi$،داده شده. 
تابع پاداش به عنوان یک بردار تصادفی، 
$R \in \EuScript{Z}$،
وعملگر انتقال تعریف می‌شود،
$P^\pi : \EuScript{Z} \rightarrow \EuScript{Z}$
\begin{align}
P^{\pi} Z(x,a) := Z(X',A') \\ \numberthis
X' \sim P(.|x,a), A'\sim \pi (.|X'), \nonumber
\end{align}
عملگرتوزیعی بلمن،
$\EuScript{T}^\pi : \EuScript{Z} \rightarrow \EuScript{Z}$،
 در نظر گرفته می‌شود:
\begin{align}
 \EuScript{T}^\pi Z(x,a) := R(x,a)+ \gamma P^\pi Z(x,a)
\end{align}  

می توان نشان داد که $T^\pi$ یک نگاشت انقباضی 
\LTRfootnote{Contraction Mapping}
روی
 $Z$
  است که نقطه ثابت آن توزیع ارزش
$Z^\pi$
    است.
اگر 
$\Pi^*$
مجموعه ای از خط‌مشی‌های بهینه باشد. 
تابع توزیع ارزش بهینه تعریف میشود:
\شروع{تعریف}
توزیع ارزش بهینه، تابع توزیعِ ارزش خط‌مشیِ بهینه است. مجموعه توزیع ارزش‌های بهینه تعریف می‌شود:
$$\EuScript{Z}^* := {Z^{\pi^{*}} : \pi^* \in \Pi^* }$$
تاکید می‌شود که همه توزیع ارزش‌ها با مقدار چشمداشتی $Q^*$ 
بهینه نیستند؛ تنها در صورتی بهینه هستند که مطابق با توزیع کامل عایدی بر‌اساس خط‌مشی بهینه باشند.
\پایان{تعریف} 
\شروع{تعریف}
خط‌مشی حریصانه
$\pi$
برای
$Z \in \EuScript{Z}$
مقدار چشمداشتی
$Z$
 را بیشینه می‌کند.
 مجموعه خط‌مشی‌های حریصانه برای 
 $Z$
 به این شکل تعریف می‌شوند:
 $$\EuScript{G}_Z := \lbrace \pi : \sum_{a} \pi (a|x) \ \mathbb{E} \ Z(x,a) = \max_{a' \in \EuScript{A}} \ \mathbb{E} \ Z(x,a') \rbrace.$$
\پایان{تعریف}

\شروع{تعریف}
یک توزیع ارزش بهینه ناپایدار،
$Z^{**}$،
توزیع ارزشی‌ است که مربوط به دنباله خط‌مشی‌های بهینه است. به مجموعه‌ای از توزیع ارزش‌های بهینه ناپایدار 
 $\EuScript{Z}^{**}$
 گفته می‌شود.
\پایان{تعریف}

\شروع{قضیه}
فرض می‌شود 
$\EuScript{X}$
قابل اندازه‌گیری و 
$\EuScript{A}$
محدود باشد.
بنابراین:
$$\lim_{k\to\infty} \inf_{Z^{**} \in \EuScript{Z}^{**}} d_p (Z_k (x,a), Z^{**}(x,a)) = 0  \ \forall x,a.$$
\پایان{قضیه}

\subsubsection{ روش SAC}
نشان داده می‌شود که می‌توان الگوریتم بازیگر-منتقد بیشینه آنتروپی مستقل از خط مشی، که بازیگر-منتقد نرم
\LTRfootnote{Soft Actor Critic(SAC)}
  نامیده‌ می‌شود، را طراحی کرد.
این الگوریتم به سادگی به کارهای بسیار پیچیده با ابعاد بالا بسط داده می‌شود، جایی که روش‌های مستقل از خط مشی‌ مانند
 $DDPG$
  معمولا برای رسیدن به نتایج مناسب، با چالش روبه رو هستند.
  
یادگیری خط مشی در فضاهای عمل پیوسته آدرس‌دهی می‌شود. یک فرایند تصمیم‌گیری مارکوف افق-بی‌نهایت
\LTRfootnote{Infinite Horizon Markov Decision}
 در نظر گرفته می‌شود که در آن فضای حالت
  $S$
   و فضای عمل
   $A$
    پیوسته هستند.
یک مقیاس عملکرد عمومی‌تر بیشینه آنتروپی بررسی خواهد شد.
به عنوان مثال ببینید: 
[!!!!!!!!!!!!!!!!!!!!!!!!!!!رفرنس مقاله]
که با تقویت مقیاس عملکرد آنتروپی چشم‌داشتی خط مشی به نفع خط مشی‌های تصادفی عمل می‌کند.
\begin{align}
J_{(\pi)}= \sum\limits_{t=0}^T \mathbb{E}_\pi \left[ r(s_t , a_t) + \alpha \EuScript{H} (\pi(.|s_t)) \right]
\label{eq:jpi}
\end{align}

می توان الگوریتم بازیگر-منتقد نرم مستقل از خط مشی را با شروع از نوعی دیگر از متد تکرار خط مشی بیشینه آنتروپی استخراج کرد.
با به دست آوردن تکرار خط مشی نرم شروع خواهد شد، که یک الگوریتم عمومی برای یادگیری خط مشی‌های بیشینه آنتروپی بهینه که در چارچوب بیشینه آنتروپی بین ارزیابی و بهبود خط مشی در حال تناوب است.
در گام ارزیابی خط مشی امید است مقدار خط مشی
$\pi$
 براساس مقیاس عملکرد بیشینه آنتروپی در معادله \ref{eq:jpi} محاسبه شود. برای یک خط‌مشی ثابت، مقدار $Q$  نرم می‌تواند به صورت گام به گام با شروع از هر تابع
  $Q:S \times R \rightarrow R$ 
 و اعمال مکرر یک عملگر پشتیبان بلمن اصلاح شده،
 $\EuScript{T}^\pi$
  که در تابع ارزش حالت نرم به صورت زیر داده شده است، محاسبه شود:
\begin{align}
\EuScript{T}^\pi Q(s_t,a_t) \triangleq r(s_t,a_t)+\gamma \mathbb{E} \left[ V(s_t+1) \right]
\label{eq:tpi}
\end{align}
\begin{align}
V(s_t)= \mathbb{E}_\pi \left[ Q(s_t,a_t)- \log \pi(a_t | s_t) \right]
\end{align}
می‌توان تابع ارزش نرم برای هر خط مشی$\pi$ را با اعمال مکرر، $T^\pi$ بدست آورد.
\begin{align}
\pi_{new} = arg \min_{\pi' \in \Pi} D_{KL}  \left( \pi'(.|s_t) \parallel \frac{\exp{(Q^{\pi_{old}} (s_t,.))}}{Z^{\pi_{old}} (s_t)}\right)
\label{eq:pinew}
\end{align}
تابع پارش 
$Z^\pi (s_t)$
توزیع را یکنواخت می‌کند و از آنجائی که در حالت عمومی غیرقابل حل است، به گرادیان نسبت به خط مشی جدید کمکی نمی‌کند و به همین دلیل می‌تواند نادیده گرفته شود. برای این تابع می‌توان نشان داد که خط مشی جدید نسبت به خط مشی قدیمی با توجه به مقیاس عملکرد معادله‌ی \ref{eq:jpi} مقدار بیشتری دارد. 
الگوریت تکرار خط مشی نرم کامل، بین ارزیابی خط مشی نرم و بهبود خط‌مشی نرم در تناوب است و در میان خط‌مشی‌های عضو
$\mathbb{\Pi}$
، احتمالا به خط‌مشی بیشینه آنتروپی بهینه همگرا می‌شود.
 \paragraph{بازیگر-منتقد نرم}
به دست آوردن یک تقریب تجربی برای تکرار خط‌مشی نرم در دامنه‌های پیوسته‌ی بزرگ لازم الاجراست.  در انتها، تقریب‌گرهای تابع برای تابع
$ Q$
 و خط‌مشی استفاده خواهند شد به جای اجرای بهبود و ارزیابی برای همگرایی، بین بهینه‌سازی هر دو شبکه با نزول گرادیان تصادفی در تناوب است. تابع مقدار حالت پارامتری شده
 $V_\psi (s_t)$
 ، تابع$Q$ نرم 
 $Q_\theta (s_t,a_t)$
  و خط‌مشی قابل حل 
  $\pi_\phi (a_t | s_t)$
  بررسی خواهند شد. پارامترهای این شبکه‌ها 
  $\psi, \theta, \phi$
   هستند. به عنوان مثال، توابع مقدار می‌توانند به عنوان شبکه‌های عصبی گویا و خط‌مشی به عنوان یک گوسی با میانگین و همگرایی گرفته شده از شبکه‌های عصبی، مدل شوند.
تابع مقدار نرم برای کمینه کردن مربع خطای باقیمانده‌ها آموزش دیده شده‌است.
که 
$\EuScript{D}$
 توزیع حالت‌ها و عمل‌های نمونه‌گیری شده قبلی یا انبار تجربه است. .
\begin{align}
\hat{\nabla}_\psi J_V (\psi) = \nabla_\psi V_\psi (s_t) (V_\psi (s_t)-Q_\theta(s_t,a_t)+\log \pi_\phi (a_t |s_t)) ,
\end{align}
که عمل‌ها به جای انبار تجربه، براساس خط‌مشی فعلی نمونه گیری شده‌اند. پارامترهای تابع $Q$ نرم می‌تواند برای کمینه کردن باقیمانده‌ی بل من آموزش دیده‌شود.
\begin{align}
J_Q(\theta) = \mathbb{E}_{\EuScript{D}} \left[ \frac{1}{2} \left( Q_\theta (s_t,a_t)- \hat{Q} (s_t,a_t)\right)^2 \right] ,
\end{align}
با 
\begin{align*}
\hat{Q} (s_t,a_t) = r(s_t,a_t) + \gamma \mathbb{E} [V_\Psi (s_t+1)] ,
\end{align*}
که می‌تواند مجددا با گرادیان‌های تصادفی بهینه شود.
\begin{align}
\hat{\nabla_{\theta}} J_Q (\theta) = \nabla_\theta Q_\theta (a_t,s_t) \left( Q_\theta (s_t,a_t)- r(s_t,a_t) - \gamma V_\Psi (s_t+1) \right). 
\end{align}
در آخر، پارامترهای خط‌مشی می‌توانند با کمینه کردن مستقیم همگرایی-KL چشمداشتی در معادله \ref{eq:pinew} آموخته شوند:
\begin{align}
J_\pi(\phi) = \mathbb{E}\left[ D_{KL}  \left( \pi_\phi(.|s_t) \parallel \frac{\exp{(Q_{\theta} (s_t,.))}}{Z_{\theta} (s_t)}\right) \right]
\end{align}
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
\شروع{الگوریتم}{الگوریتم $Policy Iteration$ با مقیاس عملکرد $L_\pi$}

\دستور{خط‌مشی $\pi_0$
	را مقداردهی اولیه کن
}
$a_t \approx \pi_\phi(a_t|s_t)$ \\
$s_{t+1} \approx p(s_{t+1}|s_t,a_t)$\\
$\EuScript{D}\leftarrow \EuScript{D} \bigcup {(s_t,a_t,r(s_t,a_t),s_{t+1})}$ \\
$\psi \leftarrow \psi - \landa_V \hat{\nabla}_\spi J_V (\psi)$ \\
$\theta_i \leftarrow \theta_i - \landa_Q \hat{\nabla}_{\theta_i} J_Q(\theta_i) for i \in {1,2}$\\
$\phi \leftarrow \phi - \landa_\pi \hat{\nabla}_\phi J_\pi (Q)$ \\
$\bar{\psi} \leftarrow \tau \psi + (1-\tau) \psi$\\
\پایان{الگوریتم}
که (۶) یک بردار نویز ورودی است که از یک توزیع ثابت مانند یک گوسی کروی نمونه‌گیری شده‌است.
\subsubsection{روش DDPG}
%An obvious approach to adapting deep reinforcement learning methods such as DQN to continuous domains is to to simply discretize the action space. However, this has many limitations, most notably the curse of dimensionality: the number of actions increases exponentially with the number of degrees of freedom.
گسسته سازی فضای عمل یک روش برای سازگار‌کردن و انطباق روش‌های یادگیری تقویتی عمیق نظیر 
\lr{DQN}
با دامنه‌های پیوسته می‌یاشد. با این حال، این روش محدودیت‌های زیادی دارد، مخصوصا مشکل نفرین ابعاد.  نفرین ابعاد بیان‌گر این است که تعداد عمل‌ها به صورت نمایی با تعداد درجات آزادی افزایش پیدا می‌کند. 

%we present a model-free, off-policy actor-critic algorithm using deep function approximators that can learn policies in high-dimensional, continuous action spaces.
در روشی جدید یک الگوریتم بازیگر-منتقد مستقل از خط‌مشی، مستقل از مدل
\LTRfootnote{A model-free, off-policy actor-critic algorithm}
با استفاده از تخمین‌گر تابع عمیق ارائه می‌دهند که می‌تواند خط‌مشی‌ها را در فضاهای عمل پیوسته با ابعاد بالا یاد بگیرد.

%our work is based on the deterministic policy gradient (DPG) algorithm which we call Deep DPG (DDPG)
این روش بر اساس الگوریتم گرادیان خط‌مشی معین
\LTRfootnote{Deterministic policy gradient (DPG)}
است که آن را
گرادیان خط‌مشی معین عمیق
\LTRfootnote{Deep DPG (DDPG)}
می نامیم. 
%Here we combine the actor-critic approach with insights from Deep Q Network (DQN)
در ان روش، رویکرد بازیگر-منتقد را با بینش شبکه \lr{Q} عمیق
\LTRfootnote{Deep Q Network (DQN)} 
ترکیب می‌کنیم.


%the DPG algorithm maintains a parametrized actor function which specifies the current policy by deterministically mapping states to a specific action.
الگوریتم 
\lr{DPG}
تابع عمل پارامتری
$
\mu (s|\theta^\mu)	
$
را نگهداری می‌کند که با نگاشت معین حالت‌ها به یک عمل خاص، سیاست فعلی را مشخص می‌کند.

%the critic is learned using the Bellman equation as in Q-learning.
همانند روش یادگیری 
\lr{Q}
در اینجا نیز منتقد
$Q(s,a)$
با استفاده از معادله بلمن آموخته می‌شود.
%the actor is updated by following applying the chain rule to the expected return from the start distribution with respect to the actor parameters 
بازیگر با پیروی از اعمال قاعده زنجیره‌ای روی عایدی چشم‌داشتی از توزیع شروع
$J$
نسبت به پارامترهای بازیگر به روز می‌شود.
\begin{align}
	\nabla_{\theta \mu} J = & \mathbb{E}_{s_t \sim \rho ^{ \beta}} [ \nabla_{\theta \mu} Q(s,a|\theta^Q)|_{s=s_t,a=\mu (s_t|\theta^{\mu})}  ] \\ \nonumber
	= & \mathbb{E}_{s_t \sim \rho ^{ \beta}} [ \nabla_{\theta \mu} Q(s,a|\theta^Q)|_{s=s_t, a=\mu (s_t)} \nabla_{\theta_{\mu}} \mu (s|\theta^{\mu})|_{s=s_t} ]
\end{align}

در این روش نیز، مانند روش 
\lr{DQN}
از انبار تجربه استفاده می شود. 
در هر مرحله، بازیگر و منتقد با نمونه‌برداری یکنواخت از انبار به روز رسانی می‌شوند. از آنجا که 
\lr{DDPG}
الگوریتمی مستقل از خط‌مشی می‌باشد، انبار تجربه می تواند بزرگ باشد، که به الگوریتم اجازه می‌دهد تا از یادگیری مجموعه‌ای از انتقال‌های ناهمبسته بهره‌مند شود.
چالش اصلی یادگیری در فضاهای عمل پیوسته، اکتشاف است. یک مزیت  الگوریتم‌های مستقل از خط‌مشی مانند 
\lr{DDPG}
این است که می توان به مسئله کاوش،  مستقل از الگوریتم یادگیری پرداخت.
می‌توان یک خط‌مشی اکتشاف 
$\mu'$
را با اضافه کردن نویز (نمونه‌گیری شده از فرآیند $\EuScript{N}$) به خط‌مشی بازیگر ساخت:
\begin{equation}
\mu'(s_t) = \mu(s_t|\theta_t^mu) + \EuScript{N}
\end{equation}
می‌توان $\EuScript{N}$ 
را متناسب با محیط انتخاب کرد.
%\قسمت{روش DDPG}
%Here we combine the actor-critic approach with insights from the recent success of Deep Q Network
%(DQN) (Mnih et al., 2013; 2015). Prior to DQN, it was generally believed that learning value
%functions using large, non-linear function approximators was difficult and unstable. DQN is able
%to learn value functions using such function approximators in a stable and robust way due to two
%innovations: 1. the network is trained off-policy with samples from a replay buffer to minimize
%correlations between samples; 2. the network is trained with a target Q network to give consistent
%targets during temporal difference backups. In this work we make use of the same ideas, along with
%batch normalization (Ioffe & Szegedy, 2015), a recent advance in deep learning.

\شروع{الگوریتم}{الگوریتم DDPG}
\دستور{پارامترهای 
	$\theta_\mu$
	 و 
	 $\theta_Q$
	 به ترتیب مربوط به بازیگر 
	 $\mu(s;\theta_\mu)$
	 و منتقد
	 $Q(s,a;\theta_Q)$
	 را مقداردهی اولیه کن.
}
\دستور{پارامترهای توابع هدف $\mu'$ و $Q'$ را با وزن‌های 
$\theta_{\mu'} \longleftrightarrow \theta_\mu$
و
$\theta_{Q'} \longleftarrow \theta_Q$

مقداردهی اولیه کن
}

\دستور{حافظه تکرارها $R$ را بساز}
\‌به‌ازای{برای هر اپیزود $1...M$}
\دستور{یک تابع نویز تصادفی $\mathbb{N}$} بساز
\دستور{حالت اولیه $S_1$ را مشاهده کن}
\‌به‌ازای{برای $t=1...T$}
\دستور{عمل 
	$a_t = \mu(s_t; \theta_\mu) + \mathbb{N}_t$
	 را بر اساس خط‌مشی فعلی و نویز اکتشاف، انتخاب کن و حالت بعدی $S_{t+1}$} و پاداش $R_t$ را مشاهده کن.
 \دستور{تجربه 
 	$(s_t, a_t, r_t, s_{t+1})$
 	 را در انبار تجربه $R$ ذخیره کن}
  \دستور{یک نمونه به اندازه $N$ از تجربه‌های $(s_i, a_i, r_i, s_{i+1})$ از انبار تجربه $R$ انتخاب کن }
  \دستور{وزن‌های منتقد $\theta_Q$ را با درنظر گرفتن تابع هزینه 
  	$L = \dfrac{1}{N} \sum_i(y_i - Q(S_i, A_i; \theta_Q))^2$
  	 بروزرسانی کن}
   \دستور{وزن‌های بازیگر $\theta_\mi$ را با استفاده از گرادیان خط‌مشی نمونه 
   	$$\nabla_{\theta_\mu} J \approx \dfrac{1}{N} \sum_{i} \nabla_a Q(s,a;\theta_Q) |_{s=s_i, a=\mu(s_i)} \nabla_{\thata_\mu} \mu(s;\theta_\mu) |_{S_i}$$
   	 به روزرسانی کن}
    
   \دستور{وزن‌های توابع هدف را به شکل 
   	$$\theta_{Q'} = \tau \theta_Q + (1-\tau) \theta_{Q'} \\
   	\theta_{\mu'} = \tau \theta_\mu + (1-\tau) \theta_{\mu'}
   	$$
   	 بروزرسانی کن}
\پایان‌به‌ازای
\پایان‌به‌ازای
\پایان{الگوریتم}

%\قسمت{روش SAC}

\subsection{روش SAC}

\subsection{روش‌های مبتنی بر خط‌مشی}

%\قسمت{روش‌های مبتنی بر خط‌مشی}

در این قسمت روش‌هایی را در نظر می‌گیریم که برای دستیابی به خط‌مشی بهینه، به جای استفاده از تابع ارزشِ عمل یا ارزشِ حالت، یک خط‌مشی پارامتری‌شده 
\LTRfootnote{Parameterized} 
 را می‌آموزد. با این حال ممکن است برای یادگیری پارامترهای خط‌مشی از یک تابع ارزش استفاده شود، اما تابع ارزش برای انتخاب عمل مورد نیاز نیست. ما از نماد
$\theta \in \mathbb{R}^{d'}$ برای بردار پارامتر خط‌مشی استفاده می‌کنیم.
بر خلاف روش‌های مبتنی بر ارزش، روش‌های مبتنی بر خط‌مشی مستقیماً تابع خط‌مشی 
$\pi_\theta(a|s)$
 را تخمین می‌زنند و پارامترهای $\theta$ را با استفاده از \مهم{صعود گرادیان}
 \LTRfootnote{Gradian ascent}
  روی یک مقیاس عملکرد \LTRfootnote{Performance Measure} 
 $J(\pi_\theta)$ یا به طور مستقیم و یا با بیشینه‌سازی تخمین‌های محلی از  $J(\pi_\theta)$ بروزرسانی می‌کنند.
  این روش تقریبا همیشه به صورت on-policy  عمل می‌کند. 

همانطور که در ادامه خواهیم دید، می‌توان از توابع مختلفی برای مقیاس عملکرد $J$ استفاده نمود. یک انتخاب بدیهی 
$J(\pi_\theta) = \mathbb{E}[R_t]$
است. هدف این روش‌ها بیشینه کردن تابع $J$ است. در عبارت 
$$\theta_{t+1}=\theta_t + \alpha \hat{\nabla J(\theta_t)}$$
 
$\hat{\nabla J(\theta_t)}$،
تخمینی احتمالاتی است که امیدریاضی آن گرادیان  $J$ را نسبت به پ $\theta_t$ تخمین می‌زند.

به روش‌هایی که چنین الگویی را برای محاسبه خط‌مشی بهینه دنبال می‌کنند، روش‌های \textit{گرادیان خط‌مشی }\LTRfootnote{Policy Gradient} می‌گوییم. دسته‌ای از روش‌های گرادیان خط‌مشی وجود دارند که تلاش می‌کنند تخمینی از تابع ارزش را نیز محاسبه کنند. به چنین روش‌هایی، \textit{\مهم{بازیگر-منتقد}} 
\LTRfootnote{Actor-Critic}
 گفته می‌شود که \textit{\مهم{بازیگر}} 
 \LTRfootnote{Actor}
  اشاره به خط‌مشی آموخته شده و \textit{\مهم{منتقد}} 
\LTRfootnote{Critic}  
   اشاره به تابع ارزش آموخته شده (معمولا یک تابع ارزشِ حالت) دارد.

\شروع{قضیه}[گرادیان خط‌مشی]
$$\nabla J(\pi_\theta) \propto \sum_{s} \mu(s) \sum_{a} q_\pi(s,a) \nabla_\theta \pi_\theta(a|s)$$
که $\mu$ یک توزیع احتمال روی $S$ است که متناسب با تعداد دفعاتی است که حالت $s$ با دنبال کردن خط‌مشی 
$\pi_\theta$
 تکرار می‌شود.
\پایان{قضیه}


می‌توان نشان داد 


$$\nabla J(\pi_\theta) = \mathbb{E}_\pi \left[ R_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} \right]$$
بنابراین در هر گام
$\left[ R_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} \right]$
یک تخمین‌گر نااریب از $\nabla J(\pi_\theta)$ خواهد بود\cite{suttonbook}. پس می‌توان در هر گام $\theta$ را به شکل زیر بروزرسانی کرد
$$\theta_{t+1} = \theta_t + \alpha R_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} = \theta_t + \alpha R_t \nabla_\theta log \pi_\theta (a|S_t)$$

چند نمونه از روش‌های بهینه‌سازی خط‌مشی به شرح زیر است. \\
روش‌های بازیگر منتقد
 \LTRfootnote{Actor-Critic}
  که الگوریتم 
 گرادیان افزایشی
   را مستقیما برای بیشینه سازی 
$J(\pi\theta)$
به کار می‌برند.

روش  
\lr{Proximal Policy Optimization}
 که!!!!!!!!!!!

\subsubsection{روش‌های بازیگر-منتقد}
%\قسمت{روش‌های Actor-Critic}
یک نمونه از روش‌های بهینه‌سازی خط‌مشی، خانواده
 \lr{REINFORCE}
 از الگوریتم‌های یادگیری تقویتی است
\cite{williams1992simple}.
الگوریتم استاندارد \lr{REINFORCE} پارامترهای $\theta$ را در جهت 
 $\nabla_\theta \  log  \ \pi (a_t|s_t:\theta)R_t$ بروزرسانی می‌کند که یک تخمین نااریب از
   $\nabla_\theta \mathbb{E}[R_t]$
است. می‌توان  با تفریق یک تابع آموخته شده روی حالت‌ها، 
$b_t(s_t)$
از 
$R_t$، واریانس این تخمین را کاهش داد بطوری‌که نااریب باقی بماند. به چنین تابعی \مهم{پایه} گفته می‌شود. در نتیجه گرادیان به شکل
$\nabla_\theta \ log \ \pi(a_t|s_t;\theta) (R_t - b_t(s_t))$
خواهد بود. معمولا از تخمینی آموخته‌شده از تابع ارزش به عنوان پایه استفاده می‌شود،
$b_t(s_t) \approx v_\pi (s_t)$،
 که منجر به تخمینی با واریانس بسیار کوچک‌تر از گرادیان خط‌مشی می‌\nf شود؛ در حالیکه تخمین نااریب باقی می‌ماند و درنتیجه عملیات یادگیری با سرعت بیشتری انجام می‌شود.
این روش می‌تواند به شکل معماری \مهم{بازیگر-منتقد}  تعبیر شود که خط‌مشی $\pi$ \مهم{بازیگر} و پایه $b_t$ \مهم{منتقد} است. اگر از تخمین یک تابع ارزشِ حالت به عنوان پایه استفاده کنیم 
$b_t(s) = v_{\pi_{\theta_t}}(s)$ عبارت $R_t - b_t(s)$ می‌تواند به شکل تخمینی از \مهم{مزیت} \LTRfootnote{Advantage} عمل $a_t$ در حالت $s_t$ یا 
$A(a_t,s_t)=Q(a_t,s_t)-v(s_t)$ تعبیر شود. چراکه $R_t$ تخمینی از  $Q_\pi (a_t, s_t)$ و $b_t$ تخمینی از  $v_\pi (s_t)$ است. در این صورت به این روش 
\lr{Advantage Actor-Critic}
 یا A2C گفته می‌شود.
\شروع{الگوریتم}{الگوریتم $Advantage Actor-Critic$ }
\ورودی{یک پارامتری‌شده مشتق پذیر از خط‌مشی 
	$\pi_\theta(a|s)$}
\ورودی{یک پارامتری‌شده مشتق‌پذیر از تابع ارزشِ حالت 
	$v_\omega(s)$}
\دستور{پارامترهای خط‌مشی 
	$\theta \in \mathbb{R}^{d'}$
	و تابع ارزشِ حالت
	$\omega \in \mathbb{R}^d$
	را مقداردهی اولیه کن
}
\به‌ازای{تکرار کن}
%\اگر{$|E| > 0$}
%	\دستور{یک کاری انجام بده}
%\پایان‌اگر
\دستور{حالت اولیه $S$ را بساز}
\دستور{$1 \longrightarrow I$}
\تاوقتی{$S$ حالت نهایی نیست}
\دستور{
	$A \sim \pi_\theta(.|S)$
}
\دستور{عمل $A$ را انجام بده و حالت $S'$ و پاداش $R$ را مشاهده کن}

\دستور{
	$ \longrightarrow \delta$
	$R + \gamma v_\omega(S') - v_\omega(S)$
}
\دستور{
	$ \longrightarrow \omega$
	$\omega + \alpha^\omega I \delta \nabla_\omega v_\omega(S)$
}
\دستور{
	$ \longrightarrow \theta$
	$\theta +  \alpha^\theta   I  \delta \nabla_\theta \ ln \ \pi_\theta(A| S)$
}

\دستور{
	$\gamma I \longrightarrow I$
}

\دستور{
	$S' \longrightarrow S$
}
\پایان‌تاوقتی
\پایان‌به‌ازای
%\caption{ برگرفته شده از !!!!!!!!!!!!}
\پایان{الگوریتم}
\subsubsection{روش TRPO}
%\قسمت{روش TRPO}
\شروع{تعریف}
اگر
$\rho_\pi : S \longrightarrow \mathbb{R}$
تابع فرکانس تخفیف‌دار دیده شدن حالت‌ها باشد. یعنی $$\rho_\pi(s) = P(S_0=s) + \gamma P(S_1=s) + \gamma^2 P(S_2=s) + ...$$ \\که دنباله $S_0, S_1, S_2$ خط‌مشی $\pi$ را دنبال می‌کند.
\پایان{تعریف}
اگر $\pi$ و $\pi'$ دو خط‌مشی باشند و  $J(\pi) = \mathbb{E}_\pi[R_0]$  می‌توان نشان داد 
$$J(\pi')= J(\pi) + \sum_{s} \rho_{\pi'}(s) \sum_{a} \pi'(a|s) A_\pi(s,a)$$ که  $A_\pi(s,a) = Q_\pi (s,a) - V_\pi(s)$ تابع مزیت عمل $a$ در حالت $s$ باشد.
وابستگی پیچیده  $\rho_\pi'(s)$ در طرف راست تساوی به $\pi'$ بهینه‌سازی مستقیم را مشکل می‌کند.
برای حل این مشکل شولمن و همکارانش 
مقیاس عملکرد دیگری، $L_\pi(\pi')$، را معرفی می‌کند و نشان می‌دهد که اگر $\pi$ و $\pi'$ به اندازه کافی به یکدیگر نزدیک باشند، افزایش  $L_\pi(\pi')$  همواره با افزایش $J$ همراه خواهد‌بود \cite{schulman2015trust}.
\begin{align}
L_\pi(\pi')= J(\pi) + \sum_{s} \rho_{\pi}(s) \sum_{a} \pi'(a|s) A_\pi(s,a)
\label{l_pi}
\end{align}


در عبارت 
\ref{l_pi}
 $L_\pi$ از تابع فرکانس $\rho_\pi$ به جای $
\rho_{\pi'}$
 استفاده می‌کند.
 
\شروع{قضیه}
فرض کنید 
$\alpha = D_{TV}^max(\pi_{old}, \pi_{new})$
باشد که 
$$D_{TV}^max (\pi, \pi') = \max_{s} D_{TV}(\pi(.|s) || \pi'(.|s)$$
و 
$D_{TV}(p || q)$
دیورژانس 
\lr{total variation}
 بین دو بردار $p$ و $q$ باشد
$$D_{TV}(p || q) = \dfrac{1}{2} \sum_{i} |p_i - q_i|$$ در این صورت
$$J(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \dfrac{4 \epsilon \gamma}{(1- \gamma)^2}$$ که $\epsilon = \max_{s,a} |A_\pi(s,a)|$.
\label{q:ghas}
\پایان{قضیه}

با توجه به قضیه \ref{q:ghas} و نامعادله $D_{TV}(p || q)^2 \le D_{KL}(p || q)$  که  $D_{KL} (p || q)$ برابر با دیورژانس KL دو بردار $p$ و $q$ است\cite{schulman2015trust}. می‌توان  نتیجه گرفت
\begin{align*}
J(\pi') \ge L_{\pi}(\pi') - C \ D_{KL}^max(\pi, \pi')
\end{align*}
که
\begin{align}
C = \dfrac{4 \epsilon \gamma}{(1-\gamma)^2}
\label{eq:cc}
\end{align}
رابطه \ref{eq:cc} نشان می‌دهد که می‌توان یک دنباله صعودی از خط‌مشی‌ها داشت به طوری که
$$J(\pi_0) \le J(\pi_1) \le J(\pi_2) \le ... $$
چرا که فرض کنید
$M_i(\pi) = L_{\pi_i}(\pi) - C \ D_{KL}^max(\pi_i, \pi)$
در این صورت $$J(\pi_{i+1}) \ge M_i(\pi{i+1}) $$ 
$$J(\pi_i) = M_i(\pi_i)$$
بنابراین
$$J(\pi_{i+1}) - J(\pi_i) \ge M_i(\pi_{i+1}) - M_i(\pi_i)$$
می‌توان نتیجه گرفت با بیشینه کردن $M_i$ در هر گام می‌توان اطمینان حاصل کرد که مقیاس عملکرد واقعی $J$ غیرنزولی خواهد بود.

\شروع{الگوریتم}{الگوریتم $Policy Iteration$ با مقیاس عملکرد $L_\pi$}

\دستور{خط‌مشی $\pi_0$
	را مقداردهی اولیه کن
}
\به‌ازای{برای 
	$i=0,1,...$
	 تکرار کن}
%\اگر{$|E| > 0$}
%	\دستور{یک کاری انجام بده}
%\پایان‌اگر
\دستور{همه مزیت‌های 
	$A_{\pi_i}(s,a)$
	را محاسبه کن
}
\دستور{
$arg \max_{\pi} [L_{\pi_i}(\pi) - C \ D_{KL}^max(\pi_i, \pi)] \longrightarrow \pi_{i+1} = $
که 
$$C = (4 \epsilon \gamma) / (1-\gamma)^2$$ و  $$L_{\pi_i}(\pi) = J(\pi_i) + \sum_{s} \rho_{\pi_i}(s) \sum_{a} \pi(a|s) A_{\pi_i} (s,a)$$
}
\پایان‌به‌ازای
\پایان{الگوریتم}

اگر $\hat{A}_t$ تخمین مزیت 
$A_{\pi_t}(S_t, A_t)$
باشد که در گام $t$ محاسبه می‌شود، می‌توان نشان داد که در روش
 \lr{TRPO}
  مقیاس عملکرد $L_\pi$ در هر گام به شکل؛
$$\mathbb{E}_t\left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} \hat{A}_t \right]$$خواهد بود.
\subsubsection{روش PPO}
%\قسمت{روش PPO}
در روش \lr{TRPO} دیدیم که بیشینه‌سازی مقیاس عمکلرد $L_\pi$ ساده‌تر از مقیاس عملکرد $J$ است ولی در عوض الگوریتم صعود گرادیان تنها مجاز به اعمال تغییرات کوچک در خط‌مشی است. یک راه دیگر برای کنترل تغییرات خط‌مشی استفاده از تابع CLIP است. در روش
 \lr{TRPO}
  دیدیم که  تابع مقیاس عملکرد، در گام $t$ به شکل زیر است؛
$$L_{\pi_{old}}(\pi_{new}) = \mathbb{E_t}\left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} A_{\pi}(S_t, A_t)\right]$$
فرض کنید 
$r_t(\theta_{new})$
 نسبت احتمالات 
$r_t(\theta_{new}) = \dfrac{\pi_{\theta_{new}}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ باشد. بنابراین
$$L_{\pi_{old}}(\pi_{new}) = \mathbb{E}_t\left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} A_{\pi}(S_t, A_t)\right] = \mathbb{E}_t\left[r_t(\theta_{new}) A_{\pi_{old}}(S_t,A_t)\right].$$ مقیاس عملکرد  $L^{CLIP}(\theta)$ را به شکل زیر تعریف می‌کنیم
$$L^{CLIP}(\theta) = \mathbb{E}_t\left[min(r_t(\theta) \hat{A}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right]$$
که $\epsilon$ یک ابرپارامتر \LTRfootnote{Hyperparameter} مثلا 
$\epsilon=0.2$
است. اولین عبارت داخل min همان مقیاس عملکرد روش \lr{TRPO} است. در عبارت دوم مقادیر بزرگتر از $1+\epsilon$ یا کوچکتر از 
$1-\epsilon$
در $r_t(\theta)$ به ترتیب به $1+\epsilon$ و $1-\epsilon$ تغییر پیدا کرده‌اند تا تغییرات بزرگ خط‌مشی را کنترل کنند. نهایتا عبارت کوچکتر از میان این دو انتخاب خواهد شد. بنابراین اگر مقیاس عملکرد clip نشده (عبارت اول) کوچکتر یا مساوی با حالت clip شده (عبارت دوم) باشد، $L^{CLIP}$ دقیقا همان مقیاس عملکرد $L_\pi$ خواهد بود. در غیر این صورت مقیاس عملکرد clip شده انتخاب می‌شود تا از تغییرات بزرگ خط‌مشی جلوگیری شود.
\subsection{مقایسه روش بهینه‌سازی خط‌مشی و Q-learning}
نقطه قوت اصلی روش‌های بهینه‌سازی خط‌مشی، اصولی بودن آنهاست ، به این معنا که شما مستقیماً چیزی که می‌خواهید را بهینه‌سازی می‌کنید. در نتیجه این روش‌ها قابل اتکا و باثبات هستند. در مقابل، روش‌های 
\rl{Q-learning}
با یادگیری تابع Q، مقیاس عملکرد را به طور غیر مستقیم بهینه می‌کند. حالت‌های زیادی برای این نوع یادگیری وجود دارد که به شکست منتهی می‌شود، بنابراین این روش‌ها  ثبات کمتری دارند 
\cite{suttonbook}.
روش‌های 
\rl{Q-learning}
 می‌توانند از داده‌ها به طور موثرتری نسبت به تکنیک‌های بهینه‌سازی خط‌مشی استفاده کنند.
\paragraph{تعامل بین بهینه‌سازی خط‌مشی و Q-learning}
 بهینه‌سازی خط‌مشی و
 \lr{Q-learning }
 ناسازگار نیستند(و به نظر می‌رسد تحت برخی شرایط، معادل آن باشد) و طیف وسیعی از الگوریتم‌ها وجود دارد که بین دو حد این طیف وجود دارند. الگوریتم‌هایی که در این طیف قرار دارند قادرند از نقاط قوت  طرفین طیف استفاده کنند.
 
  به طور مثال، 
 \lr{DDPG}الگوریتمی‌است که  یک خط‌مشی قطعی و یک تابع Q را یاد می‌گیرد،
 به طوری که از هریک  برای بهبود دیگری استفاد می‌کند. روش
 \lr{SAC}،
   از خط‌مشی‌های تصادفی، تنظیم آنتروپی  \LTRfootnote{entropy regularization}و چند ترفند دیگر برای  یادگیری و کسب امتیاز بالاتر از 
\lr{DDPG}
    در محک‌های استاندارد
\LTRfootnote{Standard Benchmark}
    استفاده می‌کند.

\قسمت{روش‌های مبتنی بر مدل}

%Unlike model-free RL, there aren’t a small number of easy-to-define clusters of methods for model-based RL: there are many orthogonal ways of using models. We’ll give a few examples, but the list is far from exhaustive. In each case, the model may either be given or learned.
%
%Background: Pure Planning. The most basic approach never explicitly represents the policy, and instead, uses pure planning techniques like model-predictive control (MPC) to select actions. In MPC, each time the agent observes the environment, it computes a plan which is optimal with respect to the model, where the plan describes all actions to take over some fixed window of time after the present. (Future rewards beyond the horizon may be considered by the planning algorithm through the use of a learned value function.) The agent then executes the first action of the plan, and immediately discards the rest of it. It computes a new plan each time it prepares to interact with the environment, to avoid using an action from a plan with a shorter-than-desired planning horizon.
%
%The MBMF work explores MPC with learned environment models on some standard benchmark tasks for deep RL.
%Expert Iteration. A straightforward follow-on to pure planning involves using and learning an explicit representation of the policy, \pi_{\theta}(a|s). The agent uses a planning algorithm (like Monte Carlo Tree Search) in the model, generating candidate actions for the plan by sampling from its current policy. The planning algorithm produces an action which is better than what the policy alone would have produced, hence it is an “expert” relative to the policy. The policy is afterwards updated to produce an action more like the planning algorithm’s output.
%
%The ExIt algorithm uses this approach to train deep neural networks to play Hex.
%AlphaZero is another example of this approach.
%Data Augmentation for Model-Free Methods. Use a model-free RL algorithm to train a policy or Q-function, but either 1) augment real experiences with fictitious ones in updating the agent, or 2) use only fictitous experience for updating the agent.
%
%See MBVE for an example of augmenting real experiences with fictitious ones.
%See World Models for an example of using purely fictitious experience to train the agent, which they call “training in the dream.”
%Embedding Planning Loops into Policies. Another approach embeds the planning procedure directly into a policy as a subroutine—so that complete plans become side information for the policy—while training the output of the policy with any standard model-free algorithm. The key concept is that in this framework, the policy can learn to choose how and when to use the plans. This makes model bias less of a problem, because if the model is bad for planning in some states, the policy can simply learn to ignore it.
%
%See I2A for an example of agents being endowed with this style of imagination.

\subsection{روش مدل جهان}
%\قسمت{روش مدل جهان}
%Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own dream environment generated by its world model, and transfer this policy back into the actual environment.
مدل جهانی
\LTRfootnote{World Models}
 می‌تواند به سرعت و به روشی بدون نظارت آموزش ببیند تا یک بازنمایی از محیط را بیاموزد. سپس با استفاده از ویژگی‌های استخراج شده از مدل جهان به عنوان ورودی به یک عامل، می‌توان یک خط‌مشی ساده و فشرده را آموخت که می‌تواند وظیفه مورد نیاز را حل کند. حتی می‌توانیم عامل را کاملاً در داخل محیط رویایی خود که توسط مدل جهانی آن ایجاد شده، آموزش دهیم و این خط‌مشی آموخته شده را به محیط واقعی انتقال دهیم. در بسیاری از مسائل یادگیری تقویتی مبتنی بر مدل، عامل به مدل قدرتمندی از دینامیک محیط دسترسی دارد.

%Most existing model-based RL[29, 30] approaches learn a model of the RL environment, but still train on the actual environment. Here, we also explore fully replacing an actual RL environment with a generated one, training our agent’s controller only inside of the environment generated by its own internal world model, and transfer this policy back into the actual environment.

اکثر رویکردهای مبتنی بر مدل موجود در یادگیری تقویتی، مدلی از محیط را یاد می‌گیرند، اما همچنان در محیط واقعی آموزش می‌بینند. در این روش، ما همچنین می‌توانیم یک محیط  مصنوعی را کاملاً جایگزین محیط واقعی کنیم و خط‌مشی عامل خود را فقط در داخل محیط مصنوعی آموزش دهیم و درنهایت خط‌مشی آموخته شده را به محیط واقعی انتقال دهیم.

%We present a simple model inspired by our own cognitive system. In this model, our agent has a visual sensory component that compresses what it sees into a small representative code. It also has a memory component that makes predictions about future codes based on historical information. Finally, our agent has a decision-making component that decides what actions to take based only on the representations created by its vision and memory components.