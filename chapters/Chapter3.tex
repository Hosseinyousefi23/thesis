
\فصل{معرفی چند الگوریتم جدید}
در فصل دوم دیدیم که مسئله یادگیری تقویتی، در اصل یک مسئله بهینه‌سازی است که هدف آن یافتن یک خط‌مشی بهینه (یا دست‌کم تقریبی از آن) است که در یک فرآیند تصمیم‌گیری مارکوف تعریف می‌شود. همچنین  با معادله بلمن و روش‌های برنامه‌ریزی پویا آشنا شدیم. در این فصل، دو رویکرد جدید به مسئله خواهیم داشت که امکان استفاده از روش‌های بهینه سازی شناخته شده مانند 
\textit{\مهم{صعود گرادیان}}\LTRfootnote{Gradient Ascent}
و
\textit{\مهم{نزول گرادیان}}\LTRfootnote{Gradient Descent}
را برای حل مسئله یادگیری تقویتی فراهم می‌کند. سپس برخی از مهم‌ترین الگوریتم‌های یادگیری تقویتی جدید، مانند
\lr{DQN}،
\lr{DDPG},
\lr{TRPO} و 
\lr{Actor-Critic}
  را معرفی خواهیم کرد. 
  \قسمت{دو رویکرد مختلف به مسئله}
 در فصل گذشته  دیدیم که با تعریف یک ترتیب جزئی روی خط‌مشی‌های ممکن برای یک
\lr{MDP}،
می‌توان تعبیری برای خط‌مشی بهینه ارائه کرد. در این فصل، تعریف دیگری از خط مشی بهینه ارائه خواهد شد.  نشان دادیم که می‌توان با محاسبه تابع ارزش یک خط‌مشی، آن را بهبود بخشید. الگوریتم تکرار خط‌مشی، با شروع از یک خط‌مشی تصادفی و بهبود دادن آن به صورت گام‌به‌گام، به یک خط‌مشی بهینه همگرا می‌شود.  به طور کلی به روش‌هایی که با استفاده از بهبود گام‌به‌گام خط‌مشی، مستقیما خط‌مشی بهینه  را تقریب می‌زنند، روش‌های 
\textit{\مهم{مبتنی بر خط‌مشی}}\LTRfootnote{Policy Based}
گفته می‌شود. بنابراین روش تکرار خط‌مشی که در فصل قبل معرفی شد، یک روش مبتنی بر خط‌مشی است.
 همچنین دیدیم که همه خط‌مشی‌های بهینه، تابع ارزش مشترکی دارند که تابع ارزش بهینه نامیده  می‌شود و در معادله‌ای موسوم به معادله بهینگی بلمن صدق می‌کند.
\شروع{تعریف}[تابع مقیاس عملکرد] فرض کنید 
$\EuScript{M} = \seq{\EuScript{S},\EuScript{A},\EuScript{R},\EuScript{P}, \lambda}$ 
یک فرآیند تصمیم‌گیری مارکوف و $\Pi$ مجموعه تمام خط‌مشی‌های ممکن برای $\EuScript{M}$ باشد. تابع 
$J: \Pi \to \mathbb{R}$
به شکل زیر تعریف می‌شود
$$J(\pi) \doteq \mathbb{E}_\pi(G_0).$$
\پایان{تعریف}
منظور از 
$\mathbb{E}[G_0]$،
امیدریاضی عایدی در زمان 
$t=0$
است اگر خط‌مشی $\pi$ دنبال شود. از تعریف 
\ref{def:returninf}
داریم
$$G_0 = \sum_{k=0}^{\infty} \gamma^{k} R_{k+1} = R_1 + \gamma R_2 + \gamma^2 R_3 + \dots .$$
به تابع $J$، تابع مقیاس عملکرد خط‌مشی گفته می‌شود.
خط‌مشی بهینه، خط‌مشی‌ای است که مقیاس عملکرد $J$ را بیشینه می‌کند
$$arg \max_{\pi} J(\pi) = \Pi_*.$$
برخی از الگوریتم‌های یادگیری تقویتی، روشی برای پیداکردن نقطه بیشینه تابع $J$ (یا تقریبی از آن) ارائه می‌دهند. روش‌هایی که چنین رویکردی دارند، روش‌های 
\textit{\مهم{مبتنی بر خط‌مشی}}\LTRfootnote{Policy Based}
 نامیده می‌شوند.
 
 رویکرد دیگری که برای حل مسئله یادگیری تقویتی وجود دارد، محاسبه تابع ارزش حالت، یا تابع ارزش عمل بهینه است. منظور از تابع ارزش بهینه، تابع ارزشی است که در معادله بهینگی بلمن 
 \ref{eq:bellman-opti}
 صدق کند. برخی از روش‌های یادگیری تقویتی، روشی برای محاسبه تابع ارزش بهینه با استفاده از معادله بهینگی بلمن ارائه می‌دهند. به روش‌هایی که چنین رویکردی دارند، روش‌های 
 \textit{
 	\مهم{مبتنی بر ارزش
 		}}\LTRfootnote{Value Based}
 گفته می‌شود.

\section{روش‌های مبتنی بر خط‌مشی}
در این قسمت روش‌هایی را در نظر می‌گیریم که برای دستیابی به خط‌مشی بهینه، به جای استفاده از تابع ارزشِ عمل یا ارزشِ حالت، یک خط‌مشی پارامتری‌شده\LTRfootnote{Parameterized Policy} 
با پارامترهای $\theta$ را می‌آموزد.  این کار  معمولا توسط یک شبکه عصبی با پارامترهای 
$\theta \in \mathbb{R}^{d'}$
 به عنوان تقریب‌گر خط‌مشی 
$\pi$
شناخته شده و با نماد $\pi_\theta$ نمایش داده می‌شود. احتمال انتخاب عمل $a$ در حالت $s$ توسط خط‌مشی $\pi$ با پارامترهای $\theta$ را به صورت
$\pi_\theta(a|s)$
یا
$\pi(a|s;\theta)$
نمایش خواهیم داد.
ممکن است برای یادگیری پارامترهای خط‌مشی از یک تابع ارزش نیز استفاده شود، اما تابع ارزش برای انتخاب عمل مورد نیاز نیست. 

پارامترهای $\theta$ با استفاده از \مهم{صعود گرادیان} \LTRfootnote{Gradian ascent}
روی مقیاس عملکرد \LTRfootnote{Performance Measure} 
$J(\pi_\theta)$
(یا به اختصار $J(\theta)$)
به طور مستقیم و یا با بیشینه‌سازی تخمین‌های محلی از  $J(\theta)$ به‌روزرسانی می‌شوند.
این روش‌ها تقریبا همیشه بر خط‌مشی  عمل می‌کنند و 
 هدف آن بیشینه کردن تابع مقیاس عملکرد است. 
 
 
\subsection{روش گرادیان خط‌مشی}
روش گرادیان خط‌مشی، ساده‌ترین و پایه‌ای‌ترین روش در میان روش‌های مبتنی بر خط‌مشی است. در این روش، دنباله‌
ای از پارامترهای خط‌مشی، 
$\theta_0, \theta_1, \theta_2, ...$
ساخته می‌شود که $\theta_0$ به صورت تصادفی مقداردهی می‌شود و از جمله 
$\theta_1$
به بعد، هر جمله، از روی جمله قبلی توسط قانون به‌روزرسانی زیر به‌دست می‌آید
\begin{align}
\theta_{t+1} = \theta_t + \alpha \widehat{\nabla J(\theta_t)}
\label{eq:updatepg}
\end{align}
 
$\widehat{\nabla J(\theta_t)}$
تخمینی نااریب از گرادیان  $J$ نسبت به 
$\theta_t$
 می‌باشد که از نمونه تجربه شده به‌دست می‌آید. هم‌چنین $\alpha$ یک عدد حقیقی مثبت است و 
 \textit{ضریب یادگیری}\LTRfootnote{Learning Coefficient}
 نامیده می‌شود. می‌توان نشان داد که دنباله خط‌مشی‌های
 $\pi_{\theta_0}, \pi_{\theta_1}, \pi_{\theta_2}, ...$
 به یک خط‌مشی بهینه $\pi_*$ همگرا می‌شود. سرعت این همگرایی، به واریانس تخمین‌گر گرادیان وابسته است. هر چه واریانس بزرگتر باشد، همگرایی دیرتر اتفاق می‌افتد. به روش‌هایی که چنین الگویی را برای محاسبه خط‌مشی بهینه دنبال می‌کنند، روش‌های 
\textit{گرادیان خط‌مشی}\LTRfootnote{Policy Gradient}
 گفته می‌شود. قضیه
\ref{th:pg}
 	یکی از مهم‌ترین ویژگی‌های گرادیان مقیاس عملکرد را بیان می‌کند.

\شروع{قضیه}[گرادیان خط‌مشی] این قضیه بیان می‌کند که
\begin{align}
	\nabla_\theta J(\theta) \propto \mathbb{E}_\pi \left[ G_t \frac{\nabla_\theta \pi (A_t|S_t; \theta)}{\pi(A_t|S_t; \theta)} \right]
	\label{eq:pg}
\end{align}
%\sum_{s} \mu(s) \sum_{a} q_\pi(s,a) \nabla_\theta \pi_\theta(a|s)
%که $\mu$ یک توزیع احتمال روی 
%$\EuScript{S}$
% است که متناسب با تعداد دفعاتی است که حالت $s$ با دنبال کردن خط‌مشی 
%$\pi_\theta$
%تکرار می‌شود.
\برچسب{th:pg}
\پایان{قضیه}
\شروع{اثبات}
رجوع شود به 
\cite{suttonbook}.
\پایان{اثبات}

\subsection{الگوریتم REINFORCE}

یک نمونه از روش‌های گرادیان خط‌مشی، خانواده
\lr{REINFORCE}
از الگوریتم‌های یادگیری تقویتی است
\cite{williams1992simple}.
الگوریتم استاندارد \lr{REINFORCE} پارامترهای $\theta$ را در جهت 
$G_t \nabla_\theta  log  \pi (a_t|s_t;\theta)$
 به‌روزرسانی می‌کند که یک تخمین نااریب از
$\nabla_\theta J(\theta)$
است.
%می‌توان نشان داد 
%$$\nabla J(\pi_\theta) = \mathbb{E}_\pi \left[ R_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} \right]$$
با توجه به قضیه 
\ref{th:pg}
در گام $t$،
$\left[ G_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} \right]$
یک تخمین‌گر نااریب از $\nabla_\theta J(\pi_\theta)$ است \cite{suttonbook}. از این رو می‌توان در هر گام، پارامترهای خط‌مشی
 را به شکل زیر به‌روزرسانی کرد
\begin{align}
\theta_{t+1} = \theta_t + \alpha G_t \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} = \theta_t + \alpha G_t \nabla_\theta log \pi_\theta (a|S_t).
\label{def:updaterule}
\end{align}


\شروع{الگوریتم}{الگوریتم دوره‌ای 
\lr{REINFORCE} }
\ورودی{یک تابع پارامتری مشتق پذیر از خط‌مشی 
	$\pi(a|s;\theta)$}

\دستور{پارامترهای خط‌مشی 
	$\theta \in \mathbb{R}^{d'}$
	را مقداردهی اولیه کن
}
\به‌ازای{تکرار کن}
%\اگر{$|E| > 0$}
%	\دستور{یک کاری انجام بده}
%\پایان‌اگر
\دستور{یک دوره 
	$S_0,A_0,R_1,..., S_{T-1}, A_{T-1}, R_T$
	 را با دنبال کردن خط‌مشی $\pi_\theta$ بساز}

\به‌ازای{برای هر گام
$t=0,...,T-1$}
\دستور{
$G_t$
$\longrightarrow$
	 عایدی گام $t$ ام
	 


}
\دستور{
$\theta + \alpha \gamma^t G_t \nabla_\theta ln \pi (A_t|S_t;\theta) \longrightarrow \theta$
}
\پایان‌به‌ازای
\پایان‌به‌ازای
\پایان{الگوریتم}
%چند نمونه از روش‌های بهینه‌سازی خط‌مشی به شرح زیر است. \\
%روش‌های بازیگر منتقد
%\LTRfootnote{Actor-Critic}
%که الگوریتم 
%گرادیان افزایشی
%را مستقیما برای بیشینه سازی 
%$J(\pi\theta)$
%به کار می‌برند.
%روش  
%\lr{Proximal Policy Optimization}
%که!!!!!!!!!!!

\subsection{روش‌های بازیگر-منتقد}
در بخش قبل، با روش گرادیان خط‌مشی و یک روش عملی آن یعنی الگوریتم
\lr{REINFORCE}
آشنا شدیم. دسته‌ای از روش‌های گرادیان خط‌مشی وجود دارند که علاوه بر بهبود گام به گام خط ‌مشی، تخمینی از تابع ارزش  حالت مربوط به آن خط‌مشی را نیز محاسبه کنند. به چنین روش‌هایی، \textit{\مهم{بازیگر-منتقد}} 
\LTRfootnote{Actor-Critic}
گفته می‌شود که \textit{\مهم{بازیگر}} 
\LTRfootnote{Actor}
اشاره به خط‌مشی آموخته شده و \textit{\مهم{منتقد}} 
\LTRfootnote{Critic}  
اشاره به تابع ارزش آموخته شده (معمولا یک تابع ارزشِ حالت) دارد.

با کم کردن یک تابع دلخواه روی حالت‌ها، 
$b: \EuScript{S} \to \mathbb{R}$
از 
$G_t$
در طرف راست رابطه 
\ref{eq:pg}،
می‌توان به تعمیمی از قضیه 
\ref{th:pg}
دست یافت
\begin{align}
	\nabla J(\theta) \propto \mathbb{E}_\pi \left[ (G_t-b(S_t)) \frac{\nabla_\theta \pi (A_t|S_t; \theta)}{\pi(A_t|S_t); \theta} \right].
	\label{eq:pgbase}
\end{align}
برای اثبات درستی رابطه
\ref{eq:pgbase}
کافیست نشان دهیم
$\mathbb{E}_\pi \left[b(S_t) \frac{\nabla_\theta \pi (A_t|S_t; \theta)}{\pi(A_t|S_t); \theta}\right] = 0$.
داریم
\begin{flalign*}
\mathbb{E}_\pi \left[b(s) \frac{\nabla_\theta \pi (A_t|S_t;\theta)}{\pi(A_t|S_t); \theta}\right] & = \sum_{a} \pi(a|S_t; \theta) \mathbb{E}_\pi \left[b(S_t) \frac{\nabla_\theta \pi (a|S_t;\theta)}{\pi(a|S_t; \theta)}\right] & \\
& = \sum_{a} \mathbb{E}_\pi \left[b(S_t) \nabla_\theta \pi (a|S_t;\theta)\right] & \\
& = b(S_t) \sum_{a} \mathbb{E}_\pi\left[\nabla_\theta  \pi (a|S_t;\theta)\right] & \\
& = b(S_t) \mathbb{E}_\pi \left[\nabla_\theta \sum_{a} \pi(a|S_t;\theta) \right] & \\
& = b(S_t) \mathbb{E}_\pi \left[\nabla_\theta 1\right] \\
& = 0.
\end{flalign*}

بنابراین رابطه 
\ref{eq:pgbase}
به ازای هر تابع $b$ روی حالت‌ها برقرار است. به چنین تابعی 
\textit{\مهم{پایه}}\LTRfootnote{Baseline}
 گفته می‌شود. اگر از رابطه
  \ref{eq:pgbase}
  به عنوان گرادیان تابع مقیاس عملکرد $J$ استفاده کنیم، آنگاه قانون به‌روزرسانی 
  \ref{def:updaterule}
  به صورت زیر درخواهد آمد
\begin{align}
\theta_{t+1} \doteq \theta_t + \alpha (G_t - b_t(S_t)) \frac{\nabla_\theta \pi_\theta (a|S_t)}{\pi_\theta (a|S_t)} = \theta_t + \alpha (G_t- b_t(S_t)) \nabla_\theta log \pi_\theta (a|S_t).
\label{def:updaterulebaseline}
\end{align}
در رابطه 
\ref{def:updaterulebaseline}،
عبارت 
$\nabla_\theta  log  \pi(A_t|S_t;\theta) (G_t - b(S_t))$
به عنوان تخمینی از گرادیان تابع مقیاس عملکرد $J$ استفاده می‌شود. دیدیم که استفاده از پایه، امیدریاضی این تخمین‌گر را  تغییر نمی‌دهد و تخمین، نااریب باقی می‌ماند؛ اما می‌تواند واریانس تخمین‌گر را به طور قابل توجهی کاهش دهد. معمولا از تخمینی از تابع ارزش $v_\pi$ به عنوان پایه استفاده می‌شود،
که منجر به تخمینی با واریانس بسیار کوچک‌تر از گرادیان خط‌مشی خواهد شد؛ در حالی‌که تخمین، نااریب باقی می‌ماند و درنتیجه عملیات یادگیری با سرعت بیشتری انجام می‌شود
$$b_t(s_t) \approx v_\pi (s_t).$$

اگر درهر گام، علاوه بر پارامترهای خط‌مشی، تخمین‌های تابع ارزش نیز، به‌روزرسانی شود؛  در این صورت می‌توان این رویکرد را به عنوان معماری 
\textit{\مهم{بازیگر-منتقد}}\LTRfootnote{Actor-Critic}
 تعبیر کرد که خط‌مشی $\pi$  به عنوان \مهم{بازیگر} و پایه $b_t$ به عنوان \مهم{منتقد}  شناخته می‌شود.
از آنجایی که
$b_t(s) \approx v_{\pi_{\theta_t}}(s)$
و
$G_t \approx Q_\pi (a_t, s_t)$،
در معادله 
\ref{eq:pgbase} و
\ref{def:updaterulebaseline}،
عبارت
$G_t - b_t(S_t)$
می‌تواند به شکل تخمینی از
\textit{\مهم{مزیت}}\LTRfootnote{Advantage}
 عمل $A_t$ در حالت $S_t$ یا 
$\mathbb{A}_\pi(A_t,S_t)=Q_\pi(A_t,S_t)-v_\pi(S_t)$
 تعبیر شود. از این رو به این روش، روش 
\textit{
\مهم{بازیگر-منتقد مزیت}}\LTRfootnote{Advantage Actor-Critic}
نیز گفته می‌شود.

\شروع{الگوریتم}{الگوریتم
\lr{Advantage Actor-Critic}}
\ورودی{یک تابع پارامتری‌شده مشتق پذیر از خط‌مشی 
	$\pi(a|s;\theta)$}
\ورودی{یک تابع پارامتری‌شده مشتق‌پذیر از تابع ارزشِ حالت
$v_\omega(s)$}
\دستور{پارامترهای خط‌مشی
	$\theta \in \mathbb{R}^{d'}$
	و تابع ارزشِ حالت
	$\omega \in \mathbb{R}^d$
	را مقداردهی اولیه کن
}
\به‌ازای{تکرار کن}
%\اگر{$|E| > 0$}
%	\دستور{یک کاری انجام بده}
%\پایان‌اگر
\دستور{حالت اولیه $S$ را بساز}
\دستور{$1 \longrightarrow I$}
\تاوقتی{$S$ حالت نهایی نیست}
\دستور{
	$A \sim \pi_\theta(.|S)$
}
\دستور{عمل $A$ را انجام بده و حالت $S'$ و پاداش $R$ را مشاهده کن}

\دستور{
	$ \longrightarrow \delta$
	$R + \gamma v_\omega(S') - v_\omega(S)$
}
\دستور{
	$\longrightarrow \omega$
	$\omega + \alpha^\omega I \delta \nabla_\omega v_\omega(S)$
}
\دستور{
	$ \longrightarrow \theta$
	$\theta +  \alpha^\theta   I  \delta \nabla_\theta \ ln \ \pi_\theta(A| S)$
}

\دستور{
	$\gamma I \longrightarrow I$
}

\دستور{
	$S' \longrightarrow S$
}
\پایان‌تاوقتی
\پایان‌به‌ازای
%\caption{ برگرفته شده از !!!!!!!!!!!!}
\پایان{الگوریتم}
\subsection{روش TRPO}
در بخش قبل، تعریفی از تابع مزیت عمل $a$ در حالت $s$ تحت خط‌مشی 
$\pi$
ارائه شد
$$\mathbb{A}_\pi(s,a) \doteq q_\pi(s,a) - v_\pi(s).$$
این تابع، نقش مهمی در تغییرات تابع مقیاس عملکرد $J$ نسبت به تغییرات خط‌مشی ایفا می‌کند. لم 
\ref{lm:adv}
اهمیت تابع مزیت را در مقایسه مقیاس عملکرد دو خط‌مشی $\pi$ و $\pi'$، نشان می‌دهد
\شروع{لم}
برای هر دو خط‌مشی $\pi$ و $\pi'$
\begin{align}
J(\pi') = J(\pi) + \mathbb{E}_{\pi'}\left[\sum_{t=0}^{\infty} \gamma^t \mathbb{A}_\pi(S_t, A_t)\right].
\label{eq:adv}
\end{align}
\label{lm:adv}
\پایان{لم}
\شروع{اثبات}
رجوع شود به 
\cite{degris2012off}.
\پایان{اثبات}
%\قسمت{روش TRPO}
فرض کنید
$\rho_\pi : S \longrightarrow \mathbb{R}$
تابع 
\textit{فرکانس کاهشی دیده شدن}\LTRfootnote{Discounted Visit Frequency}
	 حالت‌ها تحت خط‌مشی $\pi$ باشد،

$$\rho_\pi(s) = P(S_0=s) + \gamma P(S_1=s) + \gamma^2 P(S_2=s) + ...$$
که دنباله 
$S_0, S_1, S_2, \dots$
 خط‌مشی $\pi$ را دنبال می‌کند.
با بسط رابطه
\ref{eq:adv}
داریم
\begin{flalign*}
J(\pi')  & = J(\pi) + \mathbb{E}_{\pi'}\left[\sum_{t=0}^{\infty} \gamma^t \mathbb{A}_\pi(S_t, A_t)\right] & \\
& = J(\pi) + \sum_{t=0}^{\infty} \sum_{s} P(S_t = s| \pi') \sum_{a} \pi'(a|s) \gamma^t \mathbb{A}_\pi(s,a) & \\
& = J(\pi) + \sum_{s} \sum_{t=0}^{\infty} \gamma^t P(S_t = s | \pi') \sum_{a} \pi'(a|s) \mathbb{A}_\pi(s,a) & \\
& = J(\pi) + \sum_{s} \rho_{\pi'}(s) \sum_{a} \pi'(a|s) \mathbb{A}_\pi(s,a). \numberthis
\label{eq:jres}
%\sum_{s} \rho_{\pi'}(s) \sum_{a} \pi'(a|s) A_\pi(s,a) \\
\end{flalign*}
%$$J(\pi')= J(\pi) + \sum_{s} \rho_{\pi'}(s) \sum_{a} \pi'(a|s) A_\pi(s,a)$$ که  $A_\pi(s,a) = Q_\pi (s,a) - V_\pi(s)$ تابع مزیت عمل $a$ در حالت $s$ باشد.
در رابطه 
\ref{eq:jres}،
وابستگی پیچیده  $\rho_\pi'(s)$ در طرف راست تساوی به $\pi'$، بهینه‌سازی مستقیم را مشکل می‌کند.
برای حل این مشکل شولمن و همکارانش 
\cite{schulman2015trust}
مقیاس عملکرد دیگری، $L_\pi(\pi')$، را معرفی می‌کنند و نشان می‌دهند که اگر $\pi$ و $\pi'$ به اندازه کافی به یکدیگر نزدیک باشند، افزایش مقیاس عملکرد جدید $L_\pi(\pi')$، همواره با افزایش $J$ همراه خواهد‌بود
\begin{align}
L_\pi(\pi') \doteq J(\pi) + \sum_{s} \rho_{\pi}(s) \sum_{a} \pi'(a|s) A_\pi(s,a)
\label{l_pi}
\end{align}
توجه کنید که در طرف راست 
\ref{l_pi}،
 از تابع فرکانس $\rho_\pi$ به جای 
 $\rho_{\pi'}$
استفاده می‌شود.

\شروع{قضیه}
فرض کنید 
$\alpha = D_{TV}^max(\pi_{old}, \pi_{new})$
باشد که 
$$D_{TV}^{max} (\pi, \pi') = \max_{s} D_{TV}\left(\pi(.|s) || \pi'(.|s)\right)$$
و 
$D_{TV}(p || q)$
دیورژانس 
\textit{کل تغییرات}\LTRfootnote{Total Variation}
بین دو بردار $p$ و $q$ باشد، یعنی
$$D_{TV}(p || q) = \dfrac{1}{2} \sum_{i} |p_i - q_i|.$$ در این صورت
$$J(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \dfrac{4 \epsilon \gamma}{(1- \gamma)^2}$$ که $\epsilon = \max_{s,a} |\mathbb{A}_\pi(s,a)|$.
\label{q:ghas}
\پایان{قضیه}
با توجه به قضیه \ref{q:ghas} و نامعادله $D_{TV}(p || q)^2 \le D_{KL}(p || q)$  که  $D_{KL} (p || q)$ را دیورژانس
\lr{Kullback-Leibler}
  دو بردار $p$ و $q$ می‌نامند
  \cite{schulman2015trust}؛
می‌توان  نتیجه گرفت
\begin{align*}
	J(\pi') \ge L_{\pi}(\pi') - C \ D_{KL}^{max}(\pi, \pi')
\end{align*}
که
\begin{align}
	C = \dfrac{4 \epsilon \gamma}{(1-\gamma)^2}.
	\label{eq:cc}
\end{align}
رابطه \ref{eq:cc} نشان می‌دهد که می‌توان یک دنباله صعودی از خط‌مشی‌ها داشت به‌طوری‌که
$$J(\pi_0) \le J(\pi_1) \le J(\pi_2) \le ... $$
چرا که فرض کنید
$M_i(\pi) = L_{\pi_i}(\pi) - C \ D_{KL}^max(\pi_i, \pi)$
در این صورت
 $$J(\pi_{i+1}) \ge M_i(\pi_{i+1}) $$ 
 و
$$J(\pi_i) = M_i(\pi_i)$$
بنابراین
$$J(\pi_{i+1}) - J(\pi_i) \ge M_i(\pi_{i+1}) - M_i(\pi_i).$$
می‌توان نتیجه گرفت با بیشینه کردن $M_i$ در هر گام می‌توان اطمینان حاصل کرد که مقیاس عملکرد اصلی $J$ غیرنزولی خواهد بود.

اگر $\hat{\mathbb{A}}_t$ تخمین مزیت 
$\mathbb{A}_{\pi_t}(S_t, A_t)$
باشد که در گام $t$ محاسبه می‌شود، می‌توان نشان داد که در روش
\lr{TRPO}
مقیاس عملکرد $L_\pi$ در هر گام به شکل
$$\mathbb{E}_t\left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} \hat{A}_t \right]$$خواهد بود
\cite{schulman2015trust}.

\شروع{الگوریتم}{الگوریتم تکرار خط‌مشی با مقیاس عملکرد $L_\pi$}

\دستور{خط‌مشی $\pi_0$
	را مقداردهی اولیه کن
}
\به‌ازای{برای 
	$i=0,1,...$
	تکرار کن}
%\اگر{$|E| > 0$}
%	\دستور{یک کاری انجام بده}
%\پایان‌اگر
\دستور{همه مزیت‌های 
	$\mathbb{A}_{\pi_i}(s,a)$
	را محاسبه کن
}
\دستور{
	$arg \max_{\pi} [L_{\pi_i}(\pi) - C \ D_{KL}^{max}(\pi_i, \pi)] \longrightarrow \pi_{i+1}$
	که 
	$$C = (4 \epsilon \gamma) / (1-\gamma)^2,$$  $$L_{\pi_i}(\pi) = J(\pi_i) + \sum_{s} \rho_{\pi_i}(s) \sum_{a} \pi(a|s) \mathbb{A}_{\pi_i} (s,a)$$
}
\پایان‌به‌ازای
\پایان{الگوریتم}

\subsection{روش PPO}
%\قسمت{روش PPO}
در روش \lr{TRPO} دیدیم که بیشینه‌سازی مقیاس عمکلرد $L_\pi$ ساده‌تر از مقیاس عملکرد $J$ است ولی در عوض الگوریتم صعود گرادیان تنها مجاز به اعمال تغییرات کوچک در خط‌مشی است. در روش 
\lr{TRPO}،این مشکل با محدود کردن خط‌مشی جدید، به داخل یک شعاع مشخص حول خط‌مشی قدیمی، برطرف شد. یک راه دیگر برای کنترل تغییرات خط‌مشی، استفاده از تابع
\lr{CLIP}
  است. در روش
\lr{TRPO}،

دیدیم که تابع مقیاس عملکرد، در گام $t$ به شکل زیر تعریف می‌شود
$$L_{\pi_{old}}(\pi_{new}) = \mathbb{E}_t \left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} \mathbb{A}_{\pi}(S_t, A_t)\right].$$
فرض کنید 
$r_t(\theta_{new})$
نسبت احتمالات 
$\dfrac{\pi_{\theta_{new}}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$
 باشد. بنابراین
$$L_{\pi_{old}}(\pi_{new}) = \mathbb{E}_t \left[\dfrac{\pi_{\theta_{new}}(A_t| S_t)}{\pi_{\theta_{old}}(A_t|S_t)} \mathbb{A}_{\pi}(S_t, A_t)\right] = \mathbb{E}_t\left[r_t(\theta_{new}) \mathbb{A}_{\pi_{old}}(S_t,A_t)\right].$$
 مقیاس عملکرد 
 $L^{CLIP}(\theta)$
  را به شکل زیر تعریف می‌کنیم
$$L^{CLIP}(\theta) \doteq \mathbb{E}_t\left[min(r_t(\theta) \hat{A}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right]$$
که $\epsilon$ یک ابرپارامتر\LTRfootnote{Hyperparameter}
است، مثلا 
$\epsilon=0.2$.
 اولین عبارت داخل
$min$
   همان مقیاس عملکرد روش \lr{TRPO} است. در عبارت دوم مقادیر بزرگتر از $1+\epsilon$ یا کوچکتر از 
$1-\epsilon$
در $r_t(\theta)$ به ترتیب به $1+\epsilon$ و $1-\epsilon$ تغییر پیدا کرده‌اند تا  از تغییرات بزرگ خط‌مشی، جلوگیری شود. نهایتا عبارت کوچک‌تر از میان این دو انتخاب خواهد شد. بنابراین اگر مقیاس عملکرد clip نشده (عبارت اول) کوچکتر یا مساوی با حالت clip شده (عبارت دوم) باشد، $L^{CLIP}$ دقیقا همان مقیاس عملکرد $L_\pi$ خواهد بود. در غیر این صورت مقیاس عملکرد clip شده انتخاب می‌شود  و به این شکل، از تغییرات بزرگ خط‌مشی جلوگیری می‌شود.

\section{روش‌های مبتنی بر ارزش}
%\قسمت{روش‌های مبتنی بر ارزش}


در روش‌های یادگیریِ تقویتی بدون مدل مبتنی بر ارزش،
تقریبی از تابع ارزش عمل، $q_\pi$،
 با استفاده از  یک تقریب‌گر\LTRfootnote{Function approximator}
 مانند شبکه عصبی، محاسبه می‌شود. فرض کنید
$q(s,a;\theta)$
یک تابع ارزش عمل تقریبی با پارامتر  
$\theta$ 
باشد.
الگوریتم‌های مختلفی برای به‌روزرسانی $\theta$ وجود دارد.
الگوریتم 
\lr{Q-learning}
  یک نمونه از چنین الگوریتم‌هایی ‌است.
  خانواده روش‌های 
\lr{Q-learning}
    تلاش می‌کنند مستقیما تابع ارزش عمل بهینه $q_*(s,a)$ را تخمین بزنند. آن‌ها به طور معمول از یک تابع هزینه مبتنی بر معادله بلمن استفاده می‌کنند. این بهینه‌سازی تقریباً همیشه به صورت مستقل از خط‌مشی انجام می‌شود، به این معنی که در هر به‌روزرسانی می‌توانیم از داده‌های جمع‌آوری شده در هر مرحله استفاده کنیم؛ بدون  درنظر گرفتن اینکه نحوه انتخاب عامل در هنگام به‌دست آوردن داده‌ها چگونه بوده است.  در نهایت، خط‌مشی بهینه از طریق ارتباط بین 
 $q_*$
  و
  $\pi^*$
  به‌دست می‌آید. 
  عامل بعد از یادگرفتن تابع $Q_\theta(s,a)$ به طوری‌که  $q_\theta(s,a) \approx q_*(s,a)$ می‌تواند عمل بهینه در حالت $s$ را به صورت زیر محاسبه کند $$a(s) = arg \max_a q_{\theta}(s,a).$$
  از جمله الگوریتم‌های مبتنی بر ارزش می‌توان به موارد زیر اشاره کرد:
  \begin{itemize}
  	\item روش کلاسیک DQN که حوزه یادگیری تقویتی ژرف\LTRfootnote{Deep reinforcement learning} را عمیقا ارتقا بخشید.
  	\item روش C51 که توزیعی روی عایدی را می‌آموزد که امیدریاضی آن $q_*$ است.
  \end{itemize}

\subsection{روش DQN}
روش 
\textit{\مهم{شبکه ژرف
Q}}\LTRfootnote{Deep Q-Network}
(DQN)
\ref{mnih2013playin}
 یک الگوریتم یادگیری تقویتی بدون مدل برای فضای عمل گسسته است که هدف آن تقریب مستقیم تابع ارزشِ عمل بهینه 
 $q(s,a; \theta) \approx  q^*(s,a)$
 است. در این روش، پارامترهای $\theta$ از تابع ارزشِ عمل با به حداقل رساندن تابع هزینه به شکل مرحله به مرحله آموخته می‌شوند، به شکلی که تابع هزینه $i$ام به صورت
 $$L_i(\theta_i) \doteq \mathbb{E} {\left( r+\gamma \max_{a'} q(s',a'; \theta_{i-1})- q(s,a:\theta_i) \right)}^2$$
 تعریف می‌شود که 
$s'$
 حالتی است که بعد از حالت $s$ دیده می‌شود. یک شبکه عصبی که 
\textit{\مهم{شبکه Q}}\LTRfootnote{Q-network}
  نامیده می‌شود، به عنوان تقریب‌گر تابع
   $q_*$
    استفاده می‌شود.
در این روش، که یک روش مستقل از خط‌مشی است، دوره‌ها توسط یک خط‌مشی 
$\epsilon$-حریصانه
نسبت به تقریب فعلی تابع ارزش، $q$، تولید می‌شوند. 
یک چالش جدی که در استفاده از شبکه عصبی برای یادگیری تقویتی وجود دارد این است که اغلب الگوریتم‌های بهینه‌ سازی، فرض می‌کنند که نمونه‌ها به طور یکنواخت و مستقل از یکدیگر، از فضای داده‌ها انتخاب می‌شوند. درحالی‌که در یادگیری تقویتی، این فرض معمولا برقرار نیست؛ به خصوص اگر نمونه‌ها از طریق کاوش در محیط واقعی به‌دست آمده باشد. یکی از روش های  حل این مشکل، استفاده از 
\textit{انبار تکرار}\LTRfootnote{Replay Buffer}
است. در هر گام، تجربه به‌دست آمده را در حافظه‌ای به نام انبار تکرار ذخیره می‌کنیم. هر تجربه به شکل چهارتایی 
$(s,a,r,s')$
ذخیره می‌شود. از انبار تکرار می‌توان برای نمونه‌گیری و دسته \LTRfootnote{batch} سازی استفاده کرد. اگر انبار تکرار پر شود، کهنه‌ترین تجربیات، حذف می‌شوند تا تجربیات تازه جایگزین آن‌ها شود.

%چهارتایی‌های انتقال،
%$s_t, a_t, r_t, s_{t+1}$
%که در طول آموزش به وجود می‌آیند، در جایی به نام
%\textit{انبار تکرار}\LTRfootnote{Replay Buffer}
%ذخیره می‌شوند.

 شبکه Q توسط الگوریتم نزول گرادیان روی تابع هزینه 
$L_i$
که تابعی از پارامترهای شبکه 
\lr{Q}،
$\theta_i$
 است، روی یک نمونه تصادفی از تجربیات درون انبار تکرار، آموزش داده می‌شود.
%\قسمت*{روش DQN}
%معمولا برای تقریب زدن توابع ارزش در یادگیری تقویتی، از یک تابع خطی استفاده می‌شود.
%اما گاهی اوقات از یک تقریب عملکرد غیرخطی به جای آن، مانند یک شبکه عصبی هم می‌توان استفاده کرد. شبکه‌های عصبی با عنوان شبکه Q\LTRfootnote{Q-Network} شناخته می‌شوند.
شبکه Q را می‌توان با کمینه ساختن دنباله‌ای از توابع هزینه به شکل 
$L_1(\theta_1), L_2(\theta_2), L_3(\theta_3), ... $ آموزش داد؛ به طوری‌که
$$L_i(\theta_i)=\mathbb{E}\left[(y_i - q(s,a;\theta_i))^2\right]$$  که  $$y_i = \mathbb{E}[r + \gamma \max_{a'} q(s',a'; \theta_{i-1})| s,a].$$ با مشتق گرفتن از تابع هزینه نسبت به پارامترهای $\theta_i$  خواهیم داشت: $$\nabla_{\theta_i} L_i{\theta_i} = \mathbb{E}\left[ \left(r + \gamma \max{a'} q(s',a';\theta_{i-1}) - q(s,a;\theta_i)\right) \nabla_{\theta_i} q(s,a;\theta_i)\right].$$
%Rather than computing the full expectations in the above gradient, it is often computationally expedient to optimise the loss function by stochastic gradient descent. If the weights are updated after
%every time-step, and the expectations are replaced by single samples from the behaviour distribution
%ρ and the emulator E respectively, then we arrive at the familiar Q-learning algorithm [26].
به جای محاسبه امیدریاضی کامل در گرادیان فوق، غالباً از نظر محاسباتی، بهینه‌سازی تابع هزینه با نزول گرادیان تصادفی  \LTRfootnote{stochastic gradient descend} راه‌حل بهتری است.  در روش نزول گرادیان تصادفی، در هر مقطع زمانی، وزن‌ها به‌روزرسانی می‌شود و امیدریاضی، با یک نمونه از توزیع خط‌مشی رفتار \LTRfootnote{behavior policy} جایگزین می‌شود.
% الگوریتم
%\ref{alg:qlearnn}
% Q-learning را نشان می‌دهد.
\شروع{الگوریتم}
{الگوریتم \lr{Q-learning} با \lr{Experience replay}}

\دستور{حافظه 
	replay 
	$D$
را مقدار دهی اولیه کن}
\دستور{تابع ارزشِ عمل Q را با وزن‌های تصادفی مقداردهی اولیه کن}
\به‌ازای{برای هر اپیزود 
$1...M$}
\دستور{دنباله 
	$d_1 = \{S_1\}$
	 و کدینگ 
	 $\phi_1 = \phi(d_1)$
	  را مقداردهی اولیه کن}
\به‌ازای{برای $t=1...T$}
\دستور{با احتمال 
	$1-\epsilon$
	عمل 
	$a_t = \max_{a} q_*(\phi(d_t),a;\theta)$
 را انتخاب کن، در غیر این صورت، یک عمل تصادفی $a_t$
	 را انتخاب کن}
 \دستور{عمل $a_t$ را انجام بده و حالت $S_{t+1}$ و پاداش $R_t$ را مشاهده کن}
 \دستور{قرار بده 
 	$d_{t+1} = d_t,a_t,S_{t+1}$
 	 و 
 	 $\phi_{t+1} = \phi(d_{t+1})$
  }
\دستور{تجربه 
	$(\phi_t, A_t, R_t, \phi_{t+1})$
	 را در $D$ ذخیره کن}
 \دستور{یک نمونه تصادفی از تجریه‌های  
 	$(\phi(j), A_j, R_j, \phi_{j+1})$
 	از انبار تکرار $D$ انتخاب کن}
 \دستور{قرار بده 
 	\lr{
 	$y_j =$ 
\begin{cases}
 		$r_j$
 		 &
 		  $\phi_{j+1} \  terminal$ \\
 		$r_j$ & $otherwise$
 \end{cases}}
}
\دستور{یک گام از نزول گرادیان را برای تابع هزینه 
	$(y_j - q(\phi_j, a_j; \theta))^2$
	 انجام بده}
  \پایان‌به‌ازای
\پایان‌به‌ازای
\label{alg:qlearnn}
\پایان{الگوریتم}
%توجه داشته باشید که الگوریتم \ref{alg:qlearnn} یک الگوریتم بدون مدل است. این کار وظیفه یادگیری تقویتی را مستقیماً با استفاده از نمونه‌های شبیه ساز E بدون ساختن صریح تخمین E حل می‌کند.
%\\ استراتژی حریصانه، 
% $a = \max_{a} Q(s, a; \theta)$
% ، در حالی‌که یاد می‌گیرد که
%کاوش کافی در فضای حالت را تضمین کند. توزیع رفتار اغلب توسط یک استراتژی Greed انتخاب می‌شود که استراتژی حریصانه را با احتمال 1 دنبال می‌کند و یک
%اقدام تصادفی با احتمال $\epsilon$
%!!!!!!!!!!!!!!
\subsubsection{روش C51}
در این بخش با رویکردی توزیعی در حل مسئله یادگیری تقویتی آشنا خواهیم شد. در قسمت‌های قبلی با مفهوم عایدی آشنا شدیم و تابع ارزش عمل 
$q_\pi$
مربوط به خط‌مشی $\pi$، به عنوان امیدریاضی عایدی تعریف شد
$$q_\pi(s,a) \doteq \mathbb{E}\left[G_t| S_t=s, A_t = a\right].$$
در رویکرد 
\textit{توزیعی}\LTRfootnote{Distributional}،
به جای تمرکز بر روی امیدریاضی عایدی، به خود عایدی به عنوان یک متغیر تصادفی و توزیع آن توجه می‌شود.

\شروع{تعریف}[تابع پاداش چشمداشتی]
\textit{تابع پاداش چشمداشتی}\LTRfootnote{Expected Reward Function}
$R: \EuScript{S} \times \EuScript{A} \to \mathbb{R}$
هر حالت-عمل $s,a$ را به امیدریاضی پاداش انتخاب عمل $a$ در حالت $s$ نسبت می‌دهد
\begin{align}
	R(s,a) \doteq mathbb{E}\left[R_{t+1}|S_t=s, A_t=a\right].
\end{align}
\پایان{تعریف}


\شروع{تعریف}[متریک واسرشتاین]
برای 
$F$
و 
$G$
که دو تابع توزیع تجمعی بر روی اعداد حقیقی هستند، تعریف می‌شود
\begin{align}
d_p (F,G):= \inf_{U,V} \parallel U-V \parallel_{p},
\label{eq:inf}
\end{align}
که اینفیمم روی تمام جفت متغیرهای تصادفی  
$(U,V)$
که تابع توزیع تجمعی مربوط به آن‌ها به ترتیب 
$F$
و 
$G$
است، گرفته می‌شود.
اینفیمم توسط تبدیل معکوس تابع توزیع تجمعی روی یک متغیر تصادفی 
$\EuScript{u}$
با توزیع یکنواخت در بازه 
$[0,1]$
حاصل می‌شود
$$d_p (F,G) = \parallel F^{-1}(\EuScript{U})-G^{-1} (\EuScript{U})\parallel_{p}$$
برای 
$p < \infty$
می‌توان نوشت
$$d_p (F,G) = \left( \int_{0}^{1} \mid F^{-1}(\EuScript{U})-G^{-1} (\EuScript{U})\mid^{p} d\EuScript{u} \right)^{1/p}$$
با داشتن دو متغیر تصادفی 
$(U,V)$
با توابع توزیع تجمعی 
$F_U,F_V$
می‌توان نوشت
$$d_p(U,V) := d_p(F_U,F_V).$$
می‌توان با قرار دادن خود متغیر‌های
$U$ و $V$
به جای توابع توزیع تجمعی‌شان در فرمول 
\ref{eq:inf}
به طور ساده تر نوشت
$$d_p(U,V)= \inf_{U,V} \parallel U-V\parallel_p.$$ 

به تابع $d_p$، تابع متریک
\textit{
	\مهم{واسرشتاین}
}\LTRfootnote{Wasserstein}
توزیعی گفته می‌شود (نگاه کنید به 
\cite{bickel1981some}).
\پایان{تعریف}

\شروع{تعریف}[توزیع ارزش]
تابع 
\textit{توزیع ارزش}\LTRfootnote{Value Distribution}
مربوط به خط‌مشی $\pi$،
$G_\pi : \EuScript{S} \times \EuScript{A} \to \EuScript{P}(\mathbb{R})$
تابعی است که هر حالت-عمل را به یک توزیع احتمال روی عایدی‌ها نسبت می‌دهد
$$G_\pi(s,a) = \sum_{t=0}^{\infty} \gamma^t R_t, $$
$$S_t, R_t \sim p(.|S_{t-1}, A_{t-1}), A_t \sim \pi(.|S_t), S_0 = s, A_0=a$$ 
\پایان{تعریف}
فرض کنید 
 $\EuScript{Z}$
  فضای تمام توزیع ارزش‌ها باشد و برای دو توزیع ارزش
  $G_1$
  و
  $G_2$
$$\bar{d}_p(G_1, G_2) \doteq \sup_{s,a} d_p(G_1(s,a), G_2(s,a)).$$
می‌توان نشان داد که $\bar{d}_p$ یک تابع متریک روی $\EuScript{Z}$ است
\cite{bellemare2017distributional}.
%\begin{align}
%Q^*(x,a)= \mathbb{E} R(x,a)+\gamma \mathbb{E}_P \max_{a' \in \EuScript{A}} Q^* (x',a').
%\label{eq:Q^*}
%\end{align}
%در معادله \ref{eq:Q^*} $Q^*$ نقطه ثابت منحصر به فرد است، تابع ارزش بهینه، که مربوط است به مجموعه خط‌مشی‌های بهینه 
%$\Pi^*$
%($\pi*$ بهینه است اگر
%$\mathbb{E}_{a\sim \pi^*} Q^*(x,a) = \max_{a}Q^*(x,a).$)
اگر تابع ارزش $q$ و تابع پاداش چشمداشتی $R$ را بردارهایی در فضای 
$\mathbb{R}^{\EuScript{X \times A}}$ 
در نظر بگیریم؛ در این صورت عملگر بلمن، $\EuScript{T}^\pi$، و عملگر بهینگی بلمن $\EuScript{T}$ به شکل زیر تعریف می‌شوند
\begin{align}
\EuScript{T}^\pi Q(x,a) \doteq \mathbb{E} R(x,a) + \gamma \mathbb{E}_\pi Q(x',a'), \\ \nonumber
\EuScript{T} Q(x,a) \doteq \mathbb{E} R(x,a) + \gamma \mathbb{E} \max_{a' \in \EuScript{A}} Q(x',a'). \nonumber
\end{align}
عملگر بلمن و عملگر بهینگی بلمن، به طور خاص، هردو نگاشتی انقباضی روی
$\mathbb{R}^{\EuScript{S} \times \EuScript{A}}$هستند و اعمال مکرر آنها روی یک $Q_0$ اولیه، به ترتیب به $Q^\pi$ و $Q^*$ همگرا می‌شود \cite{bertsekas1996neuro}.
فرض کنید 
$\EuScript{Z}$
فضای تمام توزیع ارزش‌ها و
$\bar{d}_p: \EuScript{Z} \to \EuScript{Z}$
که
$$\bar{d}_p(G_1, G_2) \doteq \sup_{s,a} d_p(G_1(s,a), G_2(s,a))$$
 $d_p$ متریک 
\textit{واسرشتاین}\LTRfootnote{Wasserstein}
مربوط به توابع توزیع تجمعی است.
می‌توان نشان داد که $\bar{d}_p$ یک تابع متریک روی $\EuScript{Z}$ می‌باشد
\cite{bellemare2017distributional}.

%
% امید ریاضی از معادله بلمن بیرون کشیده می‌شود و در عوض یک توزیع کامل از متغیر تصادفی 
%
%$Z^\pi$ 
%قرار داده می‌شود.

%\شروع{تعریف}
%\EuScript{Z} 
%فضای توزیع ارزش 
%
%برای دو توزیع ارزش 
%$Z_1$
%و
%$Z_2$
%عضو 
%\EuScript{Z} 
%از فرم ماکسیمال متریک واسرشتاین\LTRfootnote{Wasserstein}
%استفاده می‌کنیم:
%$$\bar{d_p}(Z_1,Z_2) := \sup_{x,a} d_p(Z_1(x,a),Z_2(x,a))$$
%از 
%$\bar{d_p}$
%برای همگرایی عملگرهای توزیعی بلمن
%\LTRfootnote{Distributional Bellman Operators}
% استفاده می‌شود.
%\پایان{تعریف}
\subsubsection{ارزیابی خط‌مشی}
در روند ارزیابی خط‌مشی، تابع ارزش
$v_\pi$
مربوط به خط‌مشیِ $\pi$ داده شده مطلوب است.
تابع پاداش به عنوان یک بردار تصادفی، 
$R \in \EuScript{Z}$،
وعملگر توزیعی انتقال 
$P^\pi : \EuScript{Z} \rightarrow \EuScript{Z}$،
به صورت زیر تعریف می‌شود$$P^{\pi} Z(x,a) \doteq Z(X',A')$$و$$X' \sim P(.|x,a), A'\sim \pi (.|X'),$$\شروع{تعریف}
اگر 
$\Pi^*$
مجموعه‌ خط‌مشی‌های بهینه باشد، 
مجموعه توابع توزیع ارزش بهینه با نماد $\EuScript{Z}^*$ تعریف می‌شود
$$\EuScript{Z}^* \doteq \{{Z^{\pi^{*}} : \pi^* \in \Pi^*\} }.$$
تاکید می‌شود که همه توزیع ارزش‌هایی که امیدریاضی آن‌ها
 $q_*$ 
باشد، بهینه نیستند؛ تنها در صورتی بهینه هستند که مطابق با توزیع کامل عایدی بر‌اساس خط‌مشی بهینه باشند.
\پایان{تعریف} 

\شروع{تعریف}
خط‌مشی حریصانه
$\pi$
نسبت
$Z \in \EuScript{Z}$
مقدار چشمداشتی
$Z$
 را بیشینه می‌کند.
 مجموعه خط‌مشی‌های حریصانه برای 
 $Z$
 به این شکل تعریف می‌شوند
 $$\EuScript{G}_Z := \lbrace \pi : \sum_{a} \pi (a|x) \ \mathbb{E} \ Z(x,a) = \max_{a' \in \EuScript{A}} \ \mathbb{E} \ Z(x,a') \rbrace.$$
\پایان{تعریف}

\شروع{تعریف}
یک توزیع ارزش بهینه غیرثابت، $Z^{**}$، توزیع ارزشی‌ است که مربوط به دنباله‌ای از خط‌مشی‌های بهینه است. به مجموعه‌ توزیع ارزش‌های بهینه غیرثابت 
$ \EuScript{Z}^{**} $
 گفته می‌شود.
\پایان{تعریف}

\شروع{قضیه}
فرض می‌شود 
$\EuScript{S}$
شمارا و 
$\EuScript{A}$
متناهی باشد.
بنابراین:
$$\forall x,a. \ \lim_{k\to\infty} \inf_{Z^{**} \in \EuScript{Z}^{**}} d_p (Z_k (x,a), Z^{**}(x,a)) = 0.$$
\پایان{قضیه}
\section{تعامل روش‌های مبتنی بر خط‌مشی، مبتنی بر ارزش و روش‌های میانی}
روش‌های مبتنی بر خط‌مشی و مبتنی بر ارزش،
ناسازگار نیستند (بلکه به نظر می‌رسد تحت برخی شرایط، معادل باشد) و تعداد زیادی از الگوریتم‌ها وجود دارند که بین دو  این طیف قرار می‌گیرند. الگوریتم‌هایی که در این طیف قرار دارند قادرند از نقاط قوت  هر دو طرف، استفاده کنند.
به طور مثال، 
\lr{DDPG}
الگوریتمی‌است که  یک خط‌مشی قطعی و یک تابع Q را یاد می‌گیرد،
به ‌طوری‌که از هریک  برای بهبود دیگری استفاد می‌کند. روش
\lr{SAC}،
از خط‌مشی‌های تصادفی، تنظیم آنتروپی  \LTRfootnote{entropy regularization}و چند ترفند دیگر برای  یادگیری و کسب امتیاز بالاتر از 
\lr{DDPG}
در محک‌های استاندارد
\LTRfootnote{Standard Benchmark}
استفاده می‌کند.
\subsection{روش DDPG}
الگوریتم 
\textit{\مهم{گرادیان خط‌مشی قطعی ژرف}}\LTRfootnote{Deep Deterministic Policy Gradient}
یک الگوریتم یادگیری تقویتی بدون مدل، برای فضاهای عمل پیوسته است. این روش از معماری بازیگر-منتقد تبعیت می‌کند. در این روش، دو شبکه عصبی،  آموزش داده می‌شود: یک خط‌مشی قطعی (که به عنوان بازیگر شناخته می‌شود)
$\pi:\EuScript{S} \to \EuScript{A}$
و یک تقریب‌گر تابع ارزش عمل (که به عنوان منتقد شناخته می‌شود)
$q: \EuScript{S} \times \EuScript{A} \to \mathbb{R}$.
وظیفه منتقد، تخمین تابع ارزش عمل بازیگر،
$q_\pi$
است.
تجربه ها توسط یک خط‌مشی رفتار $\pi_b$ تولید می‌شود که یک نسخه 
\textit{{نوفه‌دار}}\LTRfootnote{Noisy}
از $\pi$ است؛ یعنی
$$\pi_b(s) = \pi(s) + \EuScript{N}(0,1)$$
که منظور از
$\EuScript{N}(0,1)$
یک متغیر تصادفی نرمال استاندارد است که 
\textit{\مهم{نوفه}}\LTRfootnote{Noise}
نامیده می‌شود و نقش آن \مهم{حفظ اکتشاف} در خط‌مشی رفتار است.
در این روش، نحوه یادگیری منتقد مشابه روش DQN است، با این تفاوت که $y_t$ با استفاده از عملی که بازیگر مشخص می‌کند، محاسبه می‌شود
$$y_t = R_t + \gamma q(S_{t+1},\pi(S_{t+1}).$$
بازیگر نیز توسط صعود گرادیان روی مقیاس عملکرد
$\EuScript{L}_a = \mathbb{E}_s q(s, \pi(s))$
که $s$ از انبار تکرار نمونه‌گیری می‌شود، می‌آموزد
\cite{andrychowicz2017hindsight}.
گسسته سازی فضای عمل یک روش برای سازگار‌کردن و انطباق روش‌های یادگیری تقویتی عمیق نظیر 
\lr{DQN}
با دامنه‌های پیوسته می‌باشد. با این حال، این روش محدودیت‌های زیادی دارد، مخصوصا مشکل 
\textit{نفرین ابعاد}\LTRfootnote{Curse of Dimensionality}.
 نفرین ابعاد بیان‌گر این است که تعداد عمل‌ها به صورت نمایی با تعداد درجات آزادی افزایش پیدا می‌کند. 
در روشی DDPG یک الگوریتم بازیگر-منتقد مستقل از خط‌مشی و بدون مدل
\LTRfootnote{A model-free, off-policy actor-critic algorithm}
با استفاده از شبکه عصبی، ارائه می‌شود که می‌تواند خط‌مشی‌ها را در فضاهای عمل پیوسته با ابعاد بالا یاد بگیرد.
این روش، بر اساس الگوریتم گرادیان خط‌مشی معین
\LTRfootnote{Deterministic policy gradient (DPG)}
است که آن را گرادیان خط‌مشی معین عمیق
\LTRfootnote{Deep DPG (DDPG)} 
می‌نامند. 
%Here we combine the actor-critic approach with insights from Deep Q Network (DQN)
در این روش، رویکرد بازیگر-منتقد، با بینش شبکه \lr{Q} عمیق
\LTRfootnote{Deep Q Network (DQN)} 
ترکیب می‌شود.
الگوریتم 
\lr{DPG}
تابع خط‌مشی معین پارامتری
$
\pi(s;\theta)	
$
را نگهداری می‌کند که هر حالت را به یک عمل مشخص، می‌نگارد.
همانند روش 
\lr{Q-learning}
در اینجا نیز منتقد
$q(s,a)$
با استفاده از معادله بلمن آموخته می‌شود.
%the actor is updated by following applying the chain rule to the expected return from the start distribution with respect to the actor parameters 
بازیگر با پیروی از اعمال قاعده زنجیره‌ای روی امیدریاضی عایدی از توزیع شروع
$J$
نسبت به پارامترهای بازیگر به‌روز می‌شود
\begin{align}
	\nabla_{\theta \mu} J = & \mathbb{E}_{s_t \sim \rho ^{ \beta}} \left[ \nabla_{\theta \mu} Q(s,a|\theta^Q)|_{s=s_t,a=\mu (s_t|\theta^{\mu})}  \right] \\ \nonumber
	= & \mathbb{E}_{s_t \sim \rho ^{ \beta}} [ \nabla_{\theta \mu} Q(s,a|\theta^Q)|_{s=s_t, a=\mu (s_t)} \nabla_{\theta_{\mu}} \mu (s|\theta^{\mu})|_{s=s_t} ]
\end{align}
در این روش نیز، مانند روش 
\lr{DQN}
از انبار تکرار استفاده می‌شود. 
در هر مرحله، بازیگر و منتقد با نمونه‌برداری یکنواخت از انبار تکرار به‌روزرسانی می‌شوند. از آنجا که 
\lr{DDPG}
الگوریتمی مستقل از خط‌مشی ، انبار تکرار می‌تواند بزرگ باشد، که به الگوریتم اجازه می‌دهد تا از یادگیری مجموعه‌ای از انتقال‌های ناهمبسته بهره‌مند شود.
چالش اصلی یادگیری در فضاهای عمل پیوسته، اکتشاف است. یک مزیت  الگوریتم‌های مستقل از خط‌مشی مانند 
\lr{DDPG}
این است که می‌توان به مسئله کاوش، مستقل از الگوریتم یادگیری پرداخت.
می‌توان یک خط‌مشی اکتشاف 
$\mu'$
را با اضافه کردن نویز (نمونه‌گیری شده از فرآیند $\EuScript{N}$) به خط‌مشی بازیگر ساخت
\begin{equation}
\mu'(s_t) = \mu(s_t|\theta_t^mu) + \EuScript{N}
\end{equation}
می‌توان $\EuScript{N}$ 
را متناسب با محیط انتخاب کرد.
\شروع{الگوریتم}{الگوریتم DDPG}
\دستور{پارامترهای 
	$\theta_\mu$
	 و 
	 $\theta_Q$
	 به ترتیب مربوط به بازیگر 
	 $\mu(s;\theta_\mu)$
	 و منتقد
	 $Q(s,a;\theta_Q)$
	 را مقداردهی اولیه کن.
}
\دستور{پارامترهای توابع هدف $\mu'$ و $Q'$ را با وزن‌های 
$\theta_{\mu'} \longleftrightarrow \theta_\mu$
و
$\theta_{Q'} \longleftarrow \theta_Q$
مقداردهی اولیه کن
}
\دستور{حافظه تکرارها $R$ را بساز}
\‌به‌ازای{برای هر اپیزود $1...M$}
\دستور{یک تابع نویز تصادفی $\EuScript{N}$} بساز
\دستور{حالت اولیه $S_1$ را مشاهده کن}
\‌به‌ازای{برای $t=1...T$}
\دستور{عمل 
	$a_t = \mu(s_t; \theta_\mu) + \mathbb{N}_t$
	 را بر اساس خط‌مشی فعلی و نویز اکتشاف، انتخاب کن و حالت بعدی $S_{t+1}$} و پاداش $R_t$ را مشاهده کن.
 \دستور{تجربه 
 	$(s_t, a_t, r_t, s_{t+1})$
 	 را در انبار تجربه $R$ ذخیره کن}
  \دستور{یک نمونه به اندازه $N$ از تجربه‌های $(s_i, a_i, r_i, s_{i+1})$ از انبار تجربه $R$ انتخاب کن }
  \دستور{وزن‌های منتقد $\theta_Q$ را با درنظر گرفتن تابع هزینه 
  	$L = \dfrac{1}{N} \sum_i(y_i - Q(S_i, A_i; \theta_Q))^2$
  	 به‌روزرسانی کن}
   \دستور{وزن‌های بازیگر $\theta_\mi$ را با استفاده از گرادیان خط‌مشی نمونه 
   	$$\nabla_{\theta_\mu} J \approx \dfrac{1}{N} \sum_{i} \nabla_a Q(s,a;\theta_Q) |_{s=s_i, a=\mu(s_i)} \nabla_{\thata_\mu} \mu(s;\theta_\mu) |_{S_i}$$
   	 به‌روزرسانی کن}
   \دستور{وزن‌های توابع هدف را به شکل 
   	$$\theta_{Q'} = \tau \theta_Q + (1-\tau) \theta_{Q'} \\ \theta_{\mu'} = \tau \theta_\mu + (1-\tau) \theta_{\mu'}$$
   	 به‌روزرسانی کن}
%\پایان‌به‌ازای
%\پایان‌به‌ازای
\پایان{الگوریتم}
\subsection{روش SAC}
در این بخش، روش دیگری با معماری  بازیگر-منتقد و مستقل از خط مشی، با مقیاس عملکرد جدیدی به نام
\textit{بیشینه آنتروپی}\LTRfootnote{Maximum Antropy}
معرفی خواهد شد.
این روش با عنوان روش	
\textit{بازیگر-منتقد نرم}\LTRfootnote{Soft Actor Critic(SAC)}
شناخته می‌شود.
الگوریتم  بازیگر-منتقد نرم، به سادگی به کارهای بسیار پیچیده با ابعاد بالا بسط داده می‌شود، جایی که روش‌های مستقل از خط مشی‌ مانند
\lr(DDPG)
معمولا برای رسیدن به نتایج مناسب، با چالش روبه‌رو هستند.
یادگیری خط‌مشی در فضاهای عمل پیوسته صورت می‌گیرد. یک فرآیند تصمیم‌گیری مارکوف 
\textit{افق-بی‌نهایت}\LTRfootnote{Infinite Horizon}
در نظر بگیرید که در آن فضای حالت
$\EuScript{S}$
و فضای عمل
$\EuScript{A}$
پیوسته هستند.
در این بخش، یک مقیاس عملکرد عمومی‌تر یعنی بیشینه آنتروپی معرفی و بررسی خواهد شد.
به عنوان مثال 
\cite{ziebart2010modeling}
%!!!!!!!!!!!!!!!!
که با تقویت مقیاس عملکرد آنتروپی چشم‌داشتی خط‌مشی به نفع خط‌مشی‌های تصادفی عمل می‌کند
\begin{align}
J_{(\pi)}= \sum\limits_{t=0}^T \mathbb{E}_\pi \left[ r(s_t , a_t) + \alpha \EuScript{H} (\pi(.|s_t)) \right].
\label{eq:jpi}
\end{align}
می‌توان الگوریتم بازیگر-منتقد نرم مستقل از خط‌مشی را با شروع از نوعی دیگر از متد تکرار خط‌مشیِ بیشینه آنتروپی استخراج کرد.
با به‌دست آوردن تکرار خط‌مشی نرم شروع خواهد شد، که یک الگوریتم عمومی برای یادگیری خط‌مشی‌های بیشینه آنتروپی بهینه که در چارچوب بیشینه آنتروپی بین ارزیابی و بهبود خط‌مشی در حال تناوب است.
در گام ارزیابی خط‌مشی امید است مقدار خط‌مشی
$\pi$
براساس مقیاس عملکرد بیشینه آنتروپی در معادله \ref{eq:jpi} محاسبه شود. برای یک خط‌مشی ثابت، مقدار $Q$  نرم می‌تواند به صورت گام‌به‌گام با شروع از هر تابع
$Q:S \times R \rightarrow R$ 
و اعمال مکرر یک عملگر پشتیبان بلمن اصلاح شده،
$\EuScript{T}^\pi$
که در تابع ارزش حالت نرم به صورت زیر داده شده است، محاسبه شود
\begin{align}
\EuScript{T}^\pi Q(s_t,a_t) \doteq r(s_t,a_t)+\gamma \mathbb{E} \left[ V(s_t+1) \right]
\label{eq:tpi}
\end{align}
\begin{align}
V(s_t)= \mathbb{E}_\pi \left[ Q(s_t,a_t)- \log \pi(a_t | s_t) \right]
\end{align}
می‌توان تابع ارزش نرم برای هر خط‌مشی$\pi$ را با اعمال مکرر، $T^\pi$ به‌دست آورد\\
\begin{align}
\pi_{new} = arg \min_{\pi' \in \Pi} D_{KL}  \left( \pi'(.|s_t) \parallel \frac{\exp{(Q^{\pi_{old}} (s_t,.))}}{Z^{\pi_{old}} (s_t)}\right)
\label{eq:pinew}
\end{align}
تابع پارش 
$Z^\pi (s_t)$
توزیع را یکنواخت می‌کند و از آنجایی که در حالت عمومی غیرقابل حل است، به گرادیان نسبت به خط‌مشی جدید کمکی نمی‌کند و به همین دلیل می‌تواند نادیده گرفته شود. برای این تابع می‌توان نشان داد که خط‌مشی جدید نسبت به خط‌مشی قدیمی با توجه به مقیاس عملکرد معادله‌ی \ref{eq:jpi} مقدار بیشتری دارد. 
الگوریتم تکرار خط‌مشی نرم کامل، بین ارزیابی خط‌مشی نرم و بهبود خط‌مشی نرم در تناوب است و در میان خط‌مشی‌های عضو
$\Pi$،
احتمالا به خط‌مشی بیشینه آنتروپی بهینه همگرا می‌شود.
\subsubsection{بازیگر-منتقد نرم}
به‌دست آوردن یک تقریب تجربی برای تکرار خط‌مشی نرم در دامنه‌های پیوسته‌ی بزرگ موردنیاز است.  در انتها، تقریب‌گرهای تابع برای تابع
$ Q$
و خط‌مشی استفاده خواهند شد به جای اجرای بهبود و ارزیابی برای همگرایی، بین بهینه‌سازی هر دو شبکه با نزول گرادیان تصادفی در تناوب است. تابع مقدار حالت پارامتری شده
$V_\psi (s_t)$
، تابع$Q$ نرم 
$Q_\theta (s_t,a_t)$
و خط‌مشی قابل حل 
$\pi_\phi (a_t | s_t)$
بررسی خواهند شد. پارامترهای این شبکه‌ها 
$\psi, \theta, \phi$
هستند. به عنوان مثال، توابع ارزش می‌توانند به عنوان شبکه‌های عصبی گویا و خط‌مشی به عنوان یک گوسی با میانگین و همگرایی گرفته شده از شبکه‌های عصبی، مدل شوند.
تابع ارزش نرم برای کمینه کردن مربع خطای باقیمانده‌ها آموزش دیده شده‌است.
که 
$\EuScript{D}$
توزیع حالت‌ها و عمل‌های نمونه‌گیری شده قبلی یا انبار تجربه
\LTRfootnote{Replay Buffer}
است
\begin{align}
\hat{\nabla}_\psi J_V (\psi) = \nabla_\psi V_\psi (s_t) (V_\psi (s_t)-Q_\theta(s_t,a_t)+\log \pi_\phi (a_t |s_t)) ,
\end{align}
عمل‌ها به جای انبار تجربه، براساس خط‌مشی فعلی نمونه‌گیری شده‌اند. پارامترهای تابع $Q$ نرم می‌تواند برای کمینه کردن باقیمانده‌ی بلمن آموزش دیده‌شود
\begin{align}
J_Q(\theta) = \mathbb{E}_{\EuScript{D}} \left[ \frac{1}{2} \left( Q_\theta (s_t,a_t)- \hat{Q} (s_t,a_t)\right)^2 \right] ,
\end{align}
با 
\begin{align*}
\hat{Q} (s_t,a_t) = r(s_t,a_t) + \gamma \mathbb{E} [V_\Psi (s_t+1)] ,
\end{align*}
که می‌تواند مجددا با گرادیان‌های تصادفی بهینه شود
\begin{align}
\hat{\nabla_{\theta}} J_Q (\theta) = \nabla_\theta Q_\theta (a_t,s_t) \left( Q_\theta (s_t,a_t)- r(s_t,a_t) - \gamma V_\Psi (s_t+1) \right). 
\label{eq:hatnabla}
\end{align}
در آخر، پارامترهای خط‌مشی می‌توانند با کمینه کردن مستقیم همگرایی-KL چشمداشتی در معادله \ref{eq:pinew} آموخته شوند
\begin{align}
J_\pi(\phi) = \mathbb{E}\left[ D_{KL}  \left( \pi_\phi(.|s_t) \parallel \frac{\exp{(Q_{\theta} (s_t,.))}}{Z_{\theta} (s_t)}\right) \right]
\end{align}
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
\شروع{الگوریتم}{الگوریتم $Policy Iteration$ با مقیاس عملکرد $L_\pi$}
\دستور{خط‌مشی $\pi_0$
	را مقداردهی اولیه کن
}
$a_t \approx \pi_\phi(a_t|s_t)$ \\
$s_{t+1} \approx p(s_{t+1}|s_t,a_t)$\\
$\EuScript{D}\leftarrow \EuScript{D} \bigcup {(s_t,a_t,r(s_t,a_t),s_{t+1})}$ \\
$\psi \leftarrow \psi - \landa_V \hat{\nabla}_\spi J_V (\psi)$ \\
$\theta_i \leftarrow \theta_i - \landa_Q \hat{\nabla}_{\theta_i} J_Q(\theta_i) for i \in {1,2}$\\
$\phi \leftarrow \phi - \landa_\pi \hat{\nabla}_\phi J_\pi (Q)$ \\
$\bar{\psi} \leftarrow \tau \psi + (1-\tau) \psi$
\پایان{الگوریتم}
که \ref{eq:hatnabla} یک بردار نویز ورودی است که از یک توزیع ثابت مانند یک گوسی کروی نمونه‌گیری شده‌است.
\subsection{مقایسه روش های مبتنی بر خط مشی و مبتنی بر ارزش}
نقطه قوت اصلی روش‌های مبتنی بر خط‌مشی، اصولی بودن آنهاست؛ به این معنا که مستقیماً چیزی که هدف مسئله است را بهینه‌سازی می‌کنند. از این رو، این روش‌ها قابل اتکا و باثبات هستند. در مقابل، روش‌های مبتنی بر ارزش
با یادگیری تابع Q، مقیاس عملکرد را به طور غیر مستقیم بهینه می‌کند. حالت‌های زیادی برای این نوع یادگیری وجود دارد که به شکست منتهی می‌شود، بنابراین این روش‌ها  ثبات کمتری دارند 
\cite{suttonbook}.
در عوض روش‌های 
\rl{Q-learning}
 می‌توانند از تجربیات، به طور موثرتری نسبت به تکنیک‌های بهینه‌سازی خط‌مشی استفاده کنند.


%\قسمت{روش‌های مبتنی بر مدل}

%Unlike model-free RL, there aren’t a small number of easy-to-define clusters of methods for model-based RL: there are many orthogonal ways of using models. We’ll give a few examples, but the list is far from exhaustive. In each case, the model may either be given or learned.
%
%Background: Pure Planning. The most basic approach never explicitly represents the policy, and instead, uses pure planning techniques like model-predictive control (MPC) to select actions. In MPC, each time the agent observes the environment, it computes a plan which is optimal with respect to the model, where the plan describes all actions to take over some fixed window of time after the present. (Future rewards beyond the horizon may be considered by the planning algorithm through the use of a learned value function.) The agent then executes the first action of the plan, and immediately discards the rest of it. It computes a new plan each time it prepares to interact with the environment, to avoid using an action from a plan with a shorter-than-desired planning horizon.
%
%The MBMF work explores MPC with learned environment models on some standard benchmark tasks for deep RL.
%Expert Iteration. A straightforward follow-on to pure planning involves using and learning an explicit representation of the policy, \pi_{\theta}(a|s). The agent uses a planning algorithm (like Monte Carlo Tree Search) in the model, generating candidate actions for the plan by sampling from its current policy. The planning algorithm produces an action which is better than what the policy alone would have produced, hence it is an “expert” relative to the policy. The policy is afterwards updated to produce an action more like the planning algorithm’s output.
%
%The ExIt algorithm uses this approach to train deep neural networks to play Hex.
%AlphaZero is another example of this approach.
%Data Augmentation for Model-Free Methods. Use a model-free RL algorithm to train a policy or Q-function, but either 1) augment real experiences with fictitious ones in updating the agent, or 2) use only fictitous experience for updating the agent.
%
%See MBVE for an example of augmenting real experiences with fictitious ones.
%See World Models for an example of using purely fictitious experience to train the agent, which they call “training in the dream.”
%Embedding Planning Loops into Policies. Another approach embeds the planning procedure directly into a policy as a subroutine—so that complete plans become side information for the policy—while training the output of the policy with any standard model-free algorithm. The key concept is that in this framework, the policy can learn to choose how and when to use the plans. This makes model bias less of a problem, because if the model is bad for planning in some states, the policy can simply learn to ignore it.
%
%See I2A for an example of agents being endowed with this style of imagination.

%\subsection{روش مدل جهان}
%\قسمت{روش مدل جهان}
%Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own dream environment generated by its world model, and transfer this policy back into the actual environment.
%مدل جهانی
%\LTRfootnote{World Models}
% می‌تواند به سرعت و به روشی بدون نظارت آموزش ببیند تا یک بازنمایی از محیط را بیاموزد. سپس با استفاده از ویژگی‌های استخراج شده از مدل جهان به عنوان ورودی به یک عامل، می‌توان یک خط‌مشی ساده و فشرده را آموخت که می‌تواند وظیفه مورد نیاز را حل کند. حتی می‌توانیم عامل را کاملاً در داخل محیط رویایی خود که توسط مدل جهانی آن ایجاد شده، آموزش دهیم و این خط‌مشی آموخته شده را به محیط واقعی انتقال دهیم. در بسیاری از مسائل یادگیری تقویتی مبتنی بر مدل، عامل به مدل قدرتمندی از دینامیک محیط دسترسی دارد.
% \\اکثر رویکردهای مبتنی بر مدل موجود در یادگیری تقویتی، مدلی از محیط را یاد می‌گیرند، اما همچنان در محیط واقعی آموزش می‌بینند. در این روش، ما همچنین می‌توانیم یک محیط  مصنوعی را کاملاً جایگزین محیط واقعی کنیم و خط‌مشی عامل خود را فقط در داخل محیط مصنوعی آموزش دهیم و درنهایت خط‌مشی آموخته شده را به محیط واقعی انتقال دهیم.

%We present a simple model inspired by our own cognitive system. In this model, our agent has a visual sensory component that compresses what it sees into a small representative code. It also has a memory component that makes predictions about future codes based on historical information. Finally, our agent has a decision-making component that decides what actions to take based only on the representations created by its vision and memory components.